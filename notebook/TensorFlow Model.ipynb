{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "pickle_file = '/home/htan/proj/TensorFlow/data/MRMS/' +  '1timeslice.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    name = save['name']\n",
    "    feature = save['features']\n",
    "    del save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fshp = feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature = feature.reshape(fshp[0]*fshp[1], fshp[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f0 = feature[:,:,:,0]\n",
    "f0[f0 <= -2]=np.nan\n",
    "f2 = features[:,:,:,2]\n",
    "f2[f0 <= -999]=np.nan\n",
    "f3 = features[:,:,:,3]\n",
    "f3[f3 <= -999]=np.nan\n",
    "f4 = features[:,:,:,4]\n",
    "f4[f4 <= -999]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24500000, 13)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.isnan(feature).any(axis=1)\n",
    "feature = feature[~b]\n",
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = feature[:,2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24500000, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24500000, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.hstack([feature[:,:2], feature[:,5:]])\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17150000, 2450000, 4900000)\n"
     ]
    }
   ],
   "source": [
    "length = features.shape[0]\n",
    "length_list = range(0, length)\n",
    "train_len = length * 7 / 10\n",
    "validate_len = length / 10 \n",
    "test_len = length - train_len - validate_len\n",
    "print(train_len, validate_len, test_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list = length_list[0:train_len]\n",
    "validate_list = length_list[train_len:train_len + validate_len]\n",
    "test_list = length_list[train_len + validate_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = {\n",
    "    'data' : features[train_list],\n",
    "    'label' : labels[train_list],\n",
    "}\n",
    "validate_dataset= {\n",
    "    'data' : features[validate_list],\n",
    "    'label' : labels[validate_list],\n",
    "}\n",
    "test_dataset = {\n",
    "    'data' : features[test_list],\n",
    "    'label' : labels[test_list],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = train_dataset['data']\n",
    "validate_data = validate_dataset['data']\n",
    "test_data = test_dataset['data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = train_dataset['label']\n",
    "validate_label = validate_dataset['label']\n",
    "test_label = test_dataset['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((10,), (10,))\n"
     ]
    }
   ],
   "source": [
    "#dataset normalize\n",
    "mean = train_data.mean(axis = 0)\n",
    "std = train_data.std(axis = 0)\n",
    "print(mean.shape, std.shape)\n",
    "train_data_n = (train_data - mean)/std\n",
    "validate_data_n = (validate_data - mean)/std\n",
    "test_data_n = (test_data - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class tfmodel:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.steps = 25000\n",
    "        \n",
    "    def fit(self, train_data, train_label):\n",
    "        #simple regression model train_data has shape(samples, features) and label has shape(samples, 1)\n",
    "        dshp = train_data.shape\n",
    "        self.create_model(dshp[1], 1)\n",
    "        self.train(train_data, train_label)\n",
    "        \n",
    "    def create_model(self, feature_num, label_num):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.tf_train_dataset = tf.placeholder(tf.float32, shape=(self.batch_size, feature_num))\n",
    "            self.tf_train_label = tf.placeholder(tf.float32, shape=(batch_size, label_num))\n",
    "            self.weights = tf.Variable(tf.truncated_normal([feature_num, label_num]))\n",
    "            self.biases = tf.Variable(tf.zeros([label_num]))\n",
    "\n",
    "            def model(X, w, b):\n",
    "                return tf.matmul(X, w) + b\n",
    "\n",
    "            self.predicted_label = model(self.tf_train_dataset, self.weights, self.biases)\n",
    "            self.loss = tf.reduce_mean(tf.square(self.predicted_label - self.tf_train_label))\n",
    "\n",
    "            # Learning rate decay\n",
    "            global_step = tf.Variable(0)\n",
    "            starter_learning_rate = 0.01\n",
    "            self.learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 500, 0.90, staircase=True)\n",
    "            self.op = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss, global_step = global_step)\n",
    "    \n",
    "    def train(self, train_data, train_label):\n",
    "        batch_size = self.batch_size\n",
    "        with tf.Session(graph = self.graph) as session:\n",
    "            tf.initialize_all_variables().run()\n",
    "            print('Initialized')\n",
    "            for step in range(self.steps):\n",
    "                # Note: we could use better randomization across epochs.\n",
    "                offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "                # Generate a minibatch.\n",
    "                batch_data = train_data[offset:(offset + batch_size), :]\n",
    "                #print(batch_data.shape)\n",
    "                batch_labels = train_label[offset:(offset + batch_size), 0].reshape(batch_size, 1)\n",
    "                feed_dict = {self.tf_train_dataset : batch_data, self.tf_train_label : batch_labels}\n",
    "                #session.run(predicted_label, feed_dict=feed_dict)\n",
    "                _, l, _, r = session.run([self.predicted_label, self.loss, self.op, self.learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "                if (step % 500 == 0):\n",
    "                    print('step = %d, learning rate = %f, loss = %f' % (step, r, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = tfmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "step = 0, learning rate = 0.010000, loss = 1.142225\n",
      "step = 500, learning rate = 0.009000, loss = 0.059786\n",
      "step = 1000, learning rate = 0.008100, loss = 0.000162\n",
      "step = 1500, learning rate = 0.007290, loss = 0.000022\n",
      "step = 2000, learning rate = 0.006561, loss = 0.000000\n",
      "step = 2500, learning rate = 0.005905, loss = 0.000000\n",
      "step = 3000, learning rate = 0.005314, loss = 0.000000\n",
      "step = 3500, learning rate = 0.004783, loss = 0.000000\n",
      "step = 4000, learning rate = 0.004305, loss = 0.000023\n",
      "step = 4500, learning rate = 0.003874, loss = 0.000060\n",
      "step = 5000, learning rate = 0.003487, loss = 0.000001\n",
      "step = 5500, learning rate = 0.003138, loss = 0.000000\n",
      "step = 6000, learning rate = 0.002824, loss = 0.000000\n",
      "step = 6500, learning rate = 0.002542, loss = 0.000000\n",
      "step = 7000, learning rate = 0.002288, loss = 0.000000\n",
      "step = 7500, learning rate = 0.002059, loss = 0.000000\n",
      "step = 8000, learning rate = 0.001853, loss = 0.000000\n",
      "step = 8500, learning rate = 0.001668, loss = 0.000000\n",
      "step = 9000, learning rate = 0.001501, loss = 0.000000\n",
      "step = 9500, learning rate = 0.001351, loss = 0.000000\n",
      "step = 10000, learning rate = 0.001216, loss = 0.000000\n",
      "step = 10500, learning rate = 0.001094, loss = 0.000000\n",
      "step = 11000, learning rate = 0.000985, loss = 0.000002\n",
      "step = 11500, learning rate = 0.000886, loss = 0.000000\n",
      "step = 12000, learning rate = 0.000798, loss = 0.000000\n",
      "step = 12500, learning rate = 0.000718, loss = 0.000021\n",
      "step = 13000, learning rate = 0.000646, loss = 0.000111\n",
      "step = 13500, learning rate = 0.000581, loss = 0.000020\n",
      "step = 14000, learning rate = 0.000523, loss = 0.000010\n",
      "step = 14500, learning rate = 0.000471, loss = 0.001590\n",
      "step = 15000, learning rate = 0.000424, loss = 0.005750\n",
      "step = 15500, learning rate = 0.000382, loss = 0.044836\n",
      "step = 16000, learning rate = 0.000343, loss = 0.000016\n",
      "step = 16500, learning rate = 0.000309, loss = 0.000023\n",
      "step = 17000, learning rate = 0.000278, loss = 0.000025\n",
      "step = 17500, learning rate = 0.000250, loss = 0.000017\n",
      "step = 18000, learning rate = 0.000225, loss = 0.004405\n",
      "step = 18500, learning rate = 0.000203, loss = 0.003277\n",
      "step = 19000, learning rate = 0.000182, loss = 0.001752\n",
      "step = 19500, learning rate = 0.000164, loss = 0.000042\n",
      "step = 20000, learning rate = 0.000148, loss = 0.000276\n",
      "step = 20500, learning rate = 0.000133, loss = 0.000595\n",
      "step = 21000, learning rate = 0.000120, loss = 0.001530\n",
      "step = 21500, learning rate = 0.000108, loss = 0.062949\n",
      "step = 22000, learning rate = 0.000097, loss = 0.084435\n",
      "step = 22500, learning rate = 0.000087, loss = 2.766206\n",
      "step = 23000, learning rate = 0.000079, loss = 0.030379\n",
      "step = 23500, learning rate = 0.000071, loss = 0.021041\n",
      "step = 24000, learning rate = 0.000064, loss = 0.003422\n",
      "step = 24500, learning rate = 0.000057, loss = 0.000423\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_data_n, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
