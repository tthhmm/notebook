{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "pickle_file = './' +  '1day.pickle'\n",
    "\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    data = save['volume']\n",
    "    del save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 61, 61, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fshp = data.shape\n",
    "fshp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.reshape(fshp[0]*fshp[1]*fshp[2], fshp[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "f0 = feature[:,:,:,0]\n",
    "f0[f0 <= -2]=np.nan\n",
    "f2 = features[:,:,:,2]\n",
    "f2[f0 <= -999]=np.nan\n",
    "f3 = features[:,:,:,3]\n",
    "f3[f3 <= -999]=np.nan\n",
    "f4 = features[:,:,:,4]\n",
    "f4[f4 <= -999]=np.nan\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148840, 13)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.isnan(data).any(axis=1)\n",
    "data = data[~b]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = data[:,2:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148840, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148840, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.hstack([data[:,:2], data[:,5:]])\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Proprocess:\n",
    "    def __init__(self):\n",
    "        self.dataset_standerlize = self.Dataset_standerlize()\n",
    "        \n",
    "    class Dataset_standerlize:\n",
    "        def __init__(self):\n",
    "            self.mean = 0\n",
    "            self.std = 1\n",
    "            \n",
    "        def fit(self, dataset):\n",
    "            '''The dataset should has format shape(samples_numbers)'''\n",
    "            self.mean = dataset.mean(axis = 0)\n",
    "            self.std = dataset.std(axis = 0)\n",
    "            \n",
    "        def transform(self, dataset):\n",
    "            return (dataset - self.mean)/self.std\n",
    "        \n",
    "        def show_parameters(self):\n",
    "            print (self.mean, self.std)\n",
    "            \n",
    "        \n",
    "    def dataset_split(self, features, labels, ratio = [0.7, 0.1, 0.2], has_validate = False):\n",
    "        '''The features should has format shape(samples_numbers, features), \n",
    "        the labels should has formates shape(sample_nmbers, labels)'''\n",
    "        length = features.shape[0]\n",
    "        length_list = range(0, length)\n",
    "        if has_validate == True:\n",
    "            train_len = int(length * ratio[0])\n",
    "            validate_len = int(length * ratio[1]) \n",
    "            test_len = length - train_len - validate_len\n",
    "            train_list = length_list[0:train_len]\n",
    "            validate_list = length_list[train_len:train_len + validate_len]\n",
    "            test_list = length_list[train_len + validate_len:]\n",
    "            train_dataset = {\n",
    "                'data' : features[train_list],\n",
    "                'label' : labels[train_list],\n",
    "            }\n",
    "            validate_dataset= {\n",
    "                'data' : features[validate_list],\n",
    "                'label' : labels[validate_list],\n",
    "            }\n",
    "            test_dataset = {\n",
    "                'data' : features[test_list],\n",
    "                'label' : labels[test_list],\n",
    "            }\n",
    "            return train_dataset, validate_dataset, test_dataset\n",
    "        if has_validate == False:\n",
    "            train_len = int(length * (ratio[0]+ ratio[1]))\n",
    "            test_len = length - train_len\n",
    "            train_list = length_list[0:train_len]\n",
    "            test_list = length_list[train_len:]\n",
    "            train_dataset = {\n",
    "                'data' : features[train_list],\n",
    "                'label' : labels[train_list],\n",
    "            }\n",
    "            test_dataset = {\n",
    "                'data' : features[test_list],\n",
    "                'label' : labels[test_list],\n",
    "            }\n",
    "            return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prep = Proprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = prep.dataset_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "[  0.52406907   0.94947016 -87.67008972 -87.99700165 -88.31690979\n",
      " -87.22756958 -86.86695862 -94.09000397 -92.716362    -0.98706895] [  3.00981379   3.81172252  34.91673279  34.385952    33.61766815\n",
      "  35.82443237  36.4488678   24.52680016  27.6547718    0.2085733 ]\n"
     ]
    }
   ],
   "source": [
    "prep.dataset_standerlize.show_parameters()\n",
    "prep.dataset_standerlize.fit(features)\n",
    "prep.dataset_standerlize.show_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data_n = prep.dataset_standerlize.transform(train['data'])\n",
    "test_data_n = prep.dataset_standerlize.transform(test['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = train['label']\n",
    "test_label = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148840, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_n = prep.dataset_standerlize.transform(features)\n",
    "features_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 61, 61, 10)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_n = features_n.reshape(fshp[0], fshp[1], fshp[2], 10)\n",
    "features_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = labels.reshape(fshp[0], fshp[1], fshp[2], 3)\n",
    "labels_15min_mid = labels[:, 30, 30, 2:]\n",
    "labels_15min_mid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 61, 61, 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_n_VII = features_n[:, :, :, -1:]\n",
    "features_n_VII.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_batch(features, offset = 0, batch_size = 4, time_duration = 8):\n",
    "    if offset < 0 and offset > features.shape[0] - time_duration - batch_size:\n",
    "        raise ValueError('Incorrect offset value, the offset should < batch - 1 and offset > features.shape[0]')\n",
    "    shp = features.shape\n",
    "    batch = [features[offset + i:offset + i + time_duration, :, :, :].reshape(1, time_duration, shp[1], shp[2], shp[3])\n",
    "    for i in range(0, batch_size)]\n",
    "    return np.vstack(batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 8, 61, 61, 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = create_batch(features_n_VII, batch_size = 30)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = labels_15min_mid[:30]\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword can't be an expression (<ipython-input-45-ea8e339fb74b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-45-ea8e339fb74b>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    tuple(shp[0] = None)\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m keyword can't be an expression\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class tfmodel:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.steps = 25000\n",
    "        \n",
    "    def fit(self, train_data, train_label):\n",
    "        #simple regression model train_data has shape(samples, features) and label has shape(samples, 1)\n",
    "        dshp = train_data.shape\n",
    "        self.create_model(dshp[1], 1)\n",
    "        self.train(train_data, train_label)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        # TODO: return predict value\n",
    "        with tf.Session(graph = self.graph) as session:\n",
    "            tf.initialize_all_variables().run()\n",
    "            print('Initialized')\n",
    "            feed_dict = {self.tf_train_dataset : data}\n",
    "            return self.predicted_label.eval(feed_dict = feed_dict)\n",
    "        # return self.model(data)\n",
    "    \n",
    "    def model_init(self, feature_num, label_num):\n",
    "        self.weights = tf.Variable(tf.truncated_normal([feature_num, label_num]))\n",
    "        self.biases = tf.Variable(tf.zeros([label_num]))\n",
    "        \n",
    "    def model(self, X):\n",
    "        return tf.matmul(X, self.weights) + self.biases\n",
    "    \n",
    "    def create_model(self, feature_num, label_num):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.tf_train_dataset = tf.placeholder(tf.float32, shape=(None, feature_num))\n",
    "            self.tf_train_label = tf.placeholder(tf.float32, shape=(None, label_num))\n",
    "            self.model_init(feature_num, label_num)\n",
    "\n",
    "            \n",
    "\n",
    "            self.predicted_label = self.model(self.tf_train_dataset)\n",
    "            self.loss = tf.reduce_mean(tf.square(self.predicted_label - self.tf_train_label))\n",
    "\n",
    "            # Learning rate decay\n",
    "            global_step = tf.Variable(0)\n",
    "            starter_learning_rate = 0.01\n",
    "            self.learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 500, 0.90, staircase=True)\n",
    "            self.op = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss, global_step = global_step)\n",
    "    \n",
    "    def train(self, train_data, train_label):\n",
    "        batch_size = self.batch_size\n",
    "        with tf.Session(graph = self.graph) as session:\n",
    "            tf.initialize_all_variables().run()\n",
    "            print('Initialized')\n",
    "            for step in range(self.steps):\n",
    "                # Note: we could use better randomization across epochs.\n",
    "                offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "                # Generate a minibatch.\n",
    "                batch_data = train_data[offset:(offset + batch_size), :]\n",
    "                #print(batch_data.shape)\n",
    "                batch_labels = train_label[offset:(offset + batch_size), 0].reshape(batch_size, 1)\n",
    "                feed_dict = {self.tf_train_dataset : batch_data, self.tf_train_label : batch_labels}\n",
    "                #session.run(predicted_label, feed_dict=feed_dict)\n",
    "                l, _, r = session.run([self.loss, self.op, self.learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "                if (step % 500 == 0):\n",
    "                    print('step = %d, learning rate = %f, loss = %f' % (step, r, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = tfmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_data_n, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict(test_data_n[:2000, :]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TFclass optimalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model_base:\n",
    "    def __init__(self, feature_num, label_num): \n",
    "        self.info()\n",
    "        self.model_init_parameters(feature_num, label_num)\n",
    "    def info(self):\n",
    "        print('Using basic model:')\n",
    "    \n",
    "    def model(self, X):\n",
    "        return tf.matmul(X, self.weights) + self.biases\n",
    "    \n",
    "    def model_init_parameters(self, feature_num, label_num):\n",
    "        self.weights = tf.Variable(tf.truncated_normal([feature_num, label_num]))\n",
    "        self.biases = tf.Variable(tf.zeros([label_num]))\n",
    "        \n",
    "class Model_complex(Model_base):\n",
    "    \n",
    "    def info(self):\n",
    "        print('Using complex model:')\n",
    "        \n",
    "    def model(self, X):\n",
    "        hidden = tf.matmul(X, self.weights['layer1']) + self.biases['layer1']\n",
    "        return tf.matmul(tf.nn.relu(hidden), self.weights['layer2']) + self.biases['layer2']\n",
    "    \n",
    "    def model_init_parameters(self, feature_num, label_num):\n",
    "        self.weights = {'layer1': tf.Variable(tf.truncated_normal([feature_num, 5])),\n",
    "                       'layer2': tf.Variable(tf.truncated_normal([5, label_num])),}\n",
    "        self.biases = {'layer1': tf.Variable(tf.zeros([5])),\n",
    "                       'layer2': tf.Variable(tf.zeros([label_num])),}\n",
    "\n",
    "class simple_DL():\n",
    "    def __init__(self, fshp, lshp):\n",
    "        self.info()\n",
    "        self.model_init_parameters(fshp, lshp)\n",
    "        \n",
    "    def model(self, X):\n",
    "        shape = tf.pack([tf.shape(X)[0], tf.shape(X)[1]*tf.shape(X)[2]*tf.shape(X)[3]])\n",
    "        reshape = tf.reshape(X, shape)\n",
    "        hidden = tf.matmul(reshape, self.weights['layer1']) + self.biases['layer1']\n",
    "        return tf.matmul(tf.nn.relu6(hidden), self.weights['layer2']) + self.biases['layer2']\n",
    "        \n",
    "    def model_init_parameters(self, fshp, lshp):\n",
    "        if len(fshp) != 5 and len(lshp) != 2:\n",
    "            raise ValueError('Incorrect shape for fshp %s and lshp %s' % (fshp, lshp))\n",
    "        feature_num = fshp[1] * fshp[2] * fshp[3] * fshp[4]\n",
    "        label_num = lshp[1]\n",
    "        self.weights = {'layer1': tf.Variable(tf.truncated_normal([feature_num, 61])),\n",
    "                       'layer2': tf.Variable(tf.truncated_normal([61, label_num])),}\n",
    "        self.biases = {'layer1': tf.Variable(tf.zeros([61])),\n",
    "                       'layer2': tf.Variable(tf.zeros([label_num])),}\n",
    "    \n",
    "    def info(self):\n",
    "        print('Using simple Deep learning model:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_3D():\n",
    "    def __init__(self, fshp, lshp):\n",
    "        self.info()\n",
    "        self.model_init_parameters(fshp, lshp)\n",
    "        \n",
    "    def model(self, X):\n",
    "        conv = tf.nn.conv3d(X, self.weights['layer1'], [1, 1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + self.biases['layer1'])\n",
    "        conv = tf.nn.conv3d(hidden, self.weights['layer2'], [1, 1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + self.biases['layer2'])\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3] * shape[4]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, self.weights['layer3']) + self.biases['layer3'])\n",
    "        return tf.matmul(hidden, self.weights['layer4']) + self.biases['layer4']\n",
    "        \n",
    "    def model_init_parameters(self, fshp, lshp):\n",
    "        if len(fshp) != 5 and len(lshp) != 2:\n",
    "            raise ValueError('Incorrect shape for fshp %s and lshp %s' % (fshp, lshp))\n",
    "        patch_size = 3\n",
    "        channels = fshp[4]\n",
    "        depth1 = 8\n",
    "        depth2 = 16\n",
    "        label_num = lshp[1]\n",
    "        num_hidden = 61\n",
    "        self.weights = {'layer1': tf.Variable(tf.truncated_normal([patch_size, patch_size, patch_size, channels, depth1])),\n",
    "                       'layer2':tf.Variable(tf.truncated_normal([patch_size, patch_size, patch_size, depth1, depth2])),\n",
    "                       'layer3': tf.Variable(tf.truncated_normal([fshp[1]*fshp[2]*fshp[3]*depth2 , num_hidden], stddev=0.1)),\n",
    "                       'layer4': tf.Variable(tf.truncated_normal([num_hidden, label_num], stddev=0.1)),}\n",
    "                      \n",
    "        self.biases = {'layer1': tf.Variable(tf.zeros([depth1])),\n",
    "                       'layer2': tf.Variable(tf.zeros([depth2])),\n",
    "                       'layer3': tf.Variable(tf.zeros([num_hidden])),\n",
    "                       'layer4': tf.Variable(tf.zeros([label_num])),}\n",
    "    \n",
    "    def info(self):\n",
    "        print('Using 3D CNN learning model:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class tfmodel:\n",
    "    def __init__(self, batch_size = 128, steps = 25000, model_class = CNN_3D):\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = steps\n",
    "        self.model_class = model_class\n",
    "        \n",
    "    def fit(self, train_data, train_label):\n",
    "        #simple regression model train_data has shape(samples, features) and label has shape(samples, 1)\n",
    "        dshp = list(train_data.shape)\n",
    "        dshp[0] = None\n",
    "        dshp = tuple(dshp)\n",
    "        lshp = list(train_label.shape)\n",
    "        lshp[0] = None\n",
    "        lshp = tuple(lshp)\n",
    "        self.create_model(dshp, lshp)\n",
    "        self.train(train_data, train_label)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        # TODO: return predict value\n",
    "        with tf.Session(graph = self.graph) as session:\n",
    "            tf.initialize_all_variables().run()\n",
    "            print('Initialized')\n",
    "            feed_dict = {self.tf_train_dataset : data}\n",
    "            return self.predicted_label.eval(feed_dict = feed_dict)\n",
    "        # return self.model(data)\n",
    "    \n",
    "    \n",
    "    def create_model(self, feature_shape, label_shape):\n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.tf_train_dataset = tf.placeholder(tf.float32, shape=feature_shape)\n",
    "            self.tf_train_label = tf.placeholder(tf.float32, shape=label_shape)\n",
    "            self.base = self.model_class(feature_shape, feature_shape)\n",
    "            self.predicted_label = self.base.model(self.tf_train_dataset)\n",
    "            self.loss = tf.reduce_mean(tf.square(self.predicted_label - self.tf_train_label))\n",
    "\n",
    "            # Learning rate decay\n",
    "            global_step = tf.Variable(0)\n",
    "            starter_learning_rate = 0.01\n",
    "            self.learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 500, 0.90, staircase=True)\n",
    "            self.op = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss, global_step = global_step)\n",
    "    \n",
    "    def train(self, train_data, train_label):\n",
    "        batch_size = self.batch_size\n",
    "        with tf.Session(graph = self.graph) as session:\n",
    "            tf.initialize_all_variables().run()\n",
    "            print('Initialized')\n",
    "            for step in range(self.steps):\n",
    "                # Note: we could use better randomization across epochs.\n",
    "                offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "                # Generate a minibatch.\n",
    "                batch_data = train_data[offset:(offset + batch_size), :]\n",
    "                #print(batch_data.shape)\n",
    "                batch_labels = train_label[offset:(offset + batch_size), :]\n",
    "                feed_dict = {self.tf_train_dataset : batch_data, self.tf_train_label : batch_labels}\n",
    "                #session.run(predicted_label, feed_dict=feed_dict)\n",
    "                l, _, r = session.run([self.loss, self.op, self.learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "                if (step % (self.steps / 50) == 0):\n",
    "                    print('step = %d, learning rate = %f, loss = %f' % (step, r, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = tfmodel(batch_size = 4, steps = 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3D CNN learning model:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected int32, got None of type '_Message' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-c397d556d3e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-107-c12cb458c066>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, train_data, train_label)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mlshp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mlshp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlshp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdshp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlshp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-107-c12cb458c066>\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(self, feature_shape, label_shape)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_train_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicted_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_train_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicted_label\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_train_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-106-3f015b2f60ea>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layer2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mreshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layer3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layer3'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layer4'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'layer4'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/htan/.conda/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.pyc\u001b[0m in \u001b[0;36mreshape\u001b[1;34m(tensor, shape, name)\u001b[0m\n\u001b[0;32m   1381\u001b[0m   \"\"\"\n\u001b[0;32m   1382\u001b[0m   result = _op_def_lib.apply_op(\"Reshape\", tensor=tensor, shape=shape,\n\u001b[1;32m-> 1383\u001b[1;33m                                 name=name)\n\u001b[0m\u001b[0;32m   1384\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/htan/.conda/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    453\u001b[0m             values = ops.convert_to_tensor(\n\u001b[0;32m    454\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 455\u001b[1;33m                 as_ref=input_arg.is_ref)\n\u001b[0m\u001b[0;32m    456\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[1;31m# What type does convert_to_tensor think it has?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/htan/.conda/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    618\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconversion_func\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfuncs_at_priority\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m           \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/htan/.conda/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.pyc\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    177\u001b[0m                                          as_ref=False):\n\u001b[0;32m    178\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/htan/.conda/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/constant_op.pyc\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    160\u001b[0m   \u001b[0mtensor_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m--> 162\u001b[1;33m       tensor_util.make_tensor_proto(value, dtype=dtype, shape=shape))\n\u001b[0m\u001b[0;32m    163\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[1;32m/home/htan/.conda/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.pyc\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape)\u001b[0m\n\u001b[0;32m    351\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m       \u001b[0m_AssertCompatible\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m       \u001b[0mnparray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp_dt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m       \u001b[1;31m# check to them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/htan/.conda/envs/py27/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.pyc\u001b[0m in \u001b[0;36m_AssertCompatible\u001b[1;34m(values, dtype)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m       raise TypeError(\"Expected %s, got %s of type '%s' instead.\" %\n\u001b[1;32m--> 290\u001b[1;33m                       (dtype.name, repr(mismatch), type(mismatch).__name__))\n\u001b[0m\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected int32, got None of type '_Message' instead."
     ]
    }
   ],
   "source": [
    "model.fit(b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
