{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "\n",
    "\n",
    "def read_pickle_file(path):\n",
    "    '''read one pickle file '''\n",
    "    with open(path, 'rb') as f:\n",
    "        save = pickle.load(f)\n",
    "        data = save['volume']\n",
    "        del save\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = '/home/ldm/proj/TensorFlow/temp/' +  '20160705.pickle'\n",
    "data = read_pickle_file(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '/home/ldm/proj/TensorFlow/temp/'\n",
    "file_list = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f)) and f[-6:] == 'pickle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "for file_name in file_list:\n",
    "    file_path = path + '/' + file_name\n",
    "    data = read_pickle_file(file_path)\n",
    "    data_dict[file_name] = data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = data_dict['20160703.pickle']\n",
    "data2 = data_dict['20160705.pickle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nf0 = feature[:,:,:,0]\\nf0[f0 <= -2]=np.nan\\nf2 = features[:,:,:,2]\\nf2[f0 <= -999]=np.nan\\nf3 = features[:,:,:,3]\\nf3[f3 <= -999]=np.nan\\nf4 = features[:,:,:,4]\\nf4[f4 <= -999]=np.nan\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "f0 = feature[:,:,:,0]\n",
    "f0[f0 <= -2]=np.nan\n",
    "f2 = features[:,:,:,2]\n",
    "f2[f0 <= -999]=np.nan\n",
    "f3 = features[:,:,:,3]\n",
    "f3[f3 <= -999]=np.nan\n",
    "f4 = features[:,:,:,4]\n",
    "f4[f4 <= -999]=np.nan\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self):\n",
    "        self.dataset_standerlize = self.Dataset_standerlize()\n",
    "        self.dataset_preprocess = self.Dataset_preprocess()\n",
    "        \n",
    "    class Dataset_preprocess:\n",
    "        def __init__(self):\n",
    "            self.shape = 0\n",
    "            \n",
    "        def data_flat(self, dataset):\n",
    "            fshp = dataset.shape\n",
    "            reshape = dataset.reshape(fshp[0]*fshp[1]*fshp[2], fshp[3])\n",
    "            self.shape = fshp\n",
    "            return reshape\n",
    "        \n",
    "        def data_reformat(self, dataset):\n",
    "            '''reformat the flat data into its original format'''\n",
    "            fshp = self.shape\n",
    "            reshape = dataset.reshape(dataset.shape[0]/(fshp[1]*fshp[2]), fshp[1], fshp[2], dataset.shape[1])\n",
    "            return reshape\n",
    "\n",
    "        def nan_remove(self, dataset):\n",
    "            b = np.isnan(dataset).any(axis=1)\n",
    "            return dataset[~b]\n",
    "\n",
    "        def feature_labels(self, dataset, start = 2, end = 5):\n",
    "            labels = dataset[:,start:end]\n",
    "            features = np.hstack([dataset[:,:start], dataset[:,end:]])\n",
    "            return features, labels\n",
    "\n",
    "    def simple_process(self,data):\n",
    "        p = self.dataset_preprocess\n",
    "        f = p.data_flat(data)\n",
    "        nf = p.nan_remove(f)\n",
    "        fs, ls = p.feature_labels(nf)\n",
    "        data_dict = {'features':fs, 'labels':ls}\n",
    "        return(data_dict)\n",
    "        \n",
    "    class Dataset_standerlize:\n",
    "        def __init__(self):\n",
    "            self.mean = 0\n",
    "            self.std = 1\n",
    "            \n",
    "        def fit(self, dataset):\n",
    "            '''The dataset should has format shape(samples_numbers)'''\n",
    "            self.mean = dataset.mean(axis = 0)\n",
    "            self.std = dataset.std(axis = 0)\n",
    "            \n",
    "        def transform(self, dataset):\n",
    "            return (dataset - self.mean)/self.std\n",
    "        \n",
    "        def show_parameters(self):\n",
    "            print (self.mean, self.std)\n",
    "            \n",
    "        \n",
    "    def dataset_split(self, features, labels, ratio = [0.7, 0.1, 0.2], has_validate = False):\n",
    "        '''The features should has format shape(samples_numbers, features), \n",
    "        the labels should has formates shape(sample_nmbers, labels)'''\n",
    "        length = features.shape[0]\n",
    "        length_list = range(0, length)\n",
    "        if has_validate == True:\n",
    "            train_len = int(length * ratio[0])\n",
    "            validate_len = int(length * ratio[1]) \n",
    "            test_len = length - train_len - validate_len\n",
    "            train_list = length_list[0:train_len]\n",
    "            validate_list = length_list[train_len:train_len + validate_len]\n",
    "            test_list = length_list[train_len + validate_len:]\n",
    "            train_dataset = {\n",
    "                'data' : features[train_list],\n",
    "                'label' : labels[train_list],\n",
    "            }\n",
    "            validate_dataset= {\n",
    "                'data' : features[validate_list],\n",
    "                'label' : labels[validate_list],\n",
    "            }\n",
    "            test_dataset = {\n",
    "                'data' : features[test_list],\n",
    "                'label' : labels[test_list],\n",
    "            }\n",
    "            return train_dataset, validate_dataset, test_dataset\n",
    "        if has_validate == False:\n",
    "            train_len = int(length * (ratio[0]+ ratio[1]))\n",
    "            test_len = length - train_len\n",
    "            train_list = length_list[0:train_len]\n",
    "            test_list = length_list[train_len:]\n",
    "            train_dataset = {\n",
    "                'data' : features[train_list],\n",
    "                'label' : labels[train_list],\n",
    "            }\n",
    "            test_dataset = {\n",
    "                'data' : features[test_list],\n",
    "                'label' : labels[test_list],\n",
    "            }\n",
    "            return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prep = Preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = prep.dataset_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = prep.simple_process(data1)\n",
    "data2 = prep.simple_process(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prep.dataset_standerlize.fit(data1['features'])\n",
    "data1_f_n = prep.dataset_standerlize.transform(data1['features'])\n",
    "data2_f_n = prep.dataset_standerlize.transform(data2['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353495, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1_l = data1['labels']\n",
    "data2_l = data2['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353495, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1_f = prep.dataset_preprocess.data_reformat(data1_f_n)\n",
    "data2_f = prep.dataset_preprocess.data_reformat(data2_f_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 61, 61, 10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1_ll = prep.dataset_preprocess.data_reformat(data1_l)\n",
    "data2_ll = prep.dataset_preprocess.data_reformat(data2_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 61, 61, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1_l_s = data1_ll[:, 30, 30, 2:]\n",
    "data2_l_s = data2_ll[:, 30, 30, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1_n_VII = data1_f[:, :, :, -1:]\n",
    "data2_n_VII = data2_f[:, :, :, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 61, 61, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1_n_VII.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_batch(features, offset = 0, batch_size = 4, time_duration = 8):\n",
    "    if offset < 0 and offset > features.shape[0] - time_duration - batch_size:\n",
    "        raise ValueError('Incorrect offset value, the offset should < batch - 1 and offset > features.shape[0]')\n",
    "    shp = features.shape\n",
    "    batch = [features[offset + i:offset + i + time_duration, :, :, :].reshape(1, time_duration, shp[1], shp[2], shp[3])\n",
    "    for i in range(0, batch_size)]\n",
    "    return np.vstack(batch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 8, 61, 61, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = create_batch(data1_n_VII, batch_size = 80)\n",
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = data1_l_s[:80]\n",
    "c1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8, 61, 61, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2 = create_batch(data2_n_VII, batch_size = 4)\n",
    "b2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2 = data2_l_s[:4]\n",
    "c2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class tfmodel:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 128\n",
    "        self.steps = 25000\n",
    "        \n",
    "    def fit(self, train_data, train_label):\n",
    "        #simple regression model train_data has shape(samples, features) and label has shape(samples, 1)\n",
    "        dshp = train_data.shape\n",
    "        self.create_model(dshp[1], 1)\n",
    "        self.train(train_data, train_label)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        # TODO: return predict value\n",
    "        with tf.Session(graph = self.graph) as session:\n",
    "            tf.initialize_all_variables().run()\n",
    "            print('Initialized')\n",
    "            feed_dict = {self.tf_train_dataset : data}\n",
    "            return self.predicted_label.eval(feed_dict = feed_dict)\n",
    "        # return self.model(data)\n",
    "    \n",
    "    def model_init(self, feature_num, label_num):\n",
    "        self.weights = tf.Variable(tf.truncated_normal([feature_num, label_num]))\n",
    "        self.biases = tf.Variable(tf.zeros([label_num]))\n",
    "        \n",
    "    def model(self, X):\n",
    "        return tf.matmul(X, self.weights) + self.biases\n",
    "    \n",
    "    def create_model(self, feature_num, label_num):\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.tf_train_dataset = tf.placeholder(tf.float32, shape=(None, feature_num))\n",
    "            self.tf_train_label = tf.placeholder(tf.float32, shape=(None, label_num))\n",
    "            self.model_init(feature_num, label_num)\n",
    "\n",
    "            \n",
    "\n",
    "            self.predicted_label = self.model(self.tf_train_dataset)\n",
    "            self.loss = tf.reduce_mean(tf.square(self.predicted_label - self.tf_train_label))\n",
    "\n",
    "            # Learning rate decay\n",
    "            global_step = tf.Variable(0)\n",
    "            starter_learning_rate = 0.01\n",
    "            self.learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 500, 0.90, staircase=True)\n",
    "            self.op = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss, global_step = global_step)\n",
    "    \n",
    "    def train(self, train_data, train_label):\n",
    "        batch_size = self.batch_size\n",
    "        with tf.Session(graph = self.graph) as session:\n",
    "            tf.initialize_all_variables().run()\n",
    "            print('Initialized')\n",
    "            for step in range(self.steps):\n",
    "                # Note: we could use better randomization across epochs.\n",
    "                offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "                # Generate a minibatch.\n",
    "                batch_data = train_data[offset:(offset + batch_size), :]\n",
    "                #print(batch_data.shape)\n",
    "                batch_labels = train_label[offset:(offset + batch_size), 0].reshape(batch_size, 1)\n",
    "                feed_dict = {self.tf_train_dataset : batch_data, self.tf_train_label : batch_labels}\n",
    "                #session.run(predicted_label, feed_dict=feed_dict)\n",
    "                l, _, r = session.run([self.loss, self.op, self.learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "                if (step % 500 == 0):\n",
    "                    print('step = %d, learning rate = %f, loss = %f' % (step, r, l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = tfmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_data_n, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.predict(test_data_n[:2000, :]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TFclass optimalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model_base:\n",
    "    def __init__(self, feature_num, label_num): \n",
    "        self.info()\n",
    "        self.model_init_parameters(feature_num, label_num)\n",
    "    def info(self):\n",
    "        print('Using basic model:')\n",
    "    \n",
    "    def model(self, X):\n",
    "        return tf.matmul(X, self.weights) + self.biases\n",
    "    \n",
    "    def model_init_parameters(self, feature_num, label_num):\n",
    "        self.weights = tf.Variable(tf.truncated_normal([feature_num, label_num]))\n",
    "        self.biases = tf.Variable(tf.zeros([label_num]))\n",
    "        \n",
    "class Model_complex(Model_base):\n",
    "    \n",
    "    def info(self):\n",
    "        print('Using complex model:')\n",
    "        \n",
    "    def model(self, X):\n",
    "        hidden = tf.matmul(X, self.weights['layer1']) + self.biases['layer1']\n",
    "        return tf.matmul(tf.nn.relu(hidden), self.weights['layer2']) + self.biases['layer2']\n",
    "    \n",
    "    def model_init_parameters(self, feature_num, label_num):\n",
    "        self.weights = {'layer1': tf.Variable(tf.truncated_normal([feature_num, 5])),\n",
    "                       'layer2': tf.Variable(tf.truncated_normal([5, label_num])),}\n",
    "        self.biases = {'layer1': tf.Variable(tf.zeros([5])),\n",
    "                       'layer2': tf.Variable(tf.zeros([label_num])),}\n",
    "\n",
    "class simple_DL():\n",
    "    def __init__(self, fshp, lshp):\n",
    "        self.info()\n",
    "        self.model_init_parameters(fshp, lshp)\n",
    "        \n",
    "    def model(self, X):\n",
    "        shape = tf.pack([tf.shape(X)[0], tf.shape(X)[1]*tf.shape(X)[2]*tf.shape(X)[3]])\n",
    "        reshape = tf.reshape(X, shape)\n",
    "        hidden = tf.matmul(reshape, self.weights['layer1']) + self.biases['layer1']\n",
    "        return tf.matmul(tf.nn.relu6(hidden), self.weights['layer2']) + self.biases['layer2']\n",
    "        \n",
    "    def model_init_parameters(self, fshp, lshp):\n",
    "        if len(fshp) != 5 and len(lshp) != 2:\n",
    "            raise ValueError('Incorrect shape for fshp %s and lshp %s' % (fshp, lshp))\n",
    "        feature_num = fshp[1] * fshp[2] * fshp[3] * fshp[4]\n",
    "        label_num = lshp[1]\n",
    "        self.weights = {'layer1': tf.Variable(tf.truncated_normal([feature_num, 61])),\n",
    "                       'layer2': tf.Variable(tf.truncated_normal([61, label_num])),}\n",
    "        self.biases = {'layer1': tf.Variable(tf.zeros([61])),\n",
    "                       'layer2': tf.Variable(tf.zeros([label_num])),}\n",
    "    \n",
    "    def info(self):\n",
    "        print('Using simple Deep learning model:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_3D():\n",
    "    def __init__(self, fshp, lshp):\n",
    "        print(fshp, lshp)\n",
    "        self.info()\n",
    "        self.model_init_parameters(fshp, lshp)\n",
    "        \n",
    "    def model(self, X):\n",
    "        conv = tf.nn.conv3d(X, self.weights['layer1'], [1, 2, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu6(conv + self.biases['layer1'])\n",
    "        conv = tf.nn.conv3d(hidden, self.weights['layer2'], [1, 2, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu6(conv + self.biases['layer2'])\n",
    "        shape = tf.pack([tf.shape(hidden)[0], tf.shape(hidden)[1]*tf.shape(hidden)[2]*tf.shape(hidden)[3]*tf.shape(hidden)[4]])\n",
    "        reshape = tf.reshape(hidden, shape)\n",
    "        hidden = tf.nn.relu6(tf.matmul(reshape, self.weights['layer3']) + self.biases['layer3'])\n",
    "        return tf.matmul(hidden, self.weights['layer4']) + self.biases['layer4']\n",
    "        \n",
    "    def model_init_parameters(self, fshp, lshp):\n",
    "        if len(fshp) != 5 and len(lshp) != 2:\n",
    "            raise ValueError('Incorrect shape for fshp %s and lshp %s' % (fshp, lshp))\n",
    "        patch_size = 3\n",
    "        channels = fshp[4]\n",
    "        depth1 = 16\n",
    "        depth2 = 8\n",
    "        label_num = lshp[1]\n",
    "        num_hidden = 100\n",
    "       \n",
    "        self.weights = {'layer1': tf.Variable(tf.truncated_normal([patch_size, patch_size, patch_size, channels, depth1])),\n",
    "                       'layer2':tf.Variable(tf.truncated_normal([patch_size, patch_size, patch_size, depth1, depth2])),\n",
    "                       'layer3': tf.Variable(tf.truncated_normal([4096 , num_hidden], stddev=0.1)),\n",
    "                       'layer4': tf.Variable(tf.truncated_normal([num_hidden, label_num], stddev=0.1)),}\n",
    "                      \n",
    "        self.biases = {'layer1': tf.Variable(tf.zeros([depth1])),\n",
    "                       'layer2': tf.Variable(tf.zeros([depth2])),\n",
    "                       'layer3': tf.Variable(tf.zeros([num_hidden])),\n",
    "                       'layer4': tf.Variable(tf.zeros([label_num])),}\n",
    "    \n",
    "    def info(self):\n",
    "        print('Using 3D CNN learning model:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class tfmodel:\n",
    "    def __init__(self, batch_size = 128, steps = 25000, learning_rate = 0.01, model_class = CNN_3D):\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = steps\n",
    "        self.learn_rate = learning_rate\n",
    "        self.model_class = model_class\n",
    "        \n",
    "    def fit(self, train_data, train_label):\n",
    "        #simple regression model train_data has shape(samples, features) and label has shape(samples, 1)\n",
    "        dshp = list(train_data.shape)\n",
    "        dshp[0] = None\n",
    "        dshp = tuple(dshp)\n",
    "        lshp = list(train_label.shape)\n",
    "        lshp[0] = None\n",
    "        lshp = tuple(lshp)\n",
    "        self.create_model(dshp, lshp)\n",
    "        self.train(train_data, train_label, self.batch_size, self.steps, inital = True)\n",
    "        \n",
    "    def partial_fit(self, train_data, train_label, steps):\n",
    "        self.train(train_data, train_label, 1, steps)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        '''return the predict value'''\n",
    "        with tf.Session(graph = self.graph) as session:\n",
    "            self.saver.restore(session, \"/home/ldm/proj/TensorFlow/temp/model.ckpt\")\n",
    "            print('Model restored')\n",
    "            feed_dict = {self.tf_train_dataset : data}\n",
    "            return self.predicted_label.eval(feed_dict = feed_dict)\n",
    "        # return self.model(data)\n",
    "    \n",
    "    \n",
    "    def create_model(self, feature_shape, label_shape):\n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.tf_train_dataset = tf.placeholder(tf.float32, shape=feature_shape)\n",
    "            self.tf_train_label = tf.placeholder(tf.float32, shape=label_shape)\n",
    "            self.base = self.model_class(feature_shape, label_shape)\n",
    "            self.predicted_label = self.base.model(self.tf_train_dataset)\n",
    "            self.loss = tf.reduce_mean(tf.square(self.predicted_label - self.tf_train_label))\n",
    "\n",
    "            # Learning rate decay\n",
    "            global_step = tf.Variable(0)\n",
    "            starter_learning_rate = self.learn_rate\n",
    "            self.learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 20, 0.90, staircase=True)\n",
    "            self.op = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss, global_step = global_step)\n",
    "            self.saver = tf.train.Saver()\n",
    "    \n",
    "    def train(self, train_data, train_label, batch_size, steps, inital = False):\n",
    "        \n",
    "        with tf.Session(graph = self.graph) as session:\n",
    "            if inital == True:\n",
    "                tf.initialize_all_variables().run()\n",
    "                print('Initialized')\n",
    "            else:\n",
    "                self.saver.restore(session, \"/home/ldm/proj/TensorFlow/temp/model.ckpt\")\n",
    "                print('Model restored')\n",
    "            for step in range(steps):\n",
    "                # Note: we could use better randomization across epochs.\n",
    "                offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "                # Generate a minibatch.\n",
    "                batch_data = train_data[offset:(offset + batch_size), :]\n",
    "                #print(batch_data.shape)\n",
    "                batch_labels = train_label[offset:(offset + batch_size), :]\n",
    "                feed_dict = {self.tf_train_dataset : batch_data, self.tf_train_label : batch_labels}\n",
    "                #session.run(predicted_label, feed_dict=feed_dict)\n",
    "                l, _, r = session.run([self.loss, self.op, self.learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "                if (step % (steps / 50) == 0):\n",
    "                    loss_feed = {self.tf_train_dataset : train_data, self.tf_train_label : train_label}\n",
    "                    tl = self.loss.eval(feed_dict = loss_feed)\n",
    "                    print('step = %d, learning rate = %f, loss = %f' % (step, r, tl))\n",
    "            save_path = self.saver.save(session, \"/home/ldm/proj/TensorFlow/temp/model.ckpt\")\n",
    "            print('Model save in file: %s' % (save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = tfmodel(batch_size = 2, steps = 250, learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 8, 61, 61, 1) (None, 1)\n",
      "Using 3D CNN learning model:\n",
      "Initialized\n",
      "step = 0, learning rate = 0.001000, loss = 2.086961\n"
     ]
    }
   ],
   "source": [
    "model.fit(b1, c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored\n",
      "step = 0, learning rate = 0.000282, loss = 0.451763\n",
      "step = 1, learning rate = 0.000282, loss = 0.936149\n",
      "step = 2, learning rate = 0.000282, loss = 0.038624\n",
      "step = 3, learning rate = 0.000282, loss = 0.085584\n",
      "step = 4, learning rate = 0.000282, loss = 0.740418\n",
      "step = 5, learning rate = 0.000282, loss = 0.196060\n",
      "step = 6, learning rate = 0.000282, loss = 0.092863\n",
      "step = 7, learning rate = 0.000282, loss = 0.465139\n",
      "step = 8, learning rate = 0.000282, loss = 0.226488\n",
      "step = 9, learning rate = 0.000282, loss = 0.186419\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-58876ce85fe5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-30-d5cc2c9083e7>\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, train_data, train_label, steps)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-d5cc2c9083e7>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, train_data, train_label, batch_size, steps, inital)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_train_label\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[1;31m#session.run(predicted_label, feed_dict=feed_dict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msteps\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ldm/proj/miniconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 372\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    373\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ldm/proj/miniconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 636\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    637\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ldm/proj/miniconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 708\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    709\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/ldm/proj/miniconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    713\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 715\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    716\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ldm/proj/miniconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.partial_fit(b2, c2, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = model.predict(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
