{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, Image\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASOS_alone.pickle\t     CC_Under_sSample.pickle\r\n",
      "ASOS+NWP.pickle\t\t     KORD_RUC_RAP_Hourly_20051031-20150301.csv\r\n",
      "ASOS+NWP_time_serial.pickle  rough_visibility.pickle\r\n",
      "ASOS_time_serial.pickle      visibilityDataNoLagsNewPreds.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls '/home/htan/proj/TensorFlow/data/visibility/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load data first\n",
    "pickle_file = '/home/htan/proj/TensorFlow/data/visibility/' +  'ASOS+NWP_time_serial.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    #train_dataset= save['train_dataset']\n",
    "    #validate_dataset = save['validate_dataset']\n",
    "    test_dataset = save['test_dataset']\n",
    "    #test_old = save['v_t_dataset']\n",
    "    train_old = save['t_v_dataset']\n",
    "    del save\n",
    "\n",
    "#train_time = train_dataset['time']\n",
    "#train_data = train_dataset['data']\n",
    "#train_label = train_dataset['label']\n",
    "#validate_time = validate_dataset['time']\n",
    "#validate_data = validate_dataset['data']\n",
    "#validate_label = validate_dataset['label']\n",
    "#test_time = test_dataset['time']\n",
    "test_data = test_dataset['data']\n",
    "test_label = test_dataset['label']\n",
    "#test_old_data = test_old['data']\n",
    "#test_old_label = test_old['label']\n",
    "train_old_data = train_old['data']\n",
    "train_old_label = train_old['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17664, 8, 82) (17664, 8, 1)\n",
      "(70656, 8, 82) (70656, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape, test_label.shape)\n",
    "print(train_old_data.shape, train_old_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82,) (82,)\n"
     ]
    }
   ],
   "source": [
    "#dataset normalize\n",
    "mean = train_old_data.mean(axis = (0,1))\n",
    "std = train_old_data.std(axis = (0,1))\n",
    "print(mean.shape, std.shape)\n",
    "train_data_n = (train_old_data - mean)/std\n",
    "#validate_data_n = (validate_data - mean)/std\n",
    "test_data_n = (test_data - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = train_old_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LSTM regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = test_label[:10, 7, :].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = 82\n",
    "n_steps = 8\n",
    "n_labels = 1\n",
    "\n",
    "n_hidden = 140\n",
    "total_size = train_old_label.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs = tf.placeholder(\"float32\", [None, n_steps, n_features])\n",
    "    labels = tf.placeholder(\"float32\", [None, n_labels])\n",
    "    \n",
    "    weights = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_features, n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_labels]))\n",
    "    }\n",
    "    biases = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_labels]))\n",
    "    }\n",
    "    \n",
    "    def RNN(x, w, b):\n",
    "        # Prepare data shape to match `rnn` function requirements\n",
    "        # Current data input shape: (batch_size, n_steps, n_input)\n",
    "        # Required shape: 'n_steps' tensors list of shape (batch_size, n_hidden)\n",
    "    \n",
    "        # Permuting batch_size and n_steps\n",
    "        x = tf.transpose(x, [1, 0, 2])\n",
    "        # Reshaping to (n_steps*batch_size, n_input)\n",
    "        x = tf.reshape(x, [-1, n_features])\n",
    "        \n",
    "        # Linear activation\n",
    "        x = tf.matmul(x, w['hidden']) + b['hidden']\n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_hidden)\n",
    "        x = tf.split(0, n_steps, x)\n",
    "\n",
    "        # Define a lstm cell with tensorflow\n",
    "        lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "\n",
    "        # Get lstm cell output\n",
    "        outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "        # Linear activation, using rnn inner loop last output\n",
    "        return tf.matmul(outputs[-1], w['out']) + b['out']\n",
    "    \n",
    "    pred = RNN(inputs, weights, biases)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    loss = tf.reduce_mean(tf.abs(pred - labels))\n",
    "    \n",
    "    # Learning rate decay\n",
    "    global_step = tf.Variable(0)\n",
    "    starter_learning_rate = 0.15\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 250, 0.90, staircase=True)\n",
    "    op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    MAE = tf.reduce_mean(tf.abs(pred - labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "step: 0, LR = 0.150000, min batch loss = 8.971102, test MAE = 8.520988\n",
      "step: 100, LR = 0.150000, min batch loss = 1.961214, test MAE = 2.389309\n",
      "step: 200, LR = 0.150000, min batch loss = 1.589390, test MAE = 1.402911\n",
      "step: 300, LR = 0.135000, min batch loss = 2.388237, test MAE = 1.290621\n",
      "step: 400, LR = 0.135000, min batch loss = 1.566636, test MAE = 1.133223\n",
      "step: 500, LR = 0.121500, min batch loss = 0.622787, test MAE = 1.012790\n",
      "step: 600, LR = 0.121500, min batch loss = 0.896475, test MAE = 0.987309\n",
      "step: 700, LR = 0.121500, min batch loss = 1.071309, test MAE = 0.971154\n",
      "step: 800, LR = 0.109350, min batch loss = 0.889713, test MAE = 0.867428\n",
      "step: 900, LR = 0.109350, min batch loss = 1.921682, test MAE = 0.944717\n",
      "step: 1000, LR = 0.098415, min batch loss = 0.384690, test MAE = 0.812613\n",
      "step: 1100, LR = 0.098415, min batch loss = 0.416397, test MAE = 0.804676\n",
      "step: 1200, LR = 0.098415, min batch loss = 0.737781, test MAE = 0.831845\n",
      "step: 1300, LR = 0.088573, min batch loss = 0.573132, test MAE = 0.746994\n",
      "step: 1400, LR = 0.088573, min batch loss = 0.510942, test MAE = 0.741056\n",
      "step: 1500, LR = 0.079716, min batch loss = 0.767329, test MAE = 0.750478\n",
      "step: 1600, LR = 0.079716, min batch loss = 0.520807, test MAE = 0.740878\n",
      "step: 1700, LR = 0.079716, min batch loss = 1.595091, test MAE = 0.749515\n",
      "step: 1800, LR = 0.071745, min batch loss = 1.515335, test MAE = 0.681472\n",
      "step: 1900, LR = 0.071745, min batch loss = 1.126713, test MAE = 0.728418\n",
      "step: 2000, LR = 0.064570, min batch loss = 0.841939, test MAE = 0.731502\n",
      "step: 2100, LR = 0.064570, min batch loss = 1.147311, test MAE = 0.643811\n",
      "step: 2200, LR = 0.064570, min batch loss = 0.277075, test MAE = 0.681295\n",
      "step: 2300, LR = 0.058113, min batch loss = 0.393347, test MAE = 0.676212\n",
      "step: 2400, LR = 0.058113, min batch loss = 0.219728, test MAE = 0.661650\n",
      "step: 2500, LR = 0.052302, min batch loss = 0.178340, test MAE = 0.690459\n",
      "step: 2600, LR = 0.052302, min batch loss = 0.161896, test MAE = 0.645022\n",
      "step: 2700, LR = 0.052302, min batch loss = 0.582073, test MAE = 0.628722\n",
      "step: 2800, LR = 0.047072, min batch loss = 0.800498, test MAE = 0.640870\n",
      "step: 2900, LR = 0.047072, min batch loss = 1.225920, test MAE = 0.631382\n",
      "step: 3000, LR = 0.042364, min batch loss = 0.190147, test MAE = 0.642601\n",
      "step: 3100, LR = 0.042364, min batch loss = 0.444842, test MAE = 0.616751\n",
      "step: 3200, LR = 0.042364, min batch loss = 0.818871, test MAE = 0.607548\n",
      "step: 3300, LR = 0.038128, min batch loss = 0.214601, test MAE = 0.612948\n",
      "step: 3400, LR = 0.038128, min batch loss = 0.170395, test MAE = 0.633220\n",
      "step: 3500, LR = 0.034315, min batch loss = 0.341501, test MAE = 0.609143\n",
      "step: 3600, LR = 0.034315, min batch loss = 0.179331, test MAE = 0.630763\n",
      "step: 3700, LR = 0.034315, min batch loss = 0.582144, test MAE = 0.594989\n",
      "step: 3800, LR = 0.030884, min batch loss = 0.210707, test MAE = 0.601593\n",
      "step: 3900, LR = 0.030884, min batch loss = 0.236491, test MAE = 0.626265\n",
      "step: 4000, LR = 0.027795, min batch loss = 1.157915, test MAE = 0.613108\n",
      "step: 4100, LR = 0.027795, min batch loss = 0.677351, test MAE = 0.596555\n",
      "step: 4200, LR = 0.027795, min batch loss = 0.783177, test MAE = 0.608177\n",
      "step: 4300, LR = 0.025016, min batch loss = 0.188280, test MAE = 0.594490\n",
      "step: 4400, LR = 0.025016, min batch loss = 0.842248, test MAE = 0.588610\n",
      "step: 4500, LR = 0.022514, min batch loss = 0.568184, test MAE = 0.604296\n",
      "step: 4600, LR = 0.022514, min batch loss = 0.510118, test MAE = 0.597119\n",
      "step: 4700, LR = 0.022514, min batch loss = 0.685876, test MAE = 0.603709\n",
      "step: 4800, LR = 0.020263, min batch loss = 0.181525, test MAE = 0.581550\n",
      "step: 4900, LR = 0.020263, min batch loss = 1.147263, test MAE = 0.578661\n",
      "step: 5000, LR = 0.018236, min batch loss = 0.766007, test MAE = 0.591102\n",
      "step: 5100, LR = 0.018236, min batch loss = 0.298191, test MAE = 0.591255\n",
      "step: 5200, LR = 0.018236, min batch loss = 0.301742, test MAE = 0.587628\n",
      "step: 5300, LR = 0.016413, min batch loss = 0.545197, test MAE = 0.581366\n",
      "step: 5400, LR = 0.016413, min batch loss = 0.305506, test MAE = 0.578566\n",
      "step: 5500, LR = 0.014772, min batch loss = 0.866129, test MAE = 0.578036\n",
      "step: 5600, LR = 0.014772, min batch loss = 1.205243, test MAE = 0.589030\n",
      "step: 5700, LR = 0.014772, min batch loss = 0.074378, test MAE = 0.578874\n",
      "step: 5800, LR = 0.013294, min batch loss = 0.119629, test MAE = 0.595960\n",
      "step: 5900, LR = 0.013294, min batch loss = 0.359525, test MAE = 0.575383\n",
      "step: 6000, LR = 0.011965, min batch loss = 0.354193, test MAE = 0.573505\n",
      "step: 6100, LR = 0.011965, min batch loss = 0.127033, test MAE = 0.578573\n",
      "step: 6200, LR = 0.011965, min batch loss = 0.689714, test MAE = 0.580805\n",
      "step: 6300, LR = 0.010768, min batch loss = 0.106920, test MAE = 0.575552\n",
      "step: 6400, LR = 0.010768, min batch loss = 0.232319, test MAE = 0.576523\n",
      "step: 6500, LR = 0.009692, min batch loss = 0.257049, test MAE = 0.570398\n",
      "step: 6600, LR = 0.009692, min batch loss = 0.459599, test MAE = 0.570093\n",
      "step: 6700, LR = 0.009692, min batch loss = 0.414738, test MAE = 0.582497\n",
      "step: 6800, LR = 0.008722, min batch loss = 0.558756, test MAE = 0.571054\n",
      "step: 6900, LR = 0.008722, min batch loss = 0.726920, test MAE = 0.586785\n",
      "step: 7000, LR = 0.007850, min batch loss = 0.109474, test MAE = 0.572345\n",
      "step: 7100, LR = 0.007850, min batch loss = 0.518930, test MAE = 0.568660\n",
      "step: 7200, LR = 0.007850, min batch loss = 0.694139, test MAE = 0.568530\n",
      "step: 7300, LR = 0.007065, min batch loss = 0.995694, test MAE = 0.576294\n",
      "step: 7400, LR = 0.007065, min batch loss = 0.343325, test MAE = 0.571705\n",
      "step: 7500, LR = 0.006359, min batch loss = 0.623192, test MAE = 0.573000\n",
      "step: 7600, LR = 0.006359, min batch loss = 0.131952, test MAE = 0.569225\n",
      "step: 7700, LR = 0.006359, min batch loss = 0.080629, test MAE = 0.566881\n",
      "step: 7800, LR = 0.005723, min batch loss = 0.850877, test MAE = 0.574431\n",
      "step: 7900, LR = 0.005723, min batch loss = 0.508226, test MAE = 0.568925\n",
      "step: 8000, LR = 0.005151, min batch loss = 0.353982, test MAE = 0.579790\n",
      "step: 8100, LR = 0.005151, min batch loss = 0.138298, test MAE = 0.567450\n",
      "step: 8200, LR = 0.005151, min batch loss = 0.744233, test MAE = 0.566467\n",
      "step: 8300, LR = 0.004635, min batch loss = 0.673507, test MAE = 0.566254\n",
      "step: 8400, LR = 0.004635, min batch loss = 0.578519, test MAE = 0.572923\n",
      "step: 8500, LR = 0.004172, min batch loss = 0.237063, test MAE = 0.570123\n",
      "step: 8600, LR = 0.004172, min batch loss = 0.997817, test MAE = 0.572824\n",
      "step: 8700, LR = 0.004172, min batch loss = 0.255030, test MAE = 0.566101\n",
      "step: 8800, LR = 0.003755, min batch loss = 0.230232, test MAE = 0.565016\n",
      "step: 8900, LR = 0.003755, min batch loss = 0.536351, test MAE = 0.569172\n",
      "step: 9000, LR = 0.003379, min batch loss = 0.336608, test MAE = 0.567946\n",
      "step: 9100, LR = 0.003379, min batch loss = 0.197215, test MAE = 0.575732\n",
      "step: 9200, LR = 0.003379, min batch loss = 0.442082, test MAE = 0.566735\n",
      "step: 9300, LR = 0.003041, min batch loss = 1.517076, test MAE = 0.565389\n",
      "step: 9400, LR = 0.003041, min batch loss = 0.875684, test MAE = 0.564663\n",
      "step: 9500, LR = 0.002737, min batch loss = 1.017165, test MAE = 0.570001\n",
      "step: 9600, LR = 0.002737, min batch loss = 0.612271, test MAE = 0.567442\n",
      "step: 9700, LR = 0.002737, min batch loss = 0.116335, test MAE = 0.570219\n",
      "step: 9800, LR = 0.002463, min batch loss = 0.455022, test MAE = 0.565084\n",
      "step: 9900, LR = 0.002463, min batch loss = 0.153841, test MAE = 0.564080\n",
      "3199.35233498\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "batch_size = 128\n",
    "steps = 10000\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    test_feed_dict = {inputs:test_data_n, labels:test_label[:,-1,:]}\n",
    "    for step in range(steps):\n",
    "        off = step * batch_size % (total_size - batch_size)\n",
    "        batch_data = train_data_n[off:off+batch_size, :, :]\n",
    "        batch_label = train_label[off:off+batch_size, -1, :]\n",
    "        feed_dict = {inputs:batch_data, labels:batch_label}\n",
    "        l, _, r = session.run([loss, op, learning_rate], feed_dict=feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            test_mae = MAE.eval(feed_dict=test_feed_dict)\n",
    "            print('step: %d, LR = %f, min batch loss = %f, test MAE = %f' % (step, r, l, test_mae))\n",
    "            #print('batch_data = %d %d %d, batch_label = %d, predicate = %f' % (batch_data[0, 0, 0],batch_data[0, 1, 0],batch_data[0, 2, 0], batch_label[0, 0], p))\n",
    "et = time.time()\n",
    "print(et - st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LSTM classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label1 = train_label.reshape(train_label.shape[0], train_label.shape[1])\n",
    "label2 = test_label.reshape(test_label.shape[0], test_label.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Do classification later\n",
    "from sklearn.preprocessing import Binarizer\n",
    "pre = Binarizer(threshold = 1.01)\n",
    "b_train_label = pre.transform(label1)\n",
    "b_test_label = pre.transform(label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10810,), (70656, 8), (565248, 1))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_train_label = 1 - b_train_label\n",
    "r, _ = (c_train_label == 1).nonzero()\n",
    "r.shape, c_train_label.shape, c_train_label.reshape(c_train_label.shape[0]*c_train_label.shape[1], 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_train_label = 1 - b_train_label\n",
    "c_test_label = 1 - b_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change from Indice to Vector\n",
    "''''''\n",
    "def makeIndicatorVars(T):\n",
    "    # Make sure T is two-dimensiona. Should be nSamples x 1.\n",
    "    if T.ndim == 1:\n",
    "        T = T.reshape((-1,1))    \n",
    "    return (T == np.unique(T)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_train_label_t = makeIndicatorVars(c_train_label.reshape(c_train_label.shape[0]*c_train_label.shape[1], 1))\n",
    "v_test_label_t = makeIndicatorVars(c_test_label.reshape(c_test_label.shape[0]*c_test_label.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v_train_label = v_train_label_t.reshape(c_train_label.shape[0], c_train_label.shape[1], 2)\n",
    "v_test_label = v_test_label_t.reshape(c_test_label.shape[0], c_test_label.shape[1], 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with dim 3. Estimator expected <= 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-5f2b42c8e7c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0munbalanced_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'regular'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_data_n_resample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_train_label_resample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv_train_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/htan/.conda/envs/py27/lib/python2.7/site-packages/unbalanced_dataset/base_sampler.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \"\"\"\n\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/htan/.conda/envs/py27/lib/python2.7/site-packages/unbalanced_dataset/over_sampling/smote.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \"\"\"\n\u001b[0;32m    217\u001b[0m         \u001b[1;31m# Check the consistency of X and y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;31m# Call the parent function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/htan/.conda/envs/py27/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    508\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    509\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    511\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m/home/htan/.conda/envs/py27/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nd\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n\u001b[1;32m--> 396\u001b[1;33m                              % (array.ndim, estimator_name))\n\u001b[0m\u001b[0;32m    397\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with dim 3. Estimator expected <= 2."
     ]
    }
   ],
   "source": [
    "#need to balanced the dataset\n",
    "from unbalanced_dataset.over_sampling import SMOTE\n",
    "sm = SMOTE(ratio = 0.1, kind='regular')\n",
    "train_data_n_resample, c_train_label_resample = sm.fit_transform(train_data_n, v_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shuffle the data set\n",
    "arr = np.arange(v_train_label.shape[0])\n",
    "np.random.shuffle(arr)\n",
    "train_data_n_s =  train_data_n[arr]\n",
    "train_label_v_s = v_train_label[arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 82\n",
    "n_steps = 8\n",
    "n_labels = 2\n",
    "n_hidden = 140\n",
    "total_size = v_train_label.shape[0]\n",
    "ratio = 0.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs = tf.placeholder(\"float32\", [None, n_steps, n_features])\n",
    "    labels = tf.placeholder(\"float32\", [None, n_labels])\n",
    "    \n",
    "    weights = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_features, n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_labels]))\n",
    "    }\n",
    "    biases = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_labels]))\n",
    "    }\n",
    "    \n",
    "    def acc(predict, label):\n",
    "        #correct_prediction = tf.equal(predicted_label, tf_train_label)\n",
    "        correct_prediction = tf.equal(tf.argmax(predict, 1), tf.argmax(label, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        predict_event = tf.reduce_sum(tf.argmax(predict, 1))\n",
    "        label_event = tf.reduce_sum(tf.argmax(label, 1))\n",
    "        true_positive = tf.reduce_sum(tf.cast(tf.equal((tf.argmax(predict, 1) + tf.argmax(label, 1)), 2), tf.int64))\n",
    "        true_negative = tf.reduce_sum(tf.cast(tf.equal((tf.argmax(predict, 1) + tf.argmax(label, 1)), 0), tf.int64))\n",
    "        false_positive = predict_event - true_positive \n",
    "        false_negative = label_event - true_positive\n",
    "        return accuracy, false_positive, false_negative, true_positive, true_negative\n",
    "    def ROC(FP, FN, TP, TN):\n",
    "        TP_percent = TP / (TP + FN) \n",
    "        FP_percent = FP / (FP + TN) \n",
    "        return TP_percent, FP_percent\n",
    "    \n",
    "    def PRC(FP, FN, TP, TN):\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        f_score = 2 * precision * recall / (precision + recall + 1)\n",
    "        return precision, recall, f_score\n",
    "    \n",
    "    def RNN(x, w, b):\n",
    "        # Prepare data shape to match `rnn` function requirements\n",
    "        # Current data input shape: (batch_size, n_steps, n_input)\n",
    "        # Required shape: 'n_steps' tensors list of shape (batch_size, n_hidden)\n",
    "    \n",
    "        # Permuting batch_size and n_steps\n",
    "        x = tf.transpose(x, [1, 0, 2])\n",
    "        # Reshaping to (n_steps*batch_size, n_input)\n",
    "        x = tf.reshape(x, [-1, n_features])\n",
    "        \n",
    "        # Linear activation\n",
    "        x = tf.matmul(x, w['hidden']) + b['hidden']\n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_hidden)\n",
    "        x = tf.split(0, n_steps, x)\n",
    "\n",
    "        # Define a lstm cell with tensorflow\n",
    "        lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "\n",
    "        # Get lstm cell output\n",
    "        outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "        # Linear activation, using rnn inner loop last output\n",
    "        return tf.matmul(outputs[-1], w['out']) + b['out']\n",
    "    \n",
    "    pred = RNN(inputs, weights, biases)\n",
    "    class_weight = tf.constant([ratio, 1.0 - ratio])\n",
    "    weighted_pred = tf.mul(pred, class_weight) # shape [batch_size, 2]\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(weighted_pred, labels)) # Softmax loss\n",
    "    \n",
    "    # Learning rate decay\n",
    "    global_step = tf.Variable(0)\n",
    "    starter_learning_rate = 0.15\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 500, 0.90 , staircase=True)\n",
    "    op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    \n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    test_acc, FP, FN, TP, TN = acc(pred, labels)\n",
    "    pre, rec, f_s = PRC(FP, FN, TP, TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "TP = 4, FP = 247, FN = 310, TN = 17103\n",
      "precision = 0.015936, recall = 0.012739, f_score = 0.000395\n",
      "TP = 61, FP = 66, FN = 253, TN = 17284\n",
      "precision = 0.480315, recall = 0.194268, f_score = 0.111442\n",
      "TP = 70, FP = 112, FN = 244, TN = 17238\n",
      "precision = 0.384615, recall = 0.222930, f_score = 0.106675\n",
      "TP = 192, FP = 422, FN = 122, TN = 16928\n",
      "precision = 0.312704, recall = 0.611465, f_score = 0.198743\n",
      "TP = 198, FP = 679, FN = 116, TN = 16671\n",
      "precision = 0.225770, recall = 0.630573, f_score = 0.153381\n",
      "TP = 169, FP = 386, FN = 145, TN = 16964\n",
      "precision = 0.304505, recall = 0.538217, f_score = 0.177878\n",
      "TP = 154, FP = 391, FN = 160, TN = 16959\n",
      "precision = 0.282569, recall = 0.490446, f_score = 0.156327\n",
      "TP = 205, FP = 587, FN = 109, TN = 16763\n",
      "precision = 0.258838, recall = 0.652866, f_score = 0.176792\n",
      "TP = 146, FP = 241, FN = 168, TN = 17109\n",
      "precision = 0.377261, recall = 0.464968, f_score = 0.190437\n",
      "TP = 219, FP = 649, FN = 95, TN = 16701\n",
      "precision = 0.252304, recall = 0.697452, f_score = 0.180505\n",
      "TP = 218, FP = 592, FN = 96, TN = 16758\n",
      "precision = 0.269136, recall = 0.694268, f_score = 0.190335\n",
      "TP = 214, FP = 476, FN = 100, TN = 16874\n",
      "precision = 0.310145, recall = 0.681529, f_score = 0.212256\n",
      "TP = 167, FP = 260, FN = 147, TN = 17090\n",
      "precision = 0.391101, recall = 0.531847, f_score = 0.216341\n",
      "TP = 152, FP = 210, FN = 162, TN = 17140\n",
      "precision = 0.419890, recall = 0.484076, f_score = 0.213511\n",
      "TP = 156, FP = 195, FN = 158, TN = 17155\n",
      "precision = 0.444444, recall = 0.496815, f_score = 0.227488\n",
      "TP = 115, FP = 235, FN = 199, TN = 17115\n",
      "precision = 0.328571, recall = 0.366242, f_score = 0.142006\n",
      "TP = 196, FP = 313, FN = 118, TN = 17037\n",
      "precision = 0.385069, recall = 0.624204, f_score = 0.239252\n",
      "TP = 218, FP = 441, FN = 96, TN = 16909\n",
      "precision = 0.330804, recall = 0.694268, f_score = 0.226823\n",
      "TP = 209, FP = 459, FN = 105, TN = 16891\n",
      "precision = 0.312874, recall = 0.665605, f_score = 0.210516\n",
      "TP = 177, FP = 337, FN = 137, TN = 17013\n",
      "precision = 0.344358, recall = 0.563694, f_score = 0.203467\n",
      "TP = 150, FP = 223, FN = 164, TN = 17127\n",
      "precision = 0.402145, recall = 0.477707, f_score = 0.204386\n",
      "TP = 200, FP = 415, FN = 114, TN = 16935\n",
      "precision = 0.325203, recall = 0.636943, f_score = 0.211132\n",
      "TP = 179, FP = 327, FN = 135, TN = 17023\n",
      "precision = 0.353755, recall = 0.570064, f_score = 0.209649\n",
      "TP = 198, FP = 353, FN = 116, TN = 16997\n",
      "precision = 0.359347, recall = 0.630573, f_score = 0.227742\n",
      "TP = 149, FP = 228, FN = 165, TN = 17122\n",
      "precision = 0.395225, recall = 0.474522, f_score = 0.200608\n",
      "TP = 167, FP = 244, FN = 147, TN = 17106\n",
      "precision = 0.406326, recall = 0.531847, f_score = 0.222997\n",
      "TP = 116, FP = 166, FN = 198, TN = 17184\n",
      "precision = 0.411348, recall = 0.369427, f_score = 0.170670\n",
      "TP = 126, FP = 182, FN = 188, TN = 17168\n",
      "precision = 0.409091, recall = 0.401274, f_score = 0.181353\n",
      "TP = 109, FP = 126, FN = 205, TN = 17224\n",
      "precision = 0.463830, recall = 0.347134, f_score = 0.177818\n",
      "TP = 194, FP = 343, FN = 120, TN = 17007\n",
      "precision = 0.361266, recall = 0.617834, f_score = 0.225560\n",
      "TP = 177, FP = 287, FN = 137, TN = 17063\n",
      "precision = 0.381466, recall = 0.563694, f_score = 0.221092\n",
      "TP = 131, FP = 167, FN = 183, TN = 17183\n",
      "precision = 0.439597, recall = 0.417197, f_score = 0.197544\n",
      "TP = 138, FP = 152, FN = 176, TN = 17198\n",
      "precision = 0.475862, recall = 0.439490, f_score = 0.218379\n",
      "TP = 128, FP = 155, FN = 186, TN = 17195\n",
      "precision = 0.452297, recall = 0.407643, f_score = 0.198260\n",
      "TP = 207, FP = 367, FN = 107, TN = 16983\n",
      "precision = 0.360627, recall = 0.659236, f_score = 0.235400\n",
      "TP = 169, FP = 258, FN = 145, TN = 17092\n",
      "precision = 0.395785, recall = 0.538217, f_score = 0.220287\n",
      "TP = 179, FP = 281, FN = 135, TN = 17069\n",
      "precision = 0.389130, recall = 0.570064, f_score = 0.226449\n",
      "TP = 111, FP = 122, FN = 203, TN = 17228\n",
      "precision = 0.476395, recall = 0.353503, f_score = 0.184062\n",
      "TP = 174, FP = 243, FN = 140, TN = 17107\n",
      "precision = 0.417266, recall = 0.554140, f_score = 0.234578\n",
      "TP = 137, FP = 153, FN = 177, TN = 17197\n",
      "precision = 0.472414, recall = 0.436306, f_score = 0.215974\n",
      "TP = 193, FP = 270, FN = 121, TN = 17080\n",
      "precision = 0.416847, recall = 0.614650, f_score = 0.252242\n",
      "TP = 183, FP = 296, FN = 131, TN = 17054\n",
      "precision = 0.382046, recall = 0.582803, f_score = 0.226641\n",
      "TP = 125, FP = 185, FN = 189, TN = 17165\n",
      "precision = 0.403226, recall = 0.398089, f_score = 0.178225\n",
      "TP = 165, FP = 235, FN = 149, TN = 17115\n",
      "precision = 0.412500, recall = 0.525478, f_score = 0.223697\n",
      "TP = 166, FP = 196, FN = 148, TN = 17154\n",
      "precision = 0.458564, recall = 0.528662, f_score = 0.243984\n",
      "TP = 203, FP = 298, FN = 111, TN = 17052\n",
      "precision = 0.405190, recall = 0.646497, f_score = 0.255355\n",
      "TP = 180, FP = 263, FN = 134, TN = 17087\n",
      "precision = 0.406321, recall = 0.573248, f_score = 0.235327\n",
      "TP = 153, FP = 182, FN = 161, TN = 17168\n",
      "precision = 0.456716, recall = 0.487261, f_score = 0.228953\n",
      "TP = 142, FP = 147, FN = 172, TN = 17203\n",
      "precision = 0.491349, recall = 0.452229, f_score = 0.228653\n",
      "TP = 177, FP = 224, FN = 137, TN = 17126\n",
      "precision = 0.441397, recall = 0.563694, f_score = 0.248181\n",
      "TP = 143, FP = 142, FN = 171, TN = 17208\n",
      "precision = 0.501754, recall = 0.455414, f_score = 0.233507\n",
      "TP = 182, FP = 248, FN = 132, TN = 17102\n",
      "precision = 0.423256, recall = 0.579618, f_score = 0.244975\n",
      "TP = 188, FP = 295, FN = 126, TN = 17055\n",
      "precision = 0.389234, recall = 0.598726, f_score = 0.234456\n",
      "TP = 144, FP = 173, FN = 170, TN = 17177\n",
      "precision = 0.454259, recall = 0.458599, f_score = 0.217813\n",
      "TP = 169, FP = 227, FN = 145, TN = 17123\n",
      "precision = 0.426768, recall = 0.538217, f_score = 0.233787\n",
      "TP = 159, FP = 177, FN = 155, TN = 17173\n",
      "precision = 0.473214, recall = 0.506369, f_score = 0.242093\n",
      "TP = 171, FP = 218, FN = 143, TN = 17132\n",
      "precision = 0.439589, recall = 0.544586, f_score = 0.241303\n",
      "TP = 189, FP = 272, FN = 125, TN = 17078\n",
      "precision = 0.409978, recall = 0.601911, f_score = 0.245312\n",
      "TP = 160, FP = 188, FN = 154, TN = 17162\n",
      "precision = 0.459770, recall = 0.509554, f_score = 0.237927\n",
      "TP = 151, FP = 164, FN = 163, TN = 17186\n",
      "precision = 0.479365, recall = 0.480892, f_score = 0.235196\n",
      "TP = 177, FP = 257, FN = 137, TN = 17093\n",
      "precision = 0.407834, recall = 0.563694, f_score = 0.233214\n",
      "TP = 152, FP = 163, FN = 162, TN = 17187\n",
      "precision = 0.482540, recall = 0.484076, f_score = 0.237551\n",
      "TP = 182, FP = 210, FN = 132, TN = 17140\n",
      "precision = 0.464286, recall = 0.579618, f_score = 0.263328\n",
      "TP = 186, FP = 269, FN = 128, TN = 17081\n",
      "precision = 0.408791, recall = 0.592357, f_score = 0.242011\n",
      "TP = 157, FP = 175, FN = 157, TN = 17175\n",
      "precision = 0.472892, recall = 0.500000, f_score = 0.239695\n",
      "TP = 159, FP = 174, FN = 155, TN = 17176\n",
      "precision = 0.477477, recall = 0.506369, f_score = 0.243749\n",
      "TP = 150, FP = 167, FN = 164, TN = 17183\n",
      "precision = 0.473186, recall = 0.477707, f_score = 0.231734\n",
      "TP = 164, FP = 181, FN = 150, TN = 17169\n",
      "precision = 0.475362, recall = 0.522293, f_score = 0.248570\n",
      "TP = 167, FP = 210, FN = 147, TN = 17140\n",
      "precision = 0.442971, recall = 0.531847, f_score = 0.238597\n",
      "TP = 162, FP = 201, FN = 152, TN = 17149\n",
      "precision = 0.446281, recall = 0.515924, f_score = 0.234682\n",
      "TP = 153, FP = 167, FN = 161, TN = 17183\n",
      "precision = 0.478125, recall = 0.487261, f_score = 0.237075\n",
      "TP = 168, FP = 218, FN = 146, TN = 17132\n",
      "precision = 0.435233, recall = 0.535032, f_score = 0.236378\n",
      "TP = 164, FP = 193, FN = 150, TN = 17157\n",
      "precision = 0.459384, recall = 0.522293, f_score = 0.242151\n",
      "TP = 190, FP = 237, FN = 124, TN = 17113\n",
      "precision = 0.444965, recall = 0.605096, f_score = 0.262672\n",
      "TP = 174, FP = 242, FN = 140, TN = 17108\n",
      "precision = 0.418269, recall = 0.554140, f_score = 0.235022\n",
      "TP = 168, FP = 211, FN = 146, TN = 17139\n",
      "precision = 0.443272, recall = 0.535032, f_score = 0.239766\n",
      "TP = 167, FP = 202, FN = 147, TN = 17148\n",
      "precision = 0.452575, recall = 0.531847, f_score = 0.242590\n",
      "TP = 154, FP = 167, FN = 160, TN = 17183\n",
      "precision = 0.479751, recall = 0.490446, f_score = 0.238851\n",
      "TP = 157, FP = 179, FN = 157, TN = 17171\n",
      "precision = 0.467262, recall = 0.500000, f_score = 0.237519\n",
      "TP = 168, FP = 221, FN = 146, TN = 17129\n",
      "precision = 0.431877, recall = 0.535032, f_score = 0.234955\n",
      "TP = 158, FP = 190, FN = 156, TN = 17160\n",
      "precision = 0.454023, recall = 0.503185, f_score = 0.233452\n",
      "TP = 153, FP = 164, FN = 161, TN = 17186\n",
      "precision = 0.482650, recall = 0.487261, f_score = 0.238769\n",
      "TP = 164, FP = 196, FN = 150, TN = 17154\n",
      "precision = 0.455556, recall = 0.522293, f_score = 0.240598\n",
      "TP = 163, FP = 182, FN = 151, TN = 17168\n",
      "precision = 0.472464, recall = 0.519108, f_score = 0.246298\n",
      "TP = 176, FP = 228, FN = 138, TN = 17122\n",
      "precision = 0.435644, recall = 0.560510, f_score = 0.244653\n",
      "TP = 166, FP = 232, FN = 148, TN = 17118\n",
      "precision = 0.417085, recall = 0.528662, f_score = 0.226645\n",
      "TP = 167, FP = 215, FN = 147, TN = 17135\n",
      "precision = 0.437173, recall = 0.531847, f_score = 0.236167\n",
      "TP = 158, FP = 185, FN = 156, TN = 17165\n",
      "precision = 0.460641, recall = 0.503185, f_score = 0.236057\n",
      "TP = 161, FP = 186, FN = 153, TN = 17164\n",
      "precision = 0.463977, recall = 0.512739, f_score = 0.240701\n",
      "TP = 147, FP = 169, FN = 167, TN = 17181\n",
      "precision = 0.465190, recall = 0.468153, f_score = 0.225289\n",
      "TP = 162, FP = 196, FN = 152, TN = 17154\n",
      "precision = 0.452514, recall = 0.515924, f_score = 0.237206\n",
      "TP = 159, FP = 186, FN = 155, TN = 17164\n",
      "precision = 0.460870, recall = 0.506369, f_score = 0.237257\n",
      "TP = 151, FP = 176, FN = 163, TN = 17174\n",
      "precision = 0.461774, recall = 0.480892, f_score = 0.228617\n",
      "TP = 165, FP = 195, FN = 149, TN = 17155\n",
      "precision = 0.458333, recall = 0.525478, f_score = 0.242809\n",
      "TP = 161, FP = 187, FN = 153, TN = 17163\n",
      "precision = 0.462644, recall = 0.512739, f_score = 0.240172\n",
      "TP = 174, FP = 218, FN = 140, TN = 17132\n",
      "precision = 0.443878, recall = 0.554140, f_score = 0.246214\n",
      "TP = 163, FP = 218, FN = 151, TN = 17132\n",
      "precision = 0.427822, recall = 0.519108, f_score = 0.228139\n",
      "TP = 164, FP = 201, FN = 150, TN = 17149\n",
      "precision = 0.449315, recall = 0.522293, f_score = 0.238054\n",
      "TP = 158, FP = 183, FN = 156, TN = 17167\n",
      "precision = 0.463343, recall = 0.503185, f_score = 0.237116\n",
      "TP = 157, FP = 186, FN = 157, TN = 17164\n",
      "precision = 0.457726, recall = 0.500000, f_score = 0.233805\n",
      "719.876992941\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "steps = 10000\n",
    "st = time.time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    test_feed_dict = {inputs:test_data_n, labels:v_test_label[:,-1,:]}\n",
    "    for step in range(steps):\n",
    "        off = step * batch_size % (total_size - batch_size)\n",
    "        batch_data = train_data_n_s[off:off+batch_size, :, :]\n",
    "        batch_label = train_label_v_s[off:off+batch_size, -1, :]\n",
    "        feed_dict = {inputs:batch_data, labels:batch_label}\n",
    "        l, _, r = session.run([loss, op, learning_rate], feed_dict=feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            tp, fp, fn, tn, precision, recall, f_score= session.run([TP, FP, FN, TN, pre, rec, f_s], feed_dict=test_feed_dict)\n",
    "            print(\"TP = %d, FP = %d, FN = %d, TN = %d\" % (tp, fp, fn, tn))\n",
    "            print(\"precision = %f, recall = %f, f_score = %f\" % (precision, recall, f_score))\n",
    "            #print('batch_data = %d %d %d, batch_label = %d, predicate = %f' % (batch_data[0, 0, 0],batch_data[0, 1, 0],batch_data[0, 2, 0], batch_label[0, 0], p))\n",
    "            \n",
    "et = time.time()\n",
    "print(et-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
