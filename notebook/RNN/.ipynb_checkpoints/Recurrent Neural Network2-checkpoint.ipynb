{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, Image\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASOS_alone.pickle\t     CC_Under_sSample.pickle\r\n",
      "ASOS+NWP.pickle\t\t     KORD_RUC_RAP_Hourly_20051031-20150301.csv\r\n",
      "ASOS+NWP_time_serial.pickle  rough_visibility.pickle\r\n",
      "ASOS_time_serial.pickle      visibilityDataNoLagsNewPreds.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls '/home/htan/proj/TensorFlow/data/visibility/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load data first\n",
    "pickle_file = '/home/htan/proj/TensorFlow/data/visibility/' +  'ASOS+NWP_time_serial.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    #train_dataset= save['train_dataset']\n",
    "    #validate_dataset = save['validate_dataset']\n",
    "    test_dataset = save['test_dataset']\n",
    "    #test_old = save['v_t_dataset']\n",
    "    train_old = save['t_v_dataset']\n",
    "    del save\n",
    "\n",
    "#train_time = train_dataset['time']\n",
    "#train_data = train_dataset['data']\n",
    "#train_label = train_dataset['label']\n",
    "#validate_time = validate_dataset['time']\n",
    "#validate_data = validate_dataset['data']\n",
    "#validate_label = validate_dataset['label']\n",
    "#test_time = test_dataset['time']\n",
    "test_data = test_dataset['data']\n",
    "test_label = test_dataset['label']\n",
    "#test_old_data = test_old['data']\n",
    "#test_old_label = test_old['label']\n",
    "train_old_data = train_old['data']\n",
    "train_old_label = train_old['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29993, 8, 70) (29993, 8, 1)\n",
      "(119966, 8, 70) (119966, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape, test_label.shape)\n",
    "print(train_old_data.shape, train_old_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset normalize\n",
    "mean = train_old_data.mean(axis = (0,1))\n",
    "std = train_old_data.std(axis = (0,1))\n",
    "print(mean.shape, std.shape)\n",
    "train_data_n = (train_old_data - mean)/std\n",
    "#validate_data_n = (validate_data - mean)/std\n",
    "test_data_n = (test_data - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = train_old_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LSTM regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = test_label[:10, 7, :].shape\n",
    "print(total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 70\n",
    "n_steps = 8\n",
    "n_labels = 1\n",
    "\n",
    "n_hidden = 140\n",
    "total_size = train_old_label.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs = tf.placeholder(\"float32\", [None, n_steps, n_features])\n",
    "    labels = tf.placeholder(\"float32\", [None, n_labels])\n",
    "    \n",
    "    weights = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_features, n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_labels]))\n",
    "    }\n",
    "    biases = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_labels]))\n",
    "    }\n",
    "    \n",
    "    def RNN(x, w, b):\n",
    "        # Prepare data shape to match `rnn` function requirements\n",
    "        # Current data input shape: (batch_size, n_steps, n_input)\n",
    "        # Required shape: 'n_steps' tensors list of shape (batch_size, n_hidden)\n",
    "    \n",
    "        # Permuting batch_size and n_steps\n",
    "        x = tf.transpose(x, [1, 0, 2])\n",
    "        # Reshaping to (n_steps*batch_size, n_input)\n",
    "        x = tf.reshape(x, [-1, n_features])\n",
    "        \n",
    "        # Linear activation\n",
    "        x = tf.matmul(x, w['hidden']) + b['hidden']\n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_hidden)\n",
    "        x = tf.split(0, n_steps, x)\n",
    "\n",
    "        # Define a lstm cell with tensorflow\n",
    "        lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "\n",
    "        # Get lstm cell output\n",
    "        outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "        # Linear activation, using rnn inner loop last output\n",
    "        return tf.matmul(outputs[-1], w['out']) + b['out']\n",
    "    \n",
    "    pred = RNN(inputs, weights, biases)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    loss = tf.reduce_mean(tf.square(pred - labels))\n",
    "    \n",
    "    # Learning rate decay\n",
    "    global_step = tf.Variable(0)\n",
    "    starter_learning_rate = 0.05\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 500, 0.90, staircase=True)\n",
    "    op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    MAE = tf.reduce_mean(tf.abs(pred - labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "steps = 10000\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    test_feed_dict = {inputs:test_data_n, labels:test_label[:,-1,:]}\n",
    "    for step in range(steps):\n",
    "        off = step * batch_size % (total_size - batch_size)\n",
    "        batch_data = train_data_n[off:off+batch_size, :, :]\n",
    "        batch_label = train_label[off:off+batch_size, -1, :]\n",
    "        feed_dict = {inputs:batch_data, labels:batch_label}\n",
    "        l, _, r = session.run([loss, op, learning_rate], feed_dict=feed_dict)\n",
    "        if step % 10 == 0:\n",
    "            test_mae = MAE.eval(feed_dict=test_feed_dict)\n",
    "            print('step: %d, LR = %f, min batch loss = %f, test MAE = %f' % (step, r, l, test_mae))\n",
    "            #print('batch_data = %d %d %d, batch_label = %d, predicate = %f' % (batch_data[0, 0, 0],batch_data[0, 1, 0],batch_data[0, 2, 0], batch_label[0, 0], p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LSTM classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label1 = train_label.reshape(train_label.shape[0], train_label.shape[1])\n",
    "label2 = test_label.reshape(test_label.shape[0], test_label.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Do classification later\n",
    "from sklearn.preprocessing import Binarizer\n",
    "pre = Binarizer(threshold = 1.01)\n",
    "b_train_label = pre.transform(label1)\n",
    "b_test_label = pre.transform(label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_train_label = 1 - b_train_label\n",
    "r, _ = (c_train_label == 1).nonzero()\n",
    "r.shape, c_train_label.shape, c_train_label.reshape(c_train_label.shape[0]*c_train_label.shape[1], 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_train_label = 1 - b_train_label\n",
    "c_test_label = 1 - b_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change from Indice to Vector\n",
    "''''''\n",
    "def makeIndicatorVars(T):\n",
    "    # Make sure T is two-dimensiona. Should be nSamples x 1.\n",
    "    if T.ndim == 1:\n",
    "        T = T.reshape((-1,1))    \n",
    "    return (T == np.unique(T)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_train_label_t = makeIndicatorVars(c_train_label.reshape(c_train_label.shape[0]*c_train_label.shape[1], 1))\n",
    "v_test_label_t = makeIndicatorVars(c_test_label.reshape(c_test_label.shape[0]*c_test_label.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v_train_label = v_train_label_t.reshape(c_train_label.shape[0], c_train_label.shape[1], 2)\n",
    "v_test_label = v_test_label_t.reshape(c_test_label.shape[0], c_test_label.shape[1], 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#need to balanced the dataset\n",
    "from unbalanced_dataset.over_sampling import SMOTE\n",
    "sm = SMOTE(ratio = 0.1, kind='regular')\n",
    "train_data_n_resample, c_train_label_resample = sm.fit_transform(train_data_n, v_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shuffle the data set\n",
    "arr = np.arange(v_train_label.shape[0])\n",
    "np.random.shuffle(arr)\n",
    "train_data_n_s =  train_data_n[arr]\n",
    "train_label_v_s = v_train_label[arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 70\n",
    "n_steps = 8\n",
    "n_labels = 2\n",
    "n_hidden = 210\n",
    "total_size = v_train_label.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs = tf.placeholder(\"float32\", [None, n_steps, n_features])\n",
    "    labels = tf.placeholder(\"float32\", [None, n_labels])\n",
    "    \n",
    "    weights = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_features, n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden, n_labels]))\n",
    "    }\n",
    "    biases = {\n",
    "        'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_labels]))\n",
    "    }\n",
    "    \n",
    "    def acc(predict, label):\n",
    "        #correct_prediction = tf.equal(predicted_label, tf_train_label)\n",
    "        correct_prediction = tf.equal(tf.argmax(predict, 1), tf.argmax(label, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        predict_event = tf.reduce_sum(tf.argmax(predict, 1))\n",
    "        label_event = tf.reduce_sum(tf.argmax(label, 1))\n",
    "        true_positive = tf.reduce_sum(tf.cast(tf.equal((tf.argmax(predict, 1) + tf.argmax(label, 1)), 2), tf.int64))\n",
    "        true_negative = tf.reduce_sum(tf.cast(tf.equal((tf.argmax(predict, 1) + tf.argmax(label, 1)), 0), tf.int64))\n",
    "        false_positive = predict_event - true_positive \n",
    "        false_negative = label_event - true_positive\n",
    "        return accuracy, false_positive, false_negative, true_positive, true_negative\n",
    "    def ROC(FP, FN, TP, TN):\n",
    "        TP_percent = TP / (TP + FN) \n",
    "        FP_percent = FP / (FP + TN) \n",
    "        return TP_percent, FP_percent\n",
    "    \n",
    "    def PRC(FP, FN, TP, TN):\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        f_score = 2 * precision * recall / (precision + recall + 1)\n",
    "        return precision, recall, f_score\n",
    "    \n",
    "    def RNN(x, w, b):\n",
    "        # Prepare data shape to match `rnn` function requirements\n",
    "        # Current data input shape: (batch_size, n_steps, n_input)\n",
    "        # Required shape: 'n_steps' tensors list of shape (batch_size, n_hidden)\n",
    "    \n",
    "        # Permuting batch_size and n_steps\n",
    "        x = tf.transpose(x, [1, 0, 2])\n",
    "        # Reshaping to (n_steps*batch_size, n_input)\n",
    "        x = tf.reshape(x, [-1, n_features])\n",
    "        \n",
    "        # Linear activation\n",
    "        x = tf.matmul(x, w['hidden']) + b['hidden']\n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_hidden)\n",
    "        x = tf.split(0, n_steps, x)\n",
    "\n",
    "        # Define a lstm cell with tensorflow\n",
    "        lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "\n",
    "        # Get lstm cell output\n",
    "        outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "        # Linear activation, using rnn inner loop last output\n",
    "        return tf.matmul(outputs[-1], w['out']) + b['out']\n",
    "    \n",
    "    pred = RNN(inputs, weights, biases)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, labels)) # Softmax loss\n",
    "    \n",
    "    # Learning rate decay\n",
    "    global_step = tf.Variable(0)\n",
    "    starter_learning_rate = 0.03\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 500, 0.90 , staircase=True)\n",
    "    op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    \n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    test_acc, FP, FN, TP, TN = acc(pred, labels)\n",
    "    pre, rec, f_s = PRC(FP, FN, TP, TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "steps = 10000\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    test_feed_dict = {inputs:test_data_n, labels:v_test_label[:,-1,:]}\n",
    "    for step in range(steps):\n",
    "        off = step * batch_size % (total_size - batch_size)\n",
    "        batch_data = train_data_n_s[off:off+batch_size, :, :]\n",
    "        batch_label = train_label_v_s[off:off+batch_size, -1, :]\n",
    "        feed_dict = {inputs:batch_data, labels:batch_label}\n",
    "        l, _, r = session.run([loss, op, learning_rate], feed_dict=feed_dict)\n",
    "        if step % 10 == 0:\n",
    "            acc = accuracy.eval(feed_dict=feed_dict)\n",
    "            print('step: %d, LR = %f, min batch loss = %f, train acc = %f' % (step, r, l, acc))\n",
    "        if step % 100 == 0:\n",
    "            tp, fp, fn, tn, precision, recall, f_score= session.run([TP, FP, FN, TN, pre, rec, f_s], feed_dict=test_feed_dict)\n",
    "            print(\"TP = %d, FP = %d, FN = %d, TN = %d\" % (tp, fp, fn, tn))\n",
    "            print(\"precision = %f, recall = %f, f_score = %f\" % (precision, recall, f_score))\n",
    "            #print('batch_data = %d %d %d, batch_label = %d, predicate = %f' % (batch_data[0, 0, 0],batch_data[0, 1, 0],batch_data[0, 2, 0], batch_label[0, 0], p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
