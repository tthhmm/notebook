{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, Image\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load data first\n",
    "pickle_file = '/home/htan/proj/TensorFlow/data/visibility/' +  'ASOS+NWP.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    test_dataset = save['test_dataset']\n",
    "    train_old = save['t_v_dataset']\n",
    "    del save\n",
    "\n",
    "\n",
    "test_data_1 = test_dataset['data_ASOS']\n",
    "test_data_2 = test_dataset['data_NWP']\n",
    "test_label = test_dataset['label']\n",
    "train_data_1 = train_old['data_ASOS']\n",
    "train_data_2 = train_old['data_NWP']\n",
    "train_label = train_old['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116657, 82), (29165, 82))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.hstack((train_data_1, train_data_2))\n",
    "test_data = np.hstack((test_data_1, test_data_2))\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82,) (82,)\n"
     ]
    }
   ],
   "source": [
    "#dataset normalize\n",
    "mean = train_data.mean(axis = 0)\n",
    "std = train_data.std(axis = 0)\n",
    "print(mean.shape, std.shape)\n",
    "train_data_n = (train_data - mean)/std\n",
    "test_data_n = (test_data - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfering the regression problem to classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "pre = Binarizer(threshold = 1.01)\n",
    "b_train_label = pre.transform(train_label.reshape(1, -1))\n",
    "b_test_label = pre.transform(test_label.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_train_label = 1 - b_train_label[0]\n",
    "\n",
    "c_test_label = 1 - b_test_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#shuffle the data set\n",
    "arr = np.arange(c_train_label.shape[0])\n",
    "np.random.shuffle(arr)\n",
    "train_data_n_s =  train_data_n[arr]\n",
    "c_train_label_s = c_train_label[arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining classes statistics... 2 classes detected: Counter({0.0: 114677, 1.0: 1980})\n",
      "Finding the 5 nearest neighbours...\n",
      "done!\n",
      "Creating synthetic samples...Generated 1460 new samples ...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#need to balanced the dataset\n",
    "from unbalanced_dataset.over_sampling import SMOTE\n",
    "sm = SMOTE(ratio = 0.03, kind='regular')\n",
    "train_data_n_resample, c_train_label_resample = sm.fit_transform(train_data_n, c_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#shuffle the data set\n",
    "arr = np.arange(c_train_label_resample.shape[0])\n",
    "np.random.shuffle(arr)\n",
    "train_data_n_resample_s =  train_data_n_resample[arr]\n",
    "c_train_label_resample_s = c_train_label_resample[arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_train_label_s[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change from Indice to Vector\n",
    "''''''\n",
    "def makeIndicatorVars(T):\n",
    "    # Make sure T is two-dimensiona. Should be nSamples x 1.\n",
    "    if T.ndim == 1:\n",
    "        T = T.reshape((-1,1))    \n",
    "    return (T == np.unique(T)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_train_label_s = makeIndicatorVars(c_train_label_s.reshape(-1, 1))\n",
    "v_test_label = makeIndicatorVars(c_test_label.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v_train_label_resample_s = makeIndicatorVars(c_train_label_resample_s.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write create simple linearLogistic Regression model\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(256, 82))\n",
    "    tf_train_label = tf.placeholder(tf.float32, shape=(256,2))\n",
    "    tf_validate_dataset = tf.constant(test_data_n.astype(np.float32)[:, :])\n",
    "    tf_validate_label = tf.constant(v_test_label.astype(np.float32)[:])\n",
    "    #tf_test_dataset1 = tf.constant(test_data_n.astype(np.float32)[:30000, :])\n",
    "    #tf_test_label1 = tf.constant(test_label.astype(np.float32)[:30000])\n",
    "    #tf_test_dataset2 = tf.constant(test_data_n.astype(np.float32)[30000:, :])\n",
    "    #tf_test_label2 = tf.constant(test_label.astype(np.float32)[30000:])\n",
    "    weights1 = tf.Variable(tf.truncated_normal([82, 10]))\n",
    "    biases1 = tf.Variable(tf.zeros([10]))\n",
    "    weights2 = tf.Variable(tf.truncated_normal([10, 2]))\n",
    "    biases2 = tf.Variable(tf.zeros([2]))\n",
    "    \n",
    "    def acc(predict, label):\n",
    "        #correct_prediction = tf.equal(predicted_label, tf_train_label)\n",
    "        correct_prediction = tf.equal(tf.argmax(predict, 1), tf.argmax(label, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        predict_event = tf.reduce_sum(tf.argmax(predict, 1))\n",
    "        label_event = tf.reduce_sum(tf.argmax(label, 1))\n",
    "        true_positive = tf.reduce_sum(tf.cast(tf.equal((tf.argmax(predict, 1) + tf.argmax(label, 1)), 2), tf.int64))\n",
    "        true_negative = tf.reduce_sum(tf.cast(tf.equal((tf.argmax(predict, 1) + tf.argmax(label, 1)), 0), tf.int64))\n",
    "        false_positive = predict_event - true_positive \n",
    "        false_negative = label_event - true_positive\n",
    "        return accuracy, false_positive, false_negative, true_positive, true_negative\n",
    "    def ROC(FP, FN, TP, TN):\n",
    "        TP_percent = TP / (TP + FN)\n",
    "        FP_percent = FP / (FP + TN)\n",
    "        return TP_percent, FP_percent\n",
    "    \n",
    "    def PRC(FP, FN, TP, TN):\n",
    "        precision = TP / (TP + FP)\n",
    "        recall = TP / (TP + FN)\n",
    "        f_score = 2 * precision * recall / (precision + recall)\n",
    "        return precision, recall, f_score\n",
    "    \n",
    "    def train_model(X):\n",
    "        hidden = tf.nn.relu6(tf.matmul(X, weights1) + biases1)\n",
    "        #hidden = tf.nn.dropout(hidden, 1.0)\n",
    "        return tf.nn.softmax(tf.matmul(hidden, weights2) + biases2)\n",
    "    \n",
    "    def eval_model(X):\n",
    "        hidden = tf.nn.relu6(tf.matmul(X, weights1) + biases1)\n",
    "        return tf.nn.softmax(tf.matmul(hidden, weights2) + biases2)\n",
    "\n",
    "    predicted_label = train_model(tf_train_dataset)\n",
    "    #loss = tf.reduce_mean(tf.square(predicted_label - tf_train_label))\n",
    "    loss = tf.reduce_mean(-tf.reduce_sum(tf_train_label * tf.log(predicted_label), reduction_indices=[1]))\n",
    "\n",
    "    # Learning rate decay\n",
    "    global_step = tf.Variable(0)\n",
    "    starter_learning_rate = 0.05\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 2500, 0.95, staircase=True)\n",
    "    op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "\n",
    "    \n",
    "    train_acc, _, _, _, _ = acc(predicted_label, tf_train_label)\n",
    "    validate_acc, FP, FN, TP, TN = acc(eval_model(tf_validate_dataset), tf_validate_label)\n",
    "    t_p, f_p = ROC(FP, FN, TP, TN)\n",
    "    pre, rec, f_s = PRC(FP, FN, TP, TN)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #train_loss = tf.reduce_mean(tf.abs(predicted_label - tf_train_label))\n",
    "    #test_loss1 = tf.reduce_mean(tf.abs(model(tf_test_dataset1, weights, biases) - tf_test_label1))\n",
    "    #test_loss2 = tf.reduce_mean(tf.abs(model(tf_test_dataset2, weights, biases) - tf_test_label2))\n",
    "    #test_loss = (test_loss1 * 30000 + test_loss2 * (test_size - 30000)) / test_size\n",
    "    #validate_loss = tf.reduce_mean(tf.abs(model(tf_validate_dataset, weights, biases) - tf_validate_label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0 : learning_rate = 0.050000, loss = 1.581797\n",
      "0 4047 421 24697\n",
      "precision = 0.000000, recall = 0.000000, f_score = nan\n",
      "Loss at step 1000 : learning_rate = 0.050000, loss = 0.059674\n",
      "26 112 395 28632\n",
      "precision = 0.188406, recall = 0.061758, f_score = 0.093023\n",
      "Loss at step 2000 : learning_rate = 0.050000, loss = 0.035964\n",
      "142 335 279 28409\n",
      "precision = 0.297694, recall = 0.337292, f_score = 0.316258\n",
      "Loss at step 3000 : learning_rate = 0.047500, loss = 0.060819\n",
      "189 340 232 28404\n",
      "precision = 0.357278, recall = 0.448931, f_score = 0.397895\n",
      "Loss at step 4000 : learning_rate = 0.047500, loss = 0.063874\n",
      "211 408 210 28336\n",
      "precision = 0.340872, recall = 0.501188, f_score = 0.405769\n",
      "Loss at step 5000 : learning_rate = 0.045125, loss = 0.065149\n",
      "180 232 241 28512\n",
      "precision = 0.436893, recall = 0.427553, f_score = 0.432173\n",
      "Loss at step 6000 : learning_rate = 0.045125, loss = 0.053525\n",
      "201 307 220 28437\n",
      "precision = 0.395669, recall = 0.477435, f_score = 0.432723\n",
      "Loss at step 7000 : learning_rate = 0.045125, loss = 0.052109\n",
      "205 323 216 28421\n",
      "precision = 0.388258, recall = 0.486936, f_score = 0.432034\n",
      "Loss at step 8000 : learning_rate = 0.042869, loss = 0.057131\n",
      "189 229 232 28515\n",
      "precision = 0.452153, recall = 0.448931, f_score = 0.450536\n",
      "Loss at step 9000 : learning_rate = 0.042869, loss = 0.031305\n",
      "207 308 214 28436\n",
      "precision = 0.401942, recall = 0.491686, f_score = 0.442308\n",
      "Loss at step 10000 : learning_rate = 0.040725, loss = 0.035268\n",
      "195 230 226 28514\n",
      "precision = 0.458824, recall = 0.463183, f_score = 0.460993\n",
      "Loss at step 11000 : learning_rate = 0.040725, loss = 0.032482\n",
      "208 285 213 28459\n",
      "precision = 0.421907, recall = 0.494062, f_score = 0.455142\n",
      "Loss at step 12000 : learning_rate = 0.040725, loss = 0.115250\n",
      "209 292 212 28452\n",
      "precision = 0.417166, recall = 0.496437, f_score = 0.453362\n",
      "Loss at step 13000 : learning_rate = 0.038689, loss = 0.071720\n",
      "195 235 226 28509\n",
      "precision = 0.453488, recall = 0.463183, f_score = 0.458284\n",
      "Loss at step 14000 : learning_rate = 0.038689, loss = 0.076505\n",
      "209 278 212 28466\n",
      "precision = 0.429158, recall = 0.496437, f_score = 0.460352\n",
      "Loss at step 15000 : learning_rate = 0.036755, loss = 0.031663\n",
      "189 189 232 28555\n",
      "precision = 0.500000, recall = 0.448931, f_score = 0.473091\n",
      "Loss at step 16000 : learning_rate = 0.036755, loss = 0.050653\n",
      "213 286 208 28458\n",
      "precision = 0.426854, recall = 0.505938, f_score = 0.463043\n",
      "Loss at step 17000 : learning_rate = 0.036755, loss = 0.068960\n",
      "209 272 212 28472\n",
      "precision = 0.434511, recall = 0.496437, f_score = 0.463415\n",
      "Loss at step 18000 : learning_rate = 0.034917, loss = 0.031336\n",
      "195 213 226 28531\n",
      "precision = 0.477941, recall = 0.463183, f_score = 0.470446\n",
      "Loss at step 19000 : learning_rate = 0.034917, loss = 0.040662\n",
      "209 262 212 28482\n",
      "precision = 0.443737, recall = 0.496437, f_score = 0.468610\n",
      "Loss at step 20000 : learning_rate = 0.033171, loss = 0.059856\n",
      "198 206 223 28538\n",
      "precision = 0.490099, recall = 0.470309, f_score = 0.480000\n",
      "Loss at step 21000 : learning_rate = 0.033171, loss = 0.045171\n",
      "210 262 211 28482\n",
      "precision = 0.444915, recall = 0.498812, f_score = 0.470325\n",
      "Loss at step 22000 : learning_rate = 0.033171, loss = 0.067780\n",
      "214 257 207 28487\n",
      "precision = 0.454352, recall = 0.508314, f_score = 0.479821\n",
      "Loss at step 23000 : learning_rate = 0.031512, loss = 0.063738\n",
      "201 221 220 28523\n",
      "precision = 0.476303, recall = 0.477435, f_score = 0.476868\n",
      "Loss at step 24000 : learning_rate = 0.031512, loss = 0.056719\n",
      "207 229 214 28515\n",
      "precision = 0.474771, recall = 0.491686, f_score = 0.483081\n",
      "Loss at step 25000 : learning_rate = 0.029937, loss = 0.051466\n",
      "198 204 223 28540\n",
      "precision = 0.492537, recall = 0.470309, f_score = 0.481166\n",
      "Loss at step 26000 : learning_rate = 0.029937, loss = 0.066917\n",
      "214 253 207 28491\n",
      "precision = 0.458244, recall = 0.508314, f_score = 0.481982\n",
      "Loss at step 27000 : learning_rate = 0.029937, loss = 0.035371\n",
      "215 253 206 28491\n",
      "precision = 0.459402, recall = 0.510689, f_score = 0.483690\n",
      "Loss at step 28000 : learning_rate = 0.028440, loss = 0.027436\n",
      "204 213 217 28531\n",
      "precision = 0.489209, recall = 0.484561, f_score = 0.486874\n",
      "Loss at step 29000 : learning_rate = 0.028440, loss = 0.049272\n",
      "207 221 214 28523\n",
      "precision = 0.483645, recall = 0.491686, f_score = 0.487633\n",
      "Loss at step 30000 : learning_rate = 0.027018, loss = 0.034917\n",
      "201 206 220 28538\n",
      "precision = 0.493857, recall = 0.477435, f_score = 0.485507\n",
      "Loss at step 31000 : learning_rate = 0.027018, loss = 0.059902\n",
      "215 238 206 28506\n",
      "precision = 0.474614, recall = 0.510689, f_score = 0.491991\n",
      "Loss at step 32000 : learning_rate = 0.027018, loss = 0.072665\n",
      "214 237 207 28507\n",
      "precision = 0.474501, recall = 0.508314, f_score = 0.490826\n",
      "Loss at step 33000 : learning_rate = 0.025667, loss = 0.055684\n",
      "202 207 219 28537\n",
      "precision = 0.493888, recall = 0.479810, f_score = 0.486747\n",
      "Loss at step 34000 : learning_rate = 0.025667, loss = 0.043005\n",
      "206 213 215 28531\n",
      "precision = 0.491647, recall = 0.489311, f_score = 0.490476\n",
      "Loss at step 35000 : learning_rate = 0.024384, loss = 0.053513\n",
      "205 204 216 28540\n",
      "precision = 0.501222, recall = 0.486936, f_score = 0.493976\n",
      "Loss at step 36000 : learning_rate = 0.024384, loss = 0.058431\n",
      "220 241 201 28503\n",
      "precision = 0.477223, recall = 0.522565, f_score = 0.498866\n",
      "Loss at step 37000 : learning_rate = 0.024384, loss = 0.073356\n",
      "222 234 199 28510\n",
      "precision = 0.486842, recall = 0.527316, f_score = 0.506271\n",
      "Loss at step 38000 : learning_rate = 0.023165, loss = 0.047502\n",
      "208 203 213 28541\n",
      "precision = 0.506083, recall = 0.494062, f_score = 0.500000\n",
      "Loss at step 39000 : learning_rate = 0.023165, loss = 0.060663\n",
      "207 202 214 28542\n",
      "precision = 0.506112, recall = 0.491686, f_score = 0.498795\n",
      "Loss at step 40000 : learning_rate = 0.022006, loss = 0.042864\n",
      "207 200 214 28544\n",
      "precision = 0.508600, recall = 0.491686, f_score = 0.500000\n",
      "Loss at step 41000 : learning_rate = 0.022006, loss = 0.076376\n",
      "223 228 198 28516\n",
      "precision = 0.494457, recall = 0.529691, f_score = 0.511468\n",
      "Loss at step 42000 : learning_rate = 0.022006, loss = 0.037615\n",
      "230 229 191 28515\n",
      "precision = 0.501089, recall = 0.546318, f_score = 0.522727\n",
      "Loss at step 43000 : learning_rate = 0.020906, loss = 0.061922\n",
      "210 200 211 28544\n",
      "precision = 0.512195, recall = 0.498812, f_score = 0.505415\n",
      "Loss at step 44000 : learning_rate = 0.020906, loss = 0.029082\n",
      "205 192 216 28552\n",
      "precision = 0.516373, recall = 0.486936, f_score = 0.501222\n",
      "Loss at step 45000 : learning_rate = 0.019861, loss = 0.034383\n",
      "206 193 215 28551\n",
      "precision = 0.516291, recall = 0.489311, f_score = 0.502439\n",
      "Loss at step 46000 : learning_rate = 0.019861, loss = 0.071740\n",
      "218 212 203 28532\n",
      "precision = 0.506977, recall = 0.517815, f_score = 0.512338\n",
      "Loss at step 47000 : learning_rate = 0.019861, loss = 0.058123\n",
      "231 227 190 28517\n",
      "precision = 0.504367, recall = 0.548694, f_score = 0.525597\n",
      "Loss at step 48000 : learning_rate = 0.018868, loss = 0.044564\n",
      "208 181 213 28563\n",
      "precision = 0.534704, recall = 0.494062, f_score = 0.513580\n",
      "Loss at step 49000 : learning_rate = 0.018868, loss = 0.032548\n",
      "206 180 215 28564\n",
      "precision = 0.533679, recall = 0.489311, f_score = 0.510533\n",
      "Loss at step 50000 : learning_rate = 0.017924, loss = 0.034034\n",
      "220 189 201 28555\n",
      "precision = 0.537897, recall = 0.522565, f_score = 0.530120\n",
      "Loss at step 51000 : learning_rate = 0.017924, loss = 0.046392\n",
      "226 208 195 28536\n",
      "precision = 0.520737, recall = 0.536817, f_score = 0.528655\n",
      "Loss at step 52000 : learning_rate = 0.017924, loss = 0.029845\n",
      "228 205 193 28539\n",
      "precision = 0.526559, recall = 0.541568, f_score = 0.533958\n",
      "Loss at step 53000 : learning_rate = 0.017028, loss = 0.129236\n",
      "220 186 201 28558\n",
      "precision = 0.541872, recall = 0.522565, f_score = 0.532044\n",
      "Loss at step 54000 : learning_rate = 0.017028, loss = 0.034260\n",
      "220 182 201 28562\n",
      "precision = 0.547264, recall = 0.522565, f_score = 0.534629\n",
      "Loss at step 55000 : learning_rate = 0.016177, loss = 0.059202\n",
      "224 190 197 28554\n",
      "precision = 0.541063, recall = 0.532067, f_score = 0.536527\n",
      "Loss at step 56000 : learning_rate = 0.016177, loss = 0.026237\n",
      "226 198 195 28546\n",
      "precision = 0.533019, recall = 0.536817, f_score = 0.534911\n",
      "Loss at step 57000 : learning_rate = 0.016177, loss = 0.074751\n",
      "228 201 193 28543\n",
      "precision = 0.531469, recall = 0.541568, f_score = 0.536471\n",
      "Loss at step 58000 : learning_rate = 0.015368, loss = 0.035592\n",
      "224 187 197 28557\n",
      "precision = 0.545012, recall = 0.532067, f_score = 0.538462\n",
      "Loss at step 59000 : learning_rate = 0.015368, loss = 0.047264\n",
      "222 180 199 28564\n",
      "precision = 0.552239, recall = 0.527316, f_score = 0.539490\n",
      "Loss at step 60000 : learning_rate = 0.014599, loss = 0.022256\n",
      "226 193 195 28551\n",
      "precision = 0.539379, recall = 0.536817, f_score = 0.538095\n",
      "Loss at step 61000 : learning_rate = 0.014599, loss = 0.028283\n",
      "228 195 193 28549\n",
      "precision = 0.539007, recall = 0.541568, f_score = 0.540284\n",
      "Loss at step 62000 : learning_rate = 0.014599, loss = 0.041539\n",
      "230 206 191 28538\n",
      "precision = 0.527523, recall = 0.546318, f_score = 0.536756\n",
      "Loss at step 63000 : learning_rate = 0.013869, loss = 0.068052\n",
      "227 192 194 28552\n",
      "precision = 0.541766, recall = 0.539192, f_score = 0.540476\n",
      "Loss at step 64000 : learning_rate = 0.013869, loss = 0.047892\n",
      "221 178 200 28566\n",
      "precision = 0.553885, recall = 0.524941, f_score = 0.539024\n",
      "Loss at step 65000 : learning_rate = 0.013176, loss = 0.060512\n",
      "225 184 196 28560\n",
      "precision = 0.550122, recall = 0.534442, f_score = 0.542169\n",
      "Loss at step 66000 : learning_rate = 0.013176, loss = 0.040062\n",
      "227 188 194 28556\n",
      "precision = 0.546988, recall = 0.539192, f_score = 0.543062\n",
      "Loss at step 67000 : learning_rate = 0.013176, loss = 0.021055\n",
      "229 205 192 28539\n",
      "precision = 0.527650, recall = 0.543943, f_score = 0.535673\n",
      "Loss at step 68000 : learning_rate = 0.012517, loss = 0.090472\n",
      "228 182 193 28562\n",
      "precision = 0.556098, recall = 0.541568, f_score = 0.548736\n",
      "Loss at step 69000 : learning_rate = 0.012517, loss = 0.024705\n",
      "226 174 195 28570\n",
      "precision = 0.565000, recall = 0.536817, f_score = 0.550548\n",
      "Loss at step 70000 : learning_rate = 0.011891, loss = 0.014144\n",
      "227 179 194 28565\n",
      "precision = 0.559113, recall = 0.539192, f_score = 0.548972\n",
      "Loss at step 71000 : learning_rate = 0.011891, loss = 0.064369\n",
      "229 185 192 28559\n",
      "precision = 0.553140, recall = 0.543943, f_score = 0.548503\n",
      "Loss at step 72000 : learning_rate = 0.011891, loss = 0.040925\n",
      "231 204 190 28540\n",
      "precision = 0.531034, recall = 0.548694, f_score = 0.539720\n",
      "Loss at step 73000 : learning_rate = 0.011297, loss = 0.038870\n",
      "228 178 193 28566\n",
      "precision = 0.561576, recall = 0.541568, f_score = 0.551391\n",
      "Loss at step 74000 : learning_rate = 0.011297, loss = 0.046365\n",
      "227 179 194 28565\n",
      "precision = 0.559113, recall = 0.539192, f_score = 0.548972\n",
      "Loss at step 75000 : learning_rate = 0.010732, loss = 0.061771\n",
      "227 170 194 28574\n",
      "precision = 0.571788, recall = 0.539192, f_score = 0.555012\n",
      "Loss at step 76000 : learning_rate = 0.010732, loss = 0.068013\n",
      "228 176 193 28568\n",
      "precision = 0.564356, recall = 0.541568, f_score = 0.552727\n",
      "Loss at step 77000 : learning_rate = 0.010732, loss = 0.058965\n",
      "231 197 190 28547\n",
      "precision = 0.539720, recall = 0.548694, f_score = 0.544170\n",
      "Loss at step 78000 : learning_rate = 0.010195, loss = 0.036204\n",
      "228 185 193 28559\n",
      "precision = 0.552058, recall = 0.541568, f_score = 0.546763\n",
      "Loss at step 79000 : learning_rate = 0.010195, loss = 0.026478\n",
      "228 173 193 28571\n",
      "precision = 0.568579, recall = 0.541568, f_score = 0.554745\n",
      "Loss at step 80000 : learning_rate = 0.009686, loss = 0.064451\n",
      "227 173 194 28571\n",
      "precision = 0.567500, recall = 0.539192, f_score = 0.552984\n",
      "Loss at step 81000 : learning_rate = 0.009686, loss = 0.046628\n",
      "228 176 193 28568\n",
      "precision = 0.564356, recall = 0.541568, f_score = 0.552727\n",
      "Loss at step 82000 : learning_rate = 0.009686, loss = 0.040939\n",
      "233 201 188 28543\n",
      "precision = 0.536866, recall = 0.553444, f_score = 0.545029\n",
      "Loss at step 83000 : learning_rate = 0.009201, loss = 0.027426\n",
      "231 191 190 28553\n",
      "precision = 0.547393, recall = 0.548694, f_score = 0.548043\n",
      "Loss at step 84000 : learning_rate = 0.009201, loss = 0.043633\n",
      "228 175 193 28569\n",
      "precision = 0.565757, recall = 0.541568, f_score = 0.553398\n",
      "Loss at step 85000 : learning_rate = 0.008741, loss = 0.068988\n",
      "227 173 194 28571\n",
      "precision = 0.567500, recall = 0.539192, f_score = 0.552984\n",
      "Loss at step 86000 : learning_rate = 0.008741, loss = 0.054603\n",
      "228 175 193 28569\n",
      "precision = 0.565757, recall = 0.541568, f_score = 0.553398\n",
      "Loss at step 87000 : learning_rate = 0.008741, loss = 0.046653\n",
      "233 197 188 28547\n",
      "precision = 0.541860, recall = 0.553444, f_score = 0.547591\n",
      "Loss at step 88000 : learning_rate = 0.008304, loss = 0.028302\n",
      "233 196 188 28548\n",
      "precision = 0.543124, recall = 0.553444, f_score = 0.548235\n",
      "Loss at step 89000 : learning_rate = 0.008304, loss = 0.074359\n",
      "228 175 193 28569\n",
      "precision = 0.565757, recall = 0.541568, f_score = 0.553398\n",
      "Loss at step 90000 : learning_rate = 0.007889, loss = 0.021822\n",
      "229 176 192 28568\n",
      "precision = 0.565432, recall = 0.543943, f_score = 0.554479\n",
      "Loss at step 91000 : learning_rate = 0.007889, loss = 0.025278\n",
      "229 175 192 28569\n",
      "precision = 0.566832, recall = 0.543943, f_score = 0.555152\n",
      "Loss at step 92000 : learning_rate = 0.007889, loss = 0.031817\n",
      "232 190 189 28554\n",
      "precision = 0.549763, recall = 0.551069, f_score = 0.550415\n",
      "Loss at step 93000 : learning_rate = 0.007495, loss = 0.052191\n",
      "235 192 186 28552\n",
      "precision = 0.550351, recall = 0.558195, f_score = 0.554245\n",
      "Loss at step 94000 : learning_rate = 0.007495, loss = 0.085265\n",
      "229 173 192 28571\n",
      "precision = 0.569652, recall = 0.543943, f_score = 0.556501\n",
      "Loss at step 95000 : learning_rate = 0.007120, loss = 0.019215\n",
      "229 176 192 28568\n",
      "precision = 0.565432, recall = 0.543943, f_score = 0.554479\n",
      "Loss at step 96000 : learning_rate = 0.007120, loss = 0.025164\n",
      "229 176 192 28568\n",
      "precision = 0.565432, recall = 0.543943, f_score = 0.554479\n",
      "Loss at step 97000 : learning_rate = 0.007120, loss = 0.026672\n",
      "234 192 187 28552\n",
      "precision = 0.549296, recall = 0.555819, f_score = 0.552538\n",
      "Loss at step 98000 : learning_rate = 0.006764, loss = 0.021387\n",
      "234 192 187 28552\n",
      "precision = 0.549296, recall = 0.555819, f_score = 0.552538\n",
      "Loss at step 99000 : learning_rate = 0.006764, loss = 0.028225\n",
      "230 178 191 28566\n",
      "precision = 0.563725, recall = 0.546318, f_score = 0.554885\n",
      "Loss at step 100000 : learning_rate = 0.006426, loss = 0.073154\n",
      "229 175 192 28569\n",
      "precision = 0.566832, recall = 0.543943, f_score = 0.555152\n",
      "Loss at step 101000 : learning_rate = 0.006426, loss = 0.048829\n",
      "230 172 191 28572\n",
      "precision = 0.572139, recall = 0.546318, f_score = 0.558931\n",
      "Loss at step 102000 : learning_rate = 0.006426, loss = 0.041181\n",
      "234 194 187 28550\n",
      "precision = 0.546729, recall = 0.555819, f_score = 0.551237\n",
      "Loss at step 103000 : learning_rate = 0.006104, loss = 0.048306\n",
      "234 194 187 28550\n",
      "precision = 0.546729, recall = 0.555819, f_score = 0.551237\n",
      "Loss at step 104000 : learning_rate = 0.006104, loss = 0.034360\n",
      "230 181 191 28563\n",
      "precision = 0.559611, recall = 0.546318, f_score = 0.552885\n",
      "Loss at step 105000 : learning_rate = 0.005799, loss = 0.043617\n",
      "228 175 193 28569\n",
      "precision = 0.565757, recall = 0.541568, f_score = 0.553398\n",
      "Loss at step 106000 : learning_rate = 0.005799, loss = 0.063224\n",
      "227 171 194 28573\n",
      "precision = 0.570352, recall = 0.539192, f_score = 0.554335\n",
      "Loss at step 107000 : learning_rate = 0.005799, loss = 0.062509\n",
      "233 193 188 28551\n",
      "precision = 0.546948, recall = 0.553444, f_score = 0.550177\n",
      "Loss at step 108000 : learning_rate = 0.005509, loss = 0.077841\n",
      "236 194 185 28550\n",
      "precision = 0.548837, recall = 0.560570, f_score = 0.554642\n",
      "Loss at step 109000 : learning_rate = 0.005509, loss = 0.021601\n",
      "229 180 192 28564\n",
      "precision = 0.559902, recall = 0.543943, f_score = 0.551807\n",
      "Loss at step 110000 : learning_rate = 0.005234, loss = 0.019507\n",
      "226 172 195 28572\n",
      "precision = 0.567839, recall = 0.536817, f_score = 0.551893\n",
      "Loss at step 111000 : learning_rate = 0.005234, loss = 0.032116\n",
      "226 170 195 28574\n",
      "precision = 0.570707, recall = 0.536817, f_score = 0.553244\n",
      "Loss at step 112000 : learning_rate = 0.005234, loss = 0.034211\n",
      "234 193 187 28551\n",
      "precision = 0.548009, recall = 0.555819, f_score = 0.551887\n",
      "Loss at step 113000 : learning_rate = 0.004972, loss = 0.029361\n",
      "234 193 187 28551\n",
      "precision = 0.548009, recall = 0.555819, f_score = 0.551887\n",
      "Loss at step 114000 : learning_rate = 0.004972, loss = 0.042082\n",
      "228 177 193 28567\n",
      "precision = 0.562963, recall = 0.541568, f_score = 0.552058\n",
      "Loss at step 115000 : learning_rate = 0.004723, loss = 0.030653\n",
      "227 171 194 28573\n",
      "precision = 0.570352, recall = 0.539192, f_score = 0.554335\n",
      "Loss at step 116000 : learning_rate = 0.004723, loss = 0.024731\n",
      "226 166 195 28578\n",
      "precision = 0.576531, recall = 0.536817, f_score = 0.555966\n",
      "Loss at step 117000 : learning_rate = 0.004723, loss = 0.097349\n",
      "233 193 188 28551\n",
      "precision = 0.546948, recall = 0.553444, f_score = 0.550177\n",
      "Loss at step 118000 : learning_rate = 0.004487, loss = 0.045799\n",
      "234 193 187 28551\n",
      "precision = 0.548009, recall = 0.555819, f_score = 0.551887\n",
      "Loss at step 119000 : learning_rate = 0.004487, loss = 0.022992\n",
      "227 173 194 28571\n",
      "precision = 0.567500, recall = 0.539192, f_score = 0.552984\n",
      "Loss at step 120000 : learning_rate = 0.004263, loss = 0.054796\n",
      "227 170 194 28574\n",
      "precision = 0.571788, recall = 0.539192, f_score = 0.555012\n",
      "Loss at step 121000 : learning_rate = 0.004263, loss = 0.026436\n",
      "226 168 195 28576\n",
      "precision = 0.573604, recall = 0.536817, f_score = 0.554601\n",
      "Loss at step 122000 : learning_rate = 0.004263, loss = 0.046066\n",
      "232 189 189 28555\n",
      "precision = 0.551069, recall = 0.551069, f_score = 0.551069\n",
      "Loss at step 123000 : learning_rate = 0.004050, loss = 0.059662\n",
      "236 193 185 28551\n",
      "precision = 0.550117, recall = 0.560570, f_score = 0.555294\n",
      "Loss at step 124000 : learning_rate = 0.004050, loss = 0.055387\n",
      "228 174 193 28570\n",
      "precision = 0.567164, recall = 0.541568, f_score = 0.554070\n",
      "Loss at step 125000 : learning_rate = 0.003847, loss = 0.033936\n",
      "227 172 194 28572\n",
      "precision = 0.568922, recall = 0.539192, f_score = 0.553659\n",
      "Loss at step 126000 : learning_rate = 0.003847, loss = 0.057464\n",
      "226 166 195 28578\n",
      "precision = 0.576531, recall = 0.536817, f_score = 0.555966\n",
      "Loss at step 127000 : learning_rate = 0.003847, loss = 0.063278\n",
      "231 183 190 28561\n",
      "precision = 0.557971, recall = 0.548694, f_score = 0.553293\n",
      "Loss at step 128000 : learning_rate = 0.003655, loss = 0.070053\n",
      "235 191 186 28553\n",
      "precision = 0.551643, recall = 0.558195, f_score = 0.554900\n",
      "Loss at step 129000 : learning_rate = 0.003655, loss = 0.031090\n",
      "228 175 193 28569\n",
      "precision = 0.565757, recall = 0.541568, f_score = 0.553398\n",
      "Loss at step 130000 : learning_rate = 0.003472, loss = 0.028908\n",
      "228 174 193 28570\n",
      "precision = 0.567164, recall = 0.541568, f_score = 0.554070\n",
      "Loss at step 131000 : learning_rate = 0.003472, loss = 0.059689\n",
      "226 166 195 28578\n",
      "precision = 0.576531, recall = 0.536817, f_score = 0.555966\n",
      "Loss at step 132000 : learning_rate = 0.003472, loss = 0.056087\n",
      "231 180 190 28564\n",
      "precision = 0.562044, recall = 0.548694, f_score = 0.555288\n",
      "Loss at step 133000 : learning_rate = 0.003299, loss = 0.047287\n",
      "233 190 188 28554\n",
      "precision = 0.550827, recall = 0.553444, f_score = 0.552133\n",
      "Loss at step 134000 : learning_rate = 0.003299, loss = 0.041299\n",
      "228 175 193 28569\n",
      "precision = 0.565757, recall = 0.541568, f_score = 0.553398\n",
      "Loss at step 135000 : learning_rate = 0.003134, loss = 0.025328\n",
      "228 173 193 28571\n",
      "precision = 0.568579, recall = 0.541568, f_score = 0.554745\n",
      "Loss at step 136000 : learning_rate = 0.003134, loss = 0.020310\n",
      "226 164 195 28580\n",
      "precision = 0.579487, recall = 0.536817, f_score = 0.557337\n",
      "Loss at step 137000 : learning_rate = 0.003134, loss = 0.039141\n",
      "230 176 191 28568\n",
      "precision = 0.566502, recall = 0.546318, f_score = 0.556227\n",
      "Loss at step 138000 : learning_rate = 0.002977, loss = 0.063179\n",
      "233 190 188 28554\n",
      "precision = 0.550827, recall = 0.553444, f_score = 0.552133\n",
      "Loss at step 139000 : learning_rate = 0.002977, loss = 0.082572\n",
      "229 174 192 28570\n",
      "precision = 0.568238, recall = 0.543943, f_score = 0.555825\n",
      "Loss at step 140000 : learning_rate = 0.002828, loss = 0.064132\n",
      "228 172 193 28572\n",
      "precision = 0.570000, recall = 0.541568, f_score = 0.555420\n",
      "Loss at step 141000 : learning_rate = 0.002828, loss = 0.050441\n",
      "226 164 195 28580\n",
      "precision = 0.579487, recall = 0.536817, f_score = 0.557337\n",
      "Loss at step 142000 : learning_rate = 0.002828, loss = 0.019380\n",
      "230 176 191 28568\n",
      "precision = 0.566502, recall = 0.546318, f_score = 0.556227\n",
      "Loss at step 143000 : learning_rate = 0.002687, loss = 0.047897\n",
      "232 189 189 28555\n",
      "precision = 0.551069, recall = 0.551069, f_score = 0.551069\n",
      "Loss at step 144000 : learning_rate = 0.002687, loss = 0.083028\n",
      "229 172 192 28572\n",
      "precision = 0.571072, recall = 0.543943, f_score = 0.557178\n",
      "Loss at step 145000 : learning_rate = 0.002552, loss = 0.068713\n",
      "228 171 193 28573\n",
      "precision = 0.571429, recall = 0.541568, f_score = 0.556098\n",
      "Loss at step 146000 : learning_rate = 0.002552, loss = 0.054385\n",
      "226 162 195 28582\n",
      "precision = 0.582474, recall = 0.536817, f_score = 0.558714\n",
      "Loss at step 147000 : learning_rate = 0.002552, loss = 0.028796\n",
      "230 175 191 28569\n",
      "precision = 0.567901, recall = 0.546318, f_score = 0.556901\n",
      "Loss at step 148000 : learning_rate = 0.002425, loss = 0.071841\n",
      "232 182 189 28562\n",
      "precision = 0.560386, recall = 0.551069, f_score = 0.555689\n",
      "Loss at step 149000 : learning_rate = 0.002425, loss = 0.072390\n",
      "229 172 192 28572\n",
      "precision = 0.571072, recall = 0.543943, f_score = 0.557178\n",
      "Loss at step 150000 : learning_rate = 0.002303, loss = 0.022225\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 151000 : learning_rate = 0.002303, loss = 0.024495\n",
      "226 162 195 28582\n",
      "precision = 0.582474, recall = 0.536817, f_score = 0.558714\n",
      "Loss at step 152000 : learning_rate = 0.002303, loss = 0.019101\n",
      "230 174 191 28570\n",
      "precision = 0.569307, recall = 0.546318, f_score = 0.557576\n",
      "Loss at step 153000 : learning_rate = 0.002188, loss = 0.018768\n",
      "232 179 189 28565\n",
      "precision = 0.564477, recall = 0.551069, f_score = 0.557692\n",
      "Loss at step 154000 : learning_rate = 0.002188, loss = 0.042884\n",
      "229 171 192 28573\n",
      "precision = 0.572500, recall = 0.543943, f_score = 0.557856\n",
      "Loss at step 155000 : learning_rate = 0.002079, loss = 0.083463\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 156000 : learning_rate = 0.002079, loss = 0.019326\n",
      "225 165 196 28579\n",
      "precision = 0.576923, recall = 0.534442, f_score = 0.554871\n",
      "Loss at step 157000 : learning_rate = 0.002079, loss = 0.036076\n",
      "230 173 191 28571\n",
      "precision = 0.570720, recall = 0.546318, f_score = 0.558252\n",
      "Loss at step 158000 : learning_rate = 0.001975, loss = 0.048930\n",
      "232 177 189 28567\n",
      "precision = 0.567237, recall = 0.551069, f_score = 0.559036\n",
      "Loss at step 159000 : learning_rate = 0.001975, loss = 0.019462\n",
      "230 172 191 28572\n",
      "precision = 0.572139, recall = 0.546318, f_score = 0.558931\n",
      "Loss at step 160000 : learning_rate = 0.001876, loss = 0.021887\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 161000 : learning_rate = 0.001876, loss = 0.015076\n",
      "225 166 196 28578\n",
      "precision = 0.575448, recall = 0.534442, f_score = 0.554187\n",
      "Loss at step 162000 : learning_rate = 0.001876, loss = 0.042554\n",
      "230 172 191 28572\n",
      "precision = 0.572139, recall = 0.546318, f_score = 0.558931\n",
      "Loss at step 163000 : learning_rate = 0.001782, loss = 0.047261\n",
      "232 176 189 28568\n",
      "precision = 0.568627, recall = 0.551069, f_score = 0.559710\n",
      "Loss at step 164000 : learning_rate = 0.001782, loss = 0.042926\n",
      "230 172 191 28572\n",
      "precision = 0.572139, recall = 0.546318, f_score = 0.558931\n",
      "Loss at step 165000 : learning_rate = 0.001693, loss = 0.020803\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 166000 : learning_rate = 0.001693, loss = 0.050706\n",
      "225 167 196 28577\n",
      "precision = 0.573980, recall = 0.534442, f_score = 0.553506\n",
      "Loss at step 167000 : learning_rate = 0.001693, loss = 0.052477\n",
      "230 172 191 28572\n",
      "precision = 0.572139, recall = 0.546318, f_score = 0.558931\n",
      "Loss at step 168000 : learning_rate = 0.001609, loss = 0.049286\n",
      "231 175 190 28569\n",
      "precision = 0.568966, recall = 0.548694, f_score = 0.558646\n",
      "Loss at step 169000 : learning_rate = 0.001609, loss = 0.030157\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 170000 : learning_rate = 0.001528, loss = 0.091484\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 171000 : learning_rate = 0.001528, loss = 0.056055\n",
      "225 167 196 28577\n",
      "precision = 0.573980, recall = 0.534442, f_score = 0.553506\n",
      "Loss at step 172000 : learning_rate = 0.001528, loss = 0.040803\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 173000 : learning_rate = 0.001452, loss = 0.047251\n",
      "231 175 190 28569\n",
      "precision = 0.568966, recall = 0.548694, f_score = 0.558646\n",
      "Loss at step 174000 : learning_rate = 0.001452, loss = 0.067473\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 175000 : learning_rate = 0.001379, loss = 0.019584\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 176000 : learning_rate = 0.001379, loss = 0.059709\n",
      "225 167 196 28577\n",
      "precision = 0.573980, recall = 0.534442, f_score = 0.553506\n",
      "Loss at step 177000 : learning_rate = 0.001379, loss = 0.063383\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 178000 : learning_rate = 0.001310, loss = 0.044190\n",
      "231 173 190 28571\n",
      "precision = 0.571782, recall = 0.548694, f_score = 0.560000\n",
      "Loss at step 179000 : learning_rate = 0.001310, loss = 0.078629\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 180000 : learning_rate = 0.001245, loss = 0.056555\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 181000 : learning_rate = 0.001245, loss = 0.042201\n",
      "226 167 195 28577\n",
      "precision = 0.575064, recall = 0.536817, f_score = 0.555283\n",
      "Loss at step 182000 : learning_rate = 0.001245, loss = 0.076794\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 183000 : learning_rate = 0.001182, loss = 0.042738\n",
      "231 172 190 28572\n",
      "precision = 0.573201, recall = 0.548694, f_score = 0.560680\n",
      "Loss at step 184000 : learning_rate = 0.001182, loss = 0.041328\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 185000 : learning_rate = 0.001123, loss = 0.033865\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 186000 : learning_rate = 0.001123, loss = 0.060400\n",
      "226 168 195 28576\n",
      "precision = 0.573604, recall = 0.536817, f_score = 0.554601\n",
      "Loss at step 187000 : learning_rate = 0.001123, loss = 0.069940\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 188000 : learning_rate = 0.001067, loss = 0.024632\n",
      "231 172 190 28572\n",
      "precision = 0.573201, recall = 0.548694, f_score = 0.560680\n",
      "Loss at step 189000 : learning_rate = 0.001067, loss = 0.037935\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 190000 : learning_rate = 0.001014, loss = 0.039370\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 191000 : learning_rate = 0.001014, loss = 0.082839\n",
      "226 168 195 28576\n",
      "precision = 0.573604, recall = 0.536817, f_score = 0.554601\n",
      "Loss at step 192000 : learning_rate = 0.001014, loss = 0.019871\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 193000 : learning_rate = 0.000963, loss = 0.053740\n",
      "232 172 189 28572\n",
      "precision = 0.574257, recall = 0.551069, f_score = 0.562424\n",
      "Loss at step 194000 : learning_rate = 0.000963, loss = 0.030868\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 195000 : learning_rate = 0.000915, loss = 0.042048\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 196000 : learning_rate = 0.000915, loss = 0.064289\n",
      "226 168 195 28576\n",
      "precision = 0.573604, recall = 0.536817, f_score = 0.554601\n",
      "Loss at step 197000 : learning_rate = 0.000915, loss = 0.037321\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 198000 : learning_rate = 0.000869, loss = 0.032825\n",
      "232 172 189 28572\n",
      "precision = 0.574257, recall = 0.551069, f_score = 0.562424\n",
      "Loss at step 199000 : learning_rate = 0.000869, loss = 0.048266\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 200000 : learning_rate = 0.000826, loss = 0.083030\n",
      "229 170 192 28574\n",
      "precision = 0.573935, recall = 0.543943, f_score = 0.558537\n",
      "Loss at step 201000 : learning_rate = 0.000826, loss = 0.026348\n",
      "226 168 195 28576\n",
      "precision = 0.573604, recall = 0.536817, f_score = 0.554601\n",
      "Loss at step 202000 : learning_rate = 0.000826, loss = 0.042727\n",
      "229 171 192 28573\n",
      "precision = 0.572500, recall = 0.543943, f_score = 0.557856\n",
      "Loss at step 203000 : learning_rate = 0.000784, loss = 0.027971\n",
      "232 172 189 28572\n",
      "precision = 0.574257, recall = 0.551069, f_score = 0.562424\n",
      "Loss at step 204000 : learning_rate = 0.000784, loss = 0.030366\n",
      "229 171 192 28573\n",
      "precision = 0.572500, recall = 0.543943, f_score = 0.557856\n",
      "Loss at step 205000 : learning_rate = 0.000745, loss = 0.044491\n",
      "229 170 192 28574\n",
      "precision = 0.573935, recall = 0.543943, f_score = 0.558537\n",
      "Loss at step 206000 : learning_rate = 0.000745, loss = 0.074167\n",
      "226 170 195 28574\n",
      "precision = 0.570707, recall = 0.536817, f_score = 0.553244\n",
      "Loss at step 207000 : learning_rate = 0.000745, loss = 0.027268\n",
      "229 170 192 28574\n",
      "precision = 0.573935, recall = 0.543943, f_score = 0.558537\n",
      "Loss at step 208000 : learning_rate = 0.000708, loss = 0.029515\n",
      "232 172 189 28572\n",
      "precision = 0.574257, recall = 0.551069, f_score = 0.562424\n",
      "Loss at step 209000 : learning_rate = 0.000708, loss = 0.028625\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 210000 : learning_rate = 0.000673, loss = 0.037808\n",
      "229 170 192 28574\n",
      "precision = 0.573935, recall = 0.543943, f_score = 0.558537\n",
      "Loss at step 211000 : learning_rate = 0.000673, loss = 0.024792\n",
      "226 168 195 28576\n",
      "precision = 0.573604, recall = 0.536817, f_score = 0.554601\n",
      "Loss at step 212000 : learning_rate = 0.000673, loss = 0.056293\n",
      "228 170 193 28574\n",
      "precision = 0.572864, recall = 0.541568, f_score = 0.556777\n",
      "Loss at step 213000 : learning_rate = 0.000639, loss = 0.029245\n",
      "232 172 189 28572\n",
      "precision = 0.574257, recall = 0.551069, f_score = 0.562424\n",
      "Loss at step 214000 : learning_rate = 0.000639, loss = 0.091176\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 215000 : learning_rate = 0.000607, loss = 0.067425\n",
      "230 170 191 28574\n",
      "precision = 0.575000, recall = 0.546318, f_score = 0.560292\n",
      "Loss at step 216000 : learning_rate = 0.000607, loss = 0.046449\n",
      "226 169 195 28575\n",
      "precision = 0.572152, recall = 0.536817, f_score = 0.553922\n",
      "Loss at step 217000 : learning_rate = 0.000607, loss = 0.029084\n",
      "228 170 193 28574\n",
      "precision = 0.572864, recall = 0.541568, f_score = 0.556777\n",
      "Loss at step 218000 : learning_rate = 0.000577, loss = 0.043866\n",
      "231 172 190 28572\n",
      "precision = 0.573201, recall = 0.548694, f_score = 0.560680\n",
      "Loss at step 219000 : learning_rate = 0.000577, loss = 0.042557\n",
      "231 171 190 28573\n",
      "precision = 0.574627, recall = 0.548694, f_score = 0.561361\n",
      "Loss at step 220000 : learning_rate = 0.000548, loss = 0.043988\n",
      "230 170 191 28574\n",
      "precision = 0.575000, recall = 0.546318, f_score = 0.560292\n",
      "Loss at step 221000 : learning_rate = 0.000548, loss = 0.030937\n",
      "226 169 195 28575\n",
      "precision = 0.572152, recall = 0.536817, f_score = 0.553922\n",
      "Loss at step 222000 : learning_rate = 0.000548, loss = 0.022429\n",
      "229 170 192 28574\n",
      "precision = 0.573935, recall = 0.543943, f_score = 0.558537\n",
      "Loss at step 223000 : learning_rate = 0.000520, loss = 0.105210\n",
      "231 172 190 28572\n",
      "precision = 0.573201, recall = 0.548694, f_score = 0.560680\n",
      "Loss at step 224000 : learning_rate = 0.000520, loss = 0.029698\n",
      "230 171 191 28573\n",
      "precision = 0.573566, recall = 0.546318, f_score = 0.559611\n",
      "Loss at step 225000 : learning_rate = 0.000494, loss = 0.026423\n",
      "231 171 190 28573\n",
      "precision = 0.574627, recall = 0.548694, f_score = 0.561361\n",
      "Loss at step 226000 : learning_rate = 0.000494, loss = 0.055086\n",
      "226 169 195 28575\n",
      "precision = 0.572152, recall = 0.536817, f_score = 0.553922\n",
      "Loss at step 227000 : learning_rate = 0.000494, loss = 0.046391\n",
      "229 170 192 28574\n",
      "precision = 0.573935, recall = 0.543943, f_score = 0.558537\n",
      "Loss at step 228000 : learning_rate = 0.000470, loss = 0.031323\n",
      "231 172 190 28572\n",
      "precision = 0.573201, recall = 0.548694, f_score = 0.560680\n",
      "Loss at step 229000 : learning_rate = 0.000470, loss = 0.038063\n",
      "231 170 190 28574\n",
      "precision = 0.576060, recall = 0.548694, f_score = 0.562044\n",
      "Loss at step 230000 : learning_rate = 0.000446, loss = 0.046551\n",
      "231 171 190 28573\n",
      "precision = 0.574627, recall = 0.548694, f_score = 0.561361\n",
      "Loss at step 231000 : learning_rate = 0.000446, loss = 0.025248\n",
      "227 169 194 28575\n",
      "precision = 0.573232, recall = 0.539192, f_score = 0.555692\n",
      "Loss at step 232000 : learning_rate = 0.000446, loss = 0.021789\n",
      "229 170 192 28574\n",
      "precision = 0.573935, recall = 0.543943, f_score = 0.558537\n",
      "Loss at step 233000 : learning_rate = 0.000424, loss = 0.076862\n",
      "231 172 190 28572\n",
      "precision = 0.573201, recall = 0.548694, f_score = 0.560680\n",
      "Loss at step 234000 : learning_rate = 0.000424, loss = 0.022236\n",
      "231 170 190 28574\n",
      "precision = 0.576060, recall = 0.548694, f_score = 0.562044\n",
      "Loss at step 235000 : learning_rate = 0.000403, loss = 0.031480\n",
      "231 171 190 28573\n",
      "precision = 0.574627, recall = 0.548694, f_score = 0.561361\n",
      "Loss at step 236000 : learning_rate = 0.000403, loss = 0.015418\n",
      "227 168 194 28576\n",
      "precision = 0.574684, recall = 0.539192, f_score = 0.556373\n",
      "Loss at step 237000 : learning_rate = 0.000403, loss = 0.028789\n",
      "229 170 192 28574\n",
      "precision = 0.573935, recall = 0.543943, f_score = 0.558537\n",
      "Loss at step 238000 : learning_rate = 0.000383, loss = 0.029928\n",
      "231 171 190 28573\n",
      "precision = 0.574627, recall = 0.548694, f_score = 0.561361\n",
      "Loss at step 239000 : learning_rate = 0.000383, loss = 0.040782\n",
      "231 170 190 28574\n",
      "precision = 0.576060, recall = 0.548694, f_score = 0.562044\n",
      "Loss at step 240000 : learning_rate = 0.000363, loss = 0.075244\n",
      "231 170 190 28574\n",
      "precision = 0.576060, recall = 0.548694, f_score = 0.562044\n",
      "Loss at step 241000 : learning_rate = 0.000363, loss = 0.031776\n",
      "227 169 194 28575\n",
      "precision = 0.573232, recall = 0.539192, f_score = 0.555692\n",
      "Loss at step 242000 : learning_rate = 0.000363, loss = 0.055363\n",
      "229 170 192 28574\n",
      "precision = 0.573935, recall = 0.543943, f_score = 0.558537\n",
      "Loss at step 243000 : learning_rate = 0.000345, loss = 0.032853\n",
      "231 170 190 28574\n",
      "precision = 0.576060, recall = 0.548694, f_score = 0.562044\n",
      "Loss at step 244000 : learning_rate = 0.000345, loss = 0.029259\n",
      "230 170 191 28574\n",
      "precision = 0.575000, recall = 0.546318, f_score = 0.560292\n",
      "Loss at step 245000 : learning_rate = 0.000328, loss = 0.046160\n",
      "231 170 190 28574\n",
      "precision = 0.576060, recall = 0.548694, f_score = 0.562044\n",
      "Loss at step 246000 : learning_rate = 0.000328, loss = 0.047620\n",
      "227 169 194 28575\n",
      "precision = 0.573232, recall = 0.539192, f_score = 0.555692\n",
      "Loss at step 247000 : learning_rate = 0.000328, loss = 0.050471\n",
      "229 169 192 28575\n",
      "precision = 0.575377, recall = 0.543943, f_score = 0.559219\n",
      "Loss at step 248000 : learning_rate = 0.000312, loss = 0.029781\n",
      "231 170 190 28574\n",
      "precision = 0.576060, recall = 0.548694, f_score = 0.562044\n",
      "Loss at step 249000 : learning_rate = 0.000312, loss = 0.041578\n",
      "230 170 191 28574\n",
      "precision = 0.575000, recall = 0.546318, f_score = 0.560292\n"
     ]
    }
   ],
   "source": [
    "#train the model with stochastic gradient descent training\n",
    "batch_size = 256\n",
    "num_steps = 250000\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_data_n_resample_s[offset:(offset + batch_size), :]\n",
    "        #print(batch_data.shape)\n",
    "        batch_labels = v_train_label_resample_s[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_label : batch_labels}\n",
    "        #session.run(predicted_label, feed_dict=feed_dict)\n",
    "        _, l, _, r = session.run([predicted_label, loss, op, learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % 1000 == 0):\n",
    "            print('Loss at step %d : learning_rate = %f, loss = %f' % (step, r, l))\n",
    "            print(TP.eval(), FP.eval(), FN.eval(), TN.eval())\n",
    "            print(\"precision = %f, recall = %f, f_score = %f\" % (pre.eval(), rec.eval(), f_s.eval()))\n",
    "            #print('Loss at step %d: LR %f MSE %f MAE %f VALIDATE MAE %f' % (step, r, l, train_loss.eval(feed_dict=feed_dict), validate_loss.eval()))\n",
    "    #print(c_train_label_resample.reshape(-1, 1)[0:128, :])\n",
    "    #print('test1, test2, test MAE: %.3f, %.3f, %.3f' % (test_loss1.eval(), test_loss2.eval(), test_loss.eval()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_train_label_resample[0:128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116657, 82), (116657,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_n.shape, train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write create simple linear regression model\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(256, 82))\n",
    "    tf_train_label = tf.placeholder(tf.float32, shape=(256, 82))\n",
    "    tf_test_dataset = tf.constant(test_data_n.astype(np.float32))\n",
    "   \n",
    "   \n",
    "    biases = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "    def model(X, w, b):\n",
    "        hidden1 = tf.nn.relu6(tf.matmul(X, w['layer1']) + b['layer1'])\n",
    "        hidden2 = tf.nn.tanh(tf.matmul(hidden1, w['layer2']) + b['layer2'])\n",
    "        hidden3 = tf.nn.relu6(tf.matmul(hidden2, w['layer3']) + b['layer3'])\n",
    "        hidden4 = tf.nn.tanh(tf.matmul(hidden3, w['layer4']) + b['layer4'])\n",
    "        return hidden4, hidden2\n",
    "\n",
    "\n",
    "    weights = {'layer1' : tf.Variable(tf.truncated_normal([82, 16])),\n",
    "               'layer2' : tf.Variable(tf.truncated_normal([16, 2])),\n",
    "               'layer3' : tf.Variable(tf.truncated_normal([2, 16])),\n",
    "               'layer4' : tf.Variable(tf.truncated_normal([16, 82])),\n",
    "               }\n",
    "    biases = {'layer1' : tf.Variable(tf.zeros([16])),\n",
    "              'layer2' : tf.Variable(tf.zeros([2])),\n",
    "              'layer3' : tf.Variable(tf.zeros([16])),\n",
    "              'layer4' : tf.Variable(tf.zeros([82])),\n",
    "             }\n",
    "    \n",
    "    predicted_label, middle = model(tf_train_dataset, weights, biases)\n",
    "    loss = tf.reduce_mean((tf.square(predicted_label - tf_train_label)))\n",
    "    \n",
    "    # Learning rate decay\n",
    "    global_step = tf.Variable(0)\n",
    "    starter_learning_rate = 0.03\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 250, 0.95, staircase=True)\n",
    "    op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "\n",
    "    #output the encoder from \n",
    "    _, hidden2_test = model(tf_test_dataset, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0 : learning_rate = 0.030000, loss = 1.663830\n",
      "Loss at step 100 : learning_rate = 0.030000, loss = 1.472725\n",
      "Loss at step 200 : learning_rate = 0.030000, loss = 1.032295\n",
      "Loss at step 300 : learning_rate = 0.028500, loss = 1.041697\n",
      "Loss at step 400 : learning_rate = 0.028500, loss = 1.010043\n",
      "Loss at step 500 : learning_rate = 0.027075, loss = 1.046748\n",
      "Loss at step 600 : learning_rate = 0.027075, loss = 1.031405\n",
      "Loss at step 700 : learning_rate = 0.027075, loss = 0.977066\n",
      "Loss at step 800 : learning_rate = 0.025721, loss = 0.967030\n",
      "Loss at step 900 : learning_rate = 0.025721, loss = 1.030428\n",
      "Loss at step 1000 : learning_rate = 0.024435, loss = 0.988777\n",
      "Loss at step 1100 : learning_rate = 0.024435, loss = 0.975351\n",
      "Loss at step 1200 : learning_rate = 0.024435, loss = 0.977347\n",
      "Loss at step 1300 : learning_rate = 0.023213, loss = 1.024334\n",
      "Loss at step 1400 : learning_rate = 0.023213, loss = 0.956326\n",
      "Loss at step 1500 : learning_rate = 0.022053, loss = 1.041771\n",
      "Loss at step 1600 : learning_rate = 0.022053, loss = 0.946755\n",
      "Loss at step 1700 : learning_rate = 0.022053, loss = 0.999161\n",
      "Loss at step 1800 : learning_rate = 0.020950, loss = 0.996586\n",
      "Loss at step 1900 : learning_rate = 0.020950, loss = 1.019187\n",
      "Loss at step 2000 : learning_rate = 0.019903, loss = 0.999402\n",
      "Loss at step 2100 : learning_rate = 0.019903, loss = 1.027042\n",
      "Loss at step 2200 : learning_rate = 0.019903, loss = 1.003099\n",
      "Loss at step 2300 : learning_rate = 0.018907, loss = 1.043143\n",
      "Loss at step 2400 : learning_rate = 0.018907, loss = 1.025541\n"
     ]
    }
   ],
   "source": [
    "#train the model with stochastic gradient descent training\n",
    "batch_size = 256\n",
    "num_steps = 2500\n",
    "with tf.Session(graph = graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_data_n_s[offset:(offset + batch_size), :]\n",
    "        #print(batch_data.shape)\n",
    "        batch_labels = batch_data\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_label : batch_labels}\n",
    "        #session.run(predicted_label, feed_dict=feed_dict)\n",
    "        _, l, _, r = session.run([predicted_label, loss, op, learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step %d : learning_rate = %f, loss = %f' % (step, r, l))\n",
    "            \n",
    "    test_2d_point = hidden2_test.eval(feed_dict=feed_dict)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29165, 2)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_2d_point.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = test_2d_point[test_label <= 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(421, 2)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  0.99999988],\n",
       "       [ 1.        ,  0.99980122],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  0.99999779],\n",
       "       [ 1.        ,  0.99923235],\n",
       "       [ 1.        ,  0.93946761],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ],\n",
       "       [ 1.        ,  1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a [:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
