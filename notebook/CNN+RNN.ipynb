{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, Image\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASOS_alone.pickle\t     CC_Under_sSample.pickle\r\n",
      "ASOS+NWP.pickle\t\t     KORD_RUC_RAP_Hourly_20051031-20150301.csv\r\n",
      "ASOS+NWP_time_serial.pickle  rough_visibility.pickle\r\n",
      "ASOS_time_serial.pickle      visibilityDataNoLagsNewPreds.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls '/home/htan/proj/TensorFlow/data/visibility/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load data first\n",
    "pickle_file = '/home/htan/proj/TensorFlow/data/visibility/' +  'ASOS+NWP_time_serial.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    #train_dataset= save['train_dataset']\n",
    "    #validate_dataset = save['validate_dataset']\n",
    "    test_dataset = save['test_dataset']\n",
    "    #test_old = save['v_t_dataset']\n",
    "    train_old = save['t_v_dataset']\n",
    "    del save\n",
    "\n",
    "#train_time = train_dataset['time']\n",
    "#train_data = train_dataset['data']\n",
    "#train_label = train_dataset['label']\n",
    "#validate_time = validate_dataset['time']\n",
    "#validate_data = validate_dataset['data']\n",
    "#validate_label = validate_dataset['label']\n",
    "#test_time = test_dataset['time']\n",
    "test_data = test_dataset['data']\n",
    "test_label = test_dataset['label']\n",
    "#test_old_data = test_old['data']\n",
    "#test_old_label = test_old['label']\n",
    "train_old_data = train_old['data']\n",
    "train_old_label = train_old['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17664, 8, 82) (17664, 8, 1)\n",
      "(70656, 8, 82) (70656, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape, test_label.shape)\n",
    "print(train_old_data.shape, train_old_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82,) (82,)\n"
     ]
    }
   ],
   "source": [
    "#dataset normalize\n",
    "mean = train_old_data.mean(axis = (0,1))\n",
    "std = train_old_data.std(axis = (0,1))\n",
    "print(mean.shape, std.shape)\n",
    "train_data_n = (train_old_data - mean)/std\n",
    "#validate_data_n = (validate_data - mean)/std\n",
    "test_data_n = (test_data - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label = train_old_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try LSTM + CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70656\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "patch_size = 7\n",
    "n_hidden = 64\n",
    "n_features1 = 70\n",
    "n_features2 = 12\n",
    "n_steps = 8\n",
    "n_labels = 1\n",
    "\n",
    "\n",
    "total_size = train_old_label.shape[0]\n",
    "print(total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs1 = tf.placeholder(\"float32\", [None, n_steps, n_features1])\n",
    "    inputs2 = tf.placeholder(\"float32\", [None, n_steps, n_features2])\n",
    "    labels = tf.placeholder(\"float32\", [None, n_labels])\n",
    "\n",
    "    weights = {\n",
    "        'layer1': tf.Variable(tf.truncated_normal([patch_size, 1, 1, 16], stddev=0.1)),\n",
    "        'layer2': tf.Variable(tf.truncated_normal([patch_size, 1, 16, 16], stddev=0.1)),\n",
    "        'layer3': tf.Variable(tf.truncated_normal([10 * 7 * 16, n_hidden], stddev=0.1)),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden+12, n_labels])),\n",
    "    }\n",
    "    biases = {\n",
    "        'layer1': tf.Variable(tf.zeros([16])),\n",
    "        'layer2': tf.Variable(tf.constant(1.0, shape=[16])),\n",
    "        'layer3': tf.Variable(tf.constant(1.0, shape=[n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_labels])),\n",
    "    }\n",
    "    \n",
    "    def cov_model(data, w, b):\n",
    "        conv = tf.nn.conv2d(data, w['layer1'], [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + b['layer1'])\n",
    "        conv = tf.nn.conv2d(hidden, w['layer2'], [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + b['layer2'])\n",
    "        #shape = hidden.get_shape().as_list()\n",
    "        shape = tf.pack([tf.shape(hidden)[0], tf.shape(hidden)[1]*tf.shape(hidden)[2]*tf.shape(hidden)[3]])\n",
    "        reshape = tf.reshape(hidden, shape)\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, w['layer3']) + b['layer3'])\n",
    "        return hidden\n",
    "    \n",
    "    def RNN(x, y, w, b):\n",
    "        # Prepare data shape to match `rnn` function requirements\n",
    "        # Current data input shape: (batch_size, n_steps, n_input)\n",
    "        # Required shape: 'n_steps' tensors list of shape (batch_size, n_hidden)\n",
    "    \n",
    "        # Permuting batch_size and n_steps\n",
    "        x = tf.transpose(x, [1, 0, 2])\n",
    "        y = tf.transpose(y, [1, 0, 2])\n",
    "        \n",
    "        # Reshaping to (n_steps*batch_size, n_input)\n",
    "        x = tf.reshape(x, [-1, n_features1])\n",
    "        y = tf.reshape(y, [-1, n_features2])\n",
    "        # Linear activation\n",
    "        x = tf.reshape(x, [-1, 7, 10, 1])\n",
    "        x = cov_model(x, w, b)\n",
    "        \n",
    "        concat = tf.concat(1, [x, y])\n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_hidden)\n",
    "        list_data = tf.split(0, n_steps, concat)\n",
    "\n",
    "        # Define a lstm cell with tensorflow\n",
    "        lstm_cell = rnn_cell.BasicLSTMCell(n_hidden+12, forget_bias=1.0, state_is_tuple=True)\n",
    "\n",
    "        # Get lstm cell output\n",
    "        outputs, states = rnn.rnn(lstm_cell, list_data, dtype=tf.float32)\n",
    "\n",
    "        # Linear activation, using rnn inner loop last output\n",
    "        return tf.matmul(outputs[-1], w['out']) + b['out']\n",
    "    \n",
    "    pred = RNN(inputs1, inputs2, weights, biases)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    loss = tf.reduce_mean(tf.abs(pred - labels))\n",
    "    \n",
    "    # Learning rate decay\n",
    "    global_step = tf.Variable(0)\n",
    "    starter_learning_rate = 0.01\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 500, 0.90, staircase=True)\n",
    "    op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    MAE = tf.reduce_mean(tf.abs(pred - labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "step: 0, LR = 0.010000, min batch loss = 4.305766, test MAE = 10.312125\n",
      "step: 100, LR = 0.010000, min batch loss = 0.278466, test MAE = 1.314774\n",
      "step: 200, LR = 0.010000, min batch loss = 0.518329, test MAE = 1.330752\n",
      "step: 300, LR = 0.010000, min batch loss = 0.782156, test MAE = 1.230568\n",
      "step: 400, LR = 0.010000, min batch loss = 0.688073, test MAE = 1.223150\n",
      "step: 500, LR = 0.009000, min batch loss = 2.514328, test MAE = 1.156975\n",
      "step: 600, LR = 0.009000, min batch loss = 0.342869, test MAE = 0.620734\n",
      "step: 700, LR = 0.009000, min batch loss = 1.031895, test MAE = 0.657341\n",
      "step: 800, LR = 0.009000, min batch loss = 1.104359, test MAE = 0.712839\n",
      "step: 900, LR = 0.009000, min batch loss = 2.461146, test MAE = 0.574431\n",
      "step: 1000, LR = 0.008100, min batch loss = 0.230700, test MAE = 0.538429\n",
      "step: 1100, LR = 0.008100, min batch loss = 0.020479, test MAE = 0.779294\n",
      "step: 1200, LR = 0.008100, min batch loss = 0.424983, test MAE = 0.525701\n",
      "step: 1300, LR = 0.008100, min batch loss = 0.364099, test MAE = 0.573876\n",
      "step: 1400, LR = 0.008100, min batch loss = 0.214297, test MAE = 0.529357\n",
      "step: 1500, LR = 0.007290, min batch loss = 0.616328, test MAE = 0.554866\n",
      "step: 1600, LR = 0.007290, min batch loss = 0.375672, test MAE = 0.488764\n",
      "step: 1700, LR = 0.007290, min batch loss = 1.507312, test MAE = 0.509546\n",
      "step: 1800, LR = 0.007290, min batch loss = 1.466824, test MAE = 0.460109\n",
      "step: 1900, LR = 0.007290, min batch loss = 0.786228, test MAE = 0.513302\n",
      "step: 2000, LR = 0.006561, min batch loss = 1.725801, test MAE = 0.505148\n",
      "step: 2100, LR = 0.006561, min batch loss = 1.374106, test MAE = 0.549219\n",
      "step: 2200, LR = 0.006561, min batch loss = 0.096072, test MAE = 0.548309\n",
      "step: 2300, LR = 0.006561, min batch loss = 0.251570, test MAE = 0.454936\n",
      "step: 2400, LR = 0.006561, min batch loss = 0.126615, test MAE = 0.518161\n",
      "step: 2500, LR = 0.005905, min batch loss = 0.062132, test MAE = 0.500783\n",
      "step: 2600, LR = 0.005905, min batch loss = 0.005796, test MAE = 0.535927\n",
      "step: 2700, LR = 0.005905, min batch loss = 0.372582, test MAE = 0.486504\n",
      "step: 2800, LR = 0.005905, min batch loss = 0.610749, test MAE = 0.501033\n",
      "step: 2900, LR = 0.005905, min batch loss = 1.145065, test MAE = 0.521385\n",
      "step: 3000, LR = 0.005314, min batch loss = 0.033131, test MAE = 0.508574\n",
      "step: 3100, LR = 0.005314, min batch loss = 0.342108, test MAE = 0.441114\n",
      "step: 3200, LR = 0.005314, min batch loss = 0.789682, test MAE = 0.487891\n",
      "step: 3300, LR = 0.005314, min batch loss = 0.093088, test MAE = 0.506415\n",
      "step: 3400, LR = 0.005314, min batch loss = 0.107007, test MAE = 0.446865\n",
      "step: 3500, LR = 0.004783, min batch loss = 0.297620, test MAE = 0.458034\n",
      "step: 3600, LR = 0.004783, min batch loss = 0.083871, test MAE = 0.464434\n",
      "step: 3700, LR = 0.004783, min batch loss = 0.499846, test MAE = 0.451690\n",
      "step: 3800, LR = 0.004783, min batch loss = 0.129888, test MAE = 0.470027\n",
      "step: 3900, LR = 0.004783, min batch loss = 0.162495, test MAE = 0.484245\n",
      "step: 4000, LR = 0.004305, min batch loss = 1.078458, test MAE = 0.475812\n",
      "step: 4100, LR = 0.004305, min batch loss = 0.584235, test MAE = 0.459310\n",
      "step: 4200, LR = 0.004305, min batch loss = 0.721868, test MAE = 0.434541\n",
      "step: 4300, LR = 0.004305, min batch loss = 0.061763, test MAE = 0.471968\n",
      "step: 4400, LR = 0.004305, min batch loss = 0.853737, test MAE = 0.439136\n",
      "step: 4500, LR = 0.003874, min batch loss = 0.478148, test MAE = 0.434536\n",
      "step: 4600, LR = 0.003874, min batch loss = 0.410183, test MAE = 0.465220\n",
      "step: 4700, LR = 0.003874, min batch loss = 0.586507, test MAE = 0.451937\n",
      "step: 4800, LR = 0.003874, min batch loss = 0.084826, test MAE = 0.462277\n",
      "step: 4900, LR = 0.003874, min batch loss = 1.233045, test MAE = 0.433281\n",
      "step: 5000, LR = 0.003487, min batch loss = 0.503554, test MAE = 0.436645\n",
      "step: 5100, LR = 0.003487, min batch loss = 0.252052, test MAE = 0.466829\n",
      "step: 5200, LR = 0.003487, min batch loss = 0.240035, test MAE = 0.454478\n",
      "step: 5300, LR = 0.003487, min batch loss = 0.572407, test MAE = 0.442415\n",
      "step: 5400, LR = 0.003487, min batch loss = 0.219537, test MAE = 0.447679\n",
      "step: 5500, LR = 0.003138, min batch loss = 0.785432, test MAE = 0.433455\n",
      "step: 5600, LR = 0.003138, min batch loss = 1.218810, test MAE = 0.425592\n",
      "step: 5700, LR = 0.003138, min batch loss = 0.015113, test MAE = 0.459850\n",
      "step: 5800, LR = 0.003138, min batch loss = 0.021645, test MAE = 0.471482\n",
      "step: 5900, LR = 0.003138, min batch loss = 0.321299, test MAE = 0.449396\n",
      "step: 6000, LR = 0.002824, min batch loss = 0.367385, test MAE = 0.437750\n",
      "step: 6100, LR = 0.002824, min batch loss = 0.016102, test MAE = 0.465354\n",
      "step: 6200, LR = 0.002824, min batch loss = 0.658217, test MAE = 0.438479\n",
      "step: 6300, LR = 0.002824, min batch loss = 0.010719, test MAE = 0.431344\n",
      "step: 6400, LR = 0.002824, min batch loss = 0.110892, test MAE = 0.450245\n",
      "step: 6500, LR = 0.002542, min batch loss = 0.179375, test MAE = 0.434792\n",
      "step: 6600, LR = 0.002542, min batch loss = 0.350909, test MAE = 0.423303\n",
      "step: 6700, LR = 0.002542, min batch loss = 0.440937, test MAE = 0.431854\n",
      "step: 6800, LR = 0.002542, min batch loss = 0.585473, test MAE = 0.423004\n",
      "step: 6900, LR = 0.002542, min batch loss = 0.630359, test MAE = 0.441642\n",
      "step: 7000, LR = 0.002288, min batch loss = 0.029500, test MAE = 0.447417\n",
      "step: 7100, LR = 0.002288, min batch loss = 0.467286, test MAE = 0.432273\n",
      "step: 7200, LR = 0.002288, min batch loss = 0.744766, test MAE = 0.429692\n",
      "step: 7300, LR = 0.002288, min batch loss = 0.985614, test MAE = 0.427274\n",
      "step: 7400, LR = 0.002288, min batch loss = 0.375122, test MAE = 0.440791\n",
      "step: 7500, LR = 0.002059, min batch loss = 0.534245, test MAE = 0.422443\n",
      "step: 7600, LR = 0.002059, min batch loss = 0.084597, test MAE = 0.430042\n",
      "step: 7700, LR = 0.002059, min batch loss = 0.007425, test MAE = 0.435291\n",
      "step: 7800, LR = 0.002059, min batch loss = 0.917032, test MAE = 0.430384\n",
      "step: 7900, LR = 0.002059, min batch loss = 0.433613, test MAE = 0.422534\n",
      "step: 8000, LR = 0.001853, min batch loss = 0.304976, test MAE = 0.442977\n",
      "step: 8100, LR = 0.001853, min batch loss = 0.009314, test MAE = 0.445457\n",
      "step: 8200, LR = 0.001853, min batch loss = 0.836861, test MAE = 0.448029\n",
      "step: 8300, LR = 0.001853, min batch loss = 0.640458, test MAE = 0.450846\n",
      "step: 8400, LR = 0.001853, min batch loss = 0.775029, test MAE = 0.419571\n",
      "step: 8500, LR = 0.001668, min batch loss = 0.154655, test MAE = 0.431691\n",
      "step: 8600, LR = 0.001668, min batch loss = 1.031304, test MAE = 0.423760\n",
      "step: 8700, LR = 0.001668, min batch loss = 0.196648, test MAE = 0.423269\n",
      "step: 8800, LR = 0.001668, min batch loss = 0.122076, test MAE = 0.422948\n",
      "step: 8900, LR = 0.001668, min batch loss = 0.587474, test MAE = 0.427105\n",
      "step: 9000, LR = 0.001501, min batch loss = 0.234236, test MAE = 0.418172\n",
      "step: 9100, LR = 0.001501, min batch loss = 0.175223, test MAE = 0.438032\n",
      "step: 9200, LR = 0.001501, min batch loss = 0.488057, test MAE = 0.424105\n",
      "step: 9300, LR = 0.001501, min batch loss = 1.551441, test MAE = 0.449418\n",
      "step: 9400, LR = 0.001501, min batch loss = 0.848903, test MAE = 0.418732\n",
      "step: 9500, LR = 0.001351, min batch loss = 0.843807, test MAE = 0.417657\n",
      "step: 9600, LR = 0.001351, min batch loss = 0.556009, test MAE = 0.429246\n",
      "step: 9700, LR = 0.001351, min batch loss = 0.044359, test MAE = 0.416907\n",
      "step: 9800, LR = 0.001351, min batch loss = 0.411027, test MAE = 0.421345\n",
      "step: 9900, LR = 0.001351, min batch loss = 0.097969, test MAE = 0.420171\n",
      "1903.18136692\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "steps = 10000\n",
    "st = time.time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    test_feed_dict = {inputs1:test_data_n[:,:,:70], inputs2:test_data_n[:,:,70:], labels:test_label[:,-1,:]}\n",
    "    for step in range(steps):\n",
    "        off = step * batch_size % (total_size - batch_size)\n",
    "        batch_data = train_data_n[off:off+batch_size, :, :]\n",
    "        batch_label = train_label[off:off+batch_size, -1, :]\n",
    "        feed_dict = {inputs1:batch_data[:,:,:70], inputs2:batch_data[:,:,70:], labels:batch_label}\n",
    "        l, _, r = session.run([loss, op, learning_rate], feed_dict=feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            test_mae = MAE.eval(feed_dict=test_feed_dict)\n",
    "            print('step: %d, LR = %f, min batch loss = %f, test MAE = %f' % (step, r, l, test_mae))\n",
    "et = time.time()\n",
    "print(et-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70656, 8, 1), (17664, 8, 1))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_label_l = train_label[:, -1, :]\n",
    "test_label_l = test_label[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 70656), (17664, 1))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_l.reshape(1, -1).shape, test_label_l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "pre = Binarizer(threshold = 1.01)\n",
    "b_train_label = pre.transform(train_label_l.reshape(1, -1))\n",
    "b_test_label = pre.transform(test_label_l.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_train_label = 1 - b_train_label[0]\n",
    "c_test_label = 1 - b_test_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70656,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change from Indice to Vector\n",
    "''''''\n",
    "def makeIndicatorVars(T):\n",
    "    # Make sure T is two-dimensiona. Should be nSamples x 1.\n",
    "    if T.ndim == 1:\n",
    "        T = T.reshape((-1,1))    \n",
    "    return (T == np.unique(T)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_train_label = makeIndicatorVars(c_train_label.reshape(-1, 1))\n",
    "v_test_label = makeIndicatorVars(c_test_label.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70656\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "patch_size = 7\n",
    "n_hidden = 64\n",
    "n_features1 = 70\n",
    "n_features2 = 12\n",
    "n_steps = 8\n",
    "n_labels = 2\n",
    "ratio = 0.2\n",
    "\n",
    "total_size = train_old_label.shape[0]\n",
    "print(total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    inputs1 = tf.placeholder(\"float32\", [None, n_steps, n_features1])\n",
    "    inputs2 = tf.placeholder(\"float32\", [None, n_steps, n_features2])\n",
    "    labels = tf.placeholder(\"float32\", [None, n_labels])\n",
    "\n",
    "    weights = {\n",
    "        'layer1': tf.Variable(tf.truncated_normal([patch_size, 1, 1, 16], stddev=0.1)),\n",
    "        'layer2': tf.Variable(tf.truncated_normal([patch_size, 1, 16, 16], stddev=0.1)),\n",
    "        'layer3': tf.Variable(tf.truncated_normal([10 * 7 * 16, n_hidden], stddev=0.1)),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden+12, n_labels])),\n",
    "    }\n",
    "    biases = {\n",
    "        'layer1': tf.Variable(tf.zeros([16])),\n",
    "        'layer2': tf.Variable(tf.constant(1.0, shape=[16])),\n",
    "        'layer3': tf.Variable(tf.constant(1.0, shape=[n_hidden])),\n",
    "        'out': tf.Variable(tf.random_normal([n_labels])),\n",
    "    }\n",
    "    \n",
    "    def acc(predict, label):\n",
    "        #correct_prediction = tf.equal(predicted_label, tf_train_label)\n",
    "        correct_prediction = tf.equal(tf.argmax(predict, 1), tf.argmax(label, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        predict_event = tf.reduce_sum(tf.argmax(predict, 1))\n",
    "        label_event = tf.reduce_sum(tf.argmax(label, 1))\n",
    "        true_positive = tf.reduce_sum(tf.cast(tf.equal((tf.argmax(predict, 1) + tf.argmax(label, 1)), 2), tf.int64))\n",
    "        true_negative = tf.reduce_sum(tf.cast(tf.equal((tf.argmax(predict, 1) + tf.argmax(label, 1)), 0), tf.int64))\n",
    "        false_positive = predict_event - true_positive \n",
    "        false_negative = label_event - true_positive\n",
    "        return accuracy, false_positive, false_negative, true_positive, true_negative\n",
    "    def ROC(FP, FN, TP, TN):\n",
    "        TP_percent = TP / (TP + FN) \n",
    "        FP_percent = FP / (FP + TN) \n",
    "        return TP_percent, FP_percent\n",
    "    \n",
    "    def PRC(FP, FN, TP, TN):\n",
    "        precision = TP / (TP + FP + 1)\n",
    "        recall = TP / (TP + FN + 1)\n",
    "        f_score = TP / (TP + FN + FP)\n",
    "        return precision, recall, f_score\n",
    "    \n",
    "    def cov_model(data, w, b):\n",
    "        conv = tf.nn.conv2d(data, w['layer1'], [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + b['layer1'])\n",
    "        conv = tf.nn.conv2d(hidden, w['layer2'], [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + b['layer2'])\n",
    "        #shape = hidden.get_shape().as_list()\n",
    "        shape = tf.pack([tf.shape(hidden)[0], tf.shape(hidden)[1]*tf.shape(hidden)[2]*tf.shape(hidden)[3]])\n",
    "        reshape = tf.reshape(hidden, shape)\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, w['layer3']) + b['layer3'])\n",
    "        return hidden\n",
    "    \n",
    "    def RNN(x, y, w, b):\n",
    "        # Prepare data shape to match `rnn` function requirements\n",
    "        # Current data input shape: (batch_size, n_steps, n_input)\n",
    "        # Required shape: 'n_steps' tensors list of shape (batch_size, n_hidden)\n",
    "    \n",
    "        # Permuting batch_size and n_steps\n",
    "        x = tf.transpose(x, [1, 0, 2])\n",
    "        y = tf.transpose(y, [1, 0, 2])\n",
    "        \n",
    "        # Reshaping to (n_steps*batch_size, n_input)\n",
    "        x = tf.reshape(x, [-1, n_features1])\n",
    "        y = tf.reshape(y, [-1, n_features2])\n",
    "        # Linear activation\n",
    "        x = tf.reshape(x, [-1, 7, 10, 1])\n",
    "        x = cov_model(x, w, b)\n",
    "        \n",
    "        concat = tf.concat(1, [x, y])\n",
    "        # Split to get a list of 'n_steps' tensors of shape (batch_size, n_hidden)\n",
    "        list_data = tf.split(0, n_steps, concat)\n",
    "\n",
    "        # Define a lstm cell with tensorflow\n",
    "        lstm_cell = rnn_cell.BasicLSTMCell(n_hidden+12, forget_bias=1.0, state_is_tuple=True)\n",
    "\n",
    "        # Get lstm cell output\n",
    "        outputs, states = rnn.rnn(lstm_cell, list_data, dtype=tf.float32)\n",
    "\n",
    "        # Linear activation, using rnn inner loop last output\n",
    "        return tf.matmul(outputs[-1], w['out']) + b['out']\n",
    "    \n",
    "    pred = RNN(inputs1, inputs2, weights, biases)\n",
    "    class_weight = tf.constant([ratio, 1.0 - ratio])\n",
    "    weighted_pred = tf.mul(pred, class_weight) # shape [batch_size, 2]\n",
    "    \n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(weighted_pred, labels)) # Softmax loss\n",
    "    \n",
    "    # Learning rate decay\n",
    "    global_step = tf.Variable(0)\n",
    "    starter_learning_rate = 0.02\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 500, 0.90, staircase=True)\n",
    "    op = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    \n",
    "    \n",
    "    # Evaluate model\n",
    "    correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    test_acc, FP, FN, TP, TN = acc(pred, labels)\n",
    "    pre, rec, f_s = PRC(FP, FN, TP, TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 learning_rate = 0.020000: loss = 5.600290\n",
      "TP = 0, FP = 0, FN = 314, TN = 17350\n",
      "precision = 0.000000, recall = 0.000000, f_score = 0.000000\n",
      "Minibatch loss at step 100 learning_rate = 0.020000: loss = 0.005007\n",
      "TP = 0, FP = 0, FN = 314, TN = 17350\n",
      "precision = 0.000000, recall = 0.000000, f_score = 0.000000\n",
      "Minibatch loss at step 200 learning_rate = 0.020000: loss = 0.086997\n",
      "TP = 0, FP = 0, FN = 314, TN = 17350\n",
      "precision = 0.000000, recall = 0.000000, f_score = 0.000000\n",
      "Minibatch loss at step 300 learning_rate = 0.020000: loss = 0.011242\n",
      "TP = 0, FP = 0, FN = 314, TN = 17350\n",
      "precision = 0.000000, recall = 0.000000, f_score = 0.000000\n",
      "Minibatch loss at step 400 learning_rate = 0.020000: loss = 0.006474\n",
      "TP = 0, FP = 0, FN = 314, TN = 17350\n",
      "precision = 0.000000, recall = 0.000000, f_score = 0.000000\n",
      "Minibatch loss at step 500 learning_rate = 0.018000: loss = 0.003683\n",
      "TP = 0, FP = 0, FN = 314, TN = 17350\n",
      "precision = 0.000000, recall = 0.000000, f_score = 0.000000\n",
      "Minibatch loss at step 600 learning_rate = 0.018000: loss = 0.022179\n",
      "TP = 0, FP = 0, FN = 314, TN = 17350\n",
      "precision = 0.000000, recall = 0.000000, f_score = 0.000000\n",
      "Minibatch loss at step 700 learning_rate = 0.018000: loss = 0.350121\n",
      "TP = 0, FP = 0, FN = 314, TN = 17350\n",
      "precision = 0.000000, recall = 0.000000, f_score = 0.000000\n",
      "Minibatch loss at step 800 learning_rate = 0.018000: loss = 0.056529\n",
      "TP = 10, FP = 10, FN = 304, TN = 17340\n",
      "precision = 0.476190, recall = 0.031746, f_score = 0.030864\n",
      "Minibatch loss at step 900 learning_rate = 0.018000: loss = 0.059621\n",
      "TP = 3, FP = 4, FN = 311, TN = 17346\n",
      "precision = 0.375000, recall = 0.009524, f_score = 0.009434\n",
      "Minibatch loss at step 1000 learning_rate = 0.016200: loss = 0.152774\n",
      "TP = 0, FP = 0, FN = 314, TN = 17350\n",
      "precision = 0.000000, recall = 0.000000, f_score = 0.000000\n",
      "Minibatch loss at step 1100 learning_rate = 0.016200: loss = 0.000874\n",
      "TP = 0, FP = 0, FN = 314, TN = 17350\n",
      "precision = 0.000000, recall = 0.000000, f_score = 0.000000\n",
      "Minibatch loss at step 1200 learning_rate = 0.016200: loss = 0.000954\n",
      "TP = 41, FP = 0, FN = 273, TN = 17350\n",
      "precision = 0.976190, recall = 0.130159, f_score = 0.130573\n",
      "Minibatch loss at step 1300 learning_rate = 0.016200: loss = 0.003335\n",
      "TP = 65, FP = 4, FN = 249, TN = 17346\n",
      "precision = 0.928571, recall = 0.206349, f_score = 0.204403\n",
      "Minibatch loss at step 1400 learning_rate = 0.016200: loss = 0.000919\n",
      "TP = 151, FP = 113, FN = 163, TN = 17237\n",
      "precision = 0.569811, recall = 0.479365, f_score = 0.353630\n",
      "Minibatch loss at step 1500 learning_rate = 0.014580: loss = 0.001134\n",
      "TP = 68, FP = 1, FN = 246, TN = 17349\n",
      "precision = 0.971429, recall = 0.215873, f_score = 0.215873\n",
      "Minibatch loss at step 1600 learning_rate = 0.014580: loss = 0.013475\n",
      "TP = 57, FP = 0, FN = 257, TN = 17350\n",
      "precision = 0.982759, recall = 0.180952, f_score = 0.181529\n",
      "Minibatch loss at step 1700 learning_rate = 0.014580: loss = 0.029003\n",
      "TP = 174, FP = 124, FN = 140, TN = 17226\n",
      "precision = 0.581940, recall = 0.552381, f_score = 0.397260\n",
      "Minibatch loss at step 1800 learning_rate = 0.014580: loss = 0.191358\n",
      "TP = 250, FP = 568, FN = 64, TN = 16782\n",
      "precision = 0.305250, recall = 0.793651, f_score = 0.283447\n",
      "Minibatch loss at step 1900 learning_rate = 0.014580: loss = 0.034598\n",
      "TP = 255, FP = 569, FN = 59, TN = 16781\n",
      "precision = 0.309091, recall = 0.809524, f_score = 0.288788\n",
      "Minibatch loss at step 2000 learning_rate = 0.013122: loss = 0.166923\n",
      "TP = 154, FP = 92, FN = 160, TN = 17258\n",
      "precision = 0.623482, recall = 0.488889, f_score = 0.379310\n",
      "Minibatch loss at step 2100 learning_rate = 0.013122: loss = 0.028148\n",
      "TP = 121, FP = 31, FN = 193, TN = 17319\n",
      "precision = 0.790850, recall = 0.384127, f_score = 0.350725\n",
      "Minibatch loss at step 2200 learning_rate = 0.013122: loss = 0.000189\n",
      "TP = 165, FP = 98, FN = 149, TN = 17252\n",
      "precision = 0.625000, recall = 0.523810, f_score = 0.400485\n",
      "Minibatch loss at step 2300 learning_rate = 0.013122: loss = 0.000255\n",
      "TP = 222, FP = 328, FN = 92, TN = 17022\n",
      "precision = 0.402904, recall = 0.704762, f_score = 0.345794\n",
      "Minibatch loss at step 2400 learning_rate = 0.013122: loss = 0.000102\n",
      "TP = 151, FP = 74, FN = 163, TN = 17276\n",
      "precision = 0.668142, recall = 0.479365, f_score = 0.389175\n",
      "Minibatch loss at step 2500 learning_rate = 0.011810: loss = 0.000440\n",
      "TP = 260, FP = 562, FN = 54, TN = 16788\n",
      "precision = 0.315917, recall = 0.825397, f_score = 0.296804\n",
      "Minibatch loss at step 2600 learning_rate = 0.011810: loss = 0.000263\n",
      "TP = 141, FP = 54, FN = 173, TN = 17296\n",
      "precision = 0.719388, recall = 0.447619, f_score = 0.383152\n",
      "Minibatch loss at step 2700 learning_rate = 0.011810: loss = 0.002425\n",
      "TP = 113, FP = 24, FN = 201, TN = 17326\n",
      "precision = 0.818841, recall = 0.358730, f_score = 0.334320\n",
      "Minibatch loss at step 2800 learning_rate = 0.011810: loss = 0.082652\n",
      "TP = 239, FP = 418, FN = 75, TN = 16932\n",
      "precision = 0.363222, recall = 0.758730, f_score = 0.326503\n",
      "Minibatch loss at step 2900 learning_rate = 0.011810: loss = 0.269296\n",
      "TP = 198, FP = 162, FN = 116, TN = 17188\n",
      "precision = 0.548476, recall = 0.628571, f_score = 0.415966\n",
      "Minibatch loss at step 3000 learning_rate = 0.010629: loss = 0.000345\n",
      "TP = 251, FP = 458, FN = 63, TN = 16892\n",
      "precision = 0.353521, recall = 0.796825, f_score = 0.325130\n",
      "Minibatch loss at step 3100 learning_rate = 0.010629: loss = 0.014552\n",
      "TP = 236, FP = 385, FN = 78, TN = 16965\n",
      "precision = 0.379421, recall = 0.749206, f_score = 0.337625\n",
      "Minibatch loss at step 3200 learning_rate = 0.010629: loss = 0.002849\n",
      "TP = 225, FP = 264, FN = 89, TN = 17086\n",
      "precision = 0.459184, recall = 0.714286, f_score = 0.389273\n",
      "Minibatch loss at step 3300 learning_rate = 0.010629: loss = 0.000320\n",
      "TP = 224, FP = 243, FN = 90, TN = 17107\n",
      "precision = 0.478632, recall = 0.711111, f_score = 0.402154\n",
      "Minibatch loss at step 3400 learning_rate = 0.010629: loss = 0.000208\n",
      "TP = 242, FP = 425, FN = 72, TN = 16925\n",
      "precision = 0.362275, recall = 0.768254, f_score = 0.327470\n",
      "Minibatch loss at step 3500 learning_rate = 0.009566: loss = 0.000378\n",
      "TP = 172, FP = 91, FN = 142, TN = 17259\n",
      "precision = 0.651515, recall = 0.546032, f_score = 0.424691\n",
      "Minibatch loss at step 3600 learning_rate = 0.009566: loss = 0.000371\n",
      "TP = 263, FP = 727, FN = 51, TN = 16623\n",
      "precision = 0.265388, recall = 0.834921, f_score = 0.252642\n",
      "Minibatch loss at step 3700 learning_rate = 0.009566: loss = 0.027087\n",
      "TP = 218, FP = 191, FN = 96, TN = 17159\n",
      "precision = 0.531707, recall = 0.692063, f_score = 0.431683\n",
      "Minibatch loss at step 3800 learning_rate = 0.009566: loss = 0.000533\n",
      "TP = 148, FP = 57, FN = 166, TN = 17293\n",
      "precision = 0.718447, recall = 0.469841, f_score = 0.398922\n",
      "Minibatch loss at step 3900 learning_rate = 0.009566: loss = 0.051658\n",
      "TP = 266, FP = 737, FN = 48, TN = 16613\n",
      "precision = 0.264940, recall = 0.844444, f_score = 0.253092\n",
      "Minibatch loss at step 4000 learning_rate = 0.008609: loss = 0.221513\n",
      "TP = 223, FP = 238, FN = 91, TN = 17112\n",
      "precision = 0.482684, recall = 0.707937, f_score = 0.403986\n",
      "Minibatch loss at step 4100 learning_rate = 0.008609: loss = 0.000542\n",
      "TP = 231, FP = 322, FN = 83, TN = 17028\n",
      "precision = 0.416968, recall = 0.733333, f_score = 0.363208\n",
      "Minibatch loss at step 4200 learning_rate = 0.008609: loss = 0.113927\n",
      "TP = 239, FP = 396, FN = 75, TN = 16954\n",
      "precision = 0.375786, recall = 0.758730, f_score = 0.336620\n",
      "Minibatch loss at step 4300 learning_rate = 0.008609: loss = 0.000198\n",
      "TP = 233, FP = 321, FN = 81, TN = 17029\n",
      "precision = 0.419820, recall = 0.739683, f_score = 0.366929\n",
      "Minibatch loss at step 4400 learning_rate = 0.008609: loss = 0.002573\n",
      "TP = 225, FP = 243, FN = 89, TN = 17107\n",
      "precision = 0.479744, recall = 0.714286, f_score = 0.403950\n",
      "Minibatch loss at step 4500 learning_rate = 0.007748: loss = 0.000925\n",
      "TP = 246, FP = 452, FN = 68, TN = 16898\n",
      "precision = 0.351931, recall = 0.780952, f_score = 0.321149\n",
      "Minibatch loss at step 4600 learning_rate = 0.007748: loss = 0.000176\n",
      "TP = 195, FP = 131, FN = 119, TN = 17219\n",
      "precision = 0.596330, recall = 0.619048, f_score = 0.438202\n",
      "Minibatch loss at step 4700 learning_rate = 0.007748: loss = 0.138179\n",
      "TP = 263, FP = 662, FN = 51, TN = 16688\n",
      "precision = 0.284017, recall = 0.834921, f_score = 0.269467\n",
      "Minibatch loss at step 4800 learning_rate = 0.007748: loss = 0.000165\n",
      "TP = 227, FP = 253, FN = 87, TN = 17097\n",
      "precision = 0.471933, recall = 0.720635, f_score = 0.400353\n",
      "Minibatch loss at step 4900 learning_rate = 0.007748: loss = 0.003890\n",
      "TP = 174, FP = 93, FN = 140, TN = 17257\n",
      "precision = 0.649254, recall = 0.552381, f_score = 0.427518\n",
      "Minibatch loss at step 5000 learning_rate = 0.006974: loss = 0.471803\n",
      "TP = 239, FP = 384, FN = 75, TN = 16966\n",
      "precision = 0.383013, recall = 0.758730, f_score = 0.342407\n",
      "Minibatch loss at step 5100 learning_rate = 0.006974: loss = 0.007554\n",
      "TP = 221, FP = 210, FN = 93, TN = 17140\n",
      "precision = 0.511574, recall = 0.701587, f_score = 0.421756\n",
      "Minibatch loss at step 5200 learning_rate = 0.006974: loss = 0.026615\n",
      "TP = 206, FP = 154, FN = 108, TN = 17196\n",
      "precision = 0.570637, recall = 0.653968, f_score = 0.440171\n",
      "Minibatch loss at step 5300 learning_rate = 0.006974: loss = 0.161638\n",
      "TP = 193, FP = 126, FN = 121, TN = 17224\n",
      "precision = 0.603125, recall = 0.612698, f_score = 0.438636\n",
      "Minibatch loss at step 5400 learning_rate = 0.006974: loss = 0.000616\n",
      "TP = 236, FP = 350, FN = 78, TN = 17000\n",
      "precision = 0.402044, recall = 0.749206, f_score = 0.355422\n",
      "Minibatch loss at step 5500 learning_rate = 0.006276: loss = 0.005930\n",
      "TP = 226, FP = 257, FN = 88, TN = 17093\n",
      "precision = 0.466942, recall = 0.717460, f_score = 0.395797\n",
      "Minibatch loss at step 5600 learning_rate = 0.006276: loss = 0.132951\n",
      "TP = 249, FP = 471, FN = 65, TN = 16879\n",
      "precision = 0.345354, recall = 0.790476, f_score = 0.317197\n",
      "Minibatch loss at step 5700 learning_rate = 0.006276: loss = 0.000156\n",
      "TP = 211, FP = 169, FN = 103, TN = 17181\n",
      "precision = 0.553806, recall = 0.669841, f_score = 0.436853\n",
      "Minibatch loss at step 5800 learning_rate = 0.006276: loss = 0.000304\n",
      "TP = 249, FP = 474, FN = 65, TN = 16876\n",
      "precision = 0.343923, recall = 0.790476, f_score = 0.315990\n",
      "Minibatch loss at step 5900 learning_rate = 0.006276: loss = 0.000320\n",
      "TP = 230, FP = 269, FN = 84, TN = 17081\n",
      "precision = 0.460000, recall = 0.730159, f_score = 0.394511\n",
      "Minibatch loss at step 6000 learning_rate = 0.005649: loss = 0.000706\n",
      "TP = 200, FP = 136, FN = 114, TN = 17214\n",
      "precision = 0.593472, recall = 0.634921, f_score = 0.444444\n",
      "Minibatch loss at step 6100 learning_rate = 0.005649: loss = 0.000444\n",
      "TP = 184, FP = 116, FN = 130, TN = 17234\n",
      "precision = 0.611296, recall = 0.584127, f_score = 0.427907\n",
      "Minibatch loss at step 6200 learning_rate = 0.005649: loss = 0.065616\n",
      "TP = 241, FP = 382, FN = 73, TN = 16968\n",
      "precision = 0.386218, recall = 0.765079, f_score = 0.346264\n",
      "Minibatch loss at step 6300 learning_rate = 0.005649: loss = 0.000258\n",
      "TP = 214, FP = 179, FN = 100, TN = 17171\n",
      "precision = 0.543147, recall = 0.679365, f_score = 0.434077\n",
      "Minibatch loss at step 6400 learning_rate = 0.005649: loss = 0.000359\n",
      "TP = 216, FP = 164, FN = 98, TN = 17186\n",
      "precision = 0.566929, recall = 0.685714, f_score = 0.451883\n",
      "Minibatch loss at step 6500 learning_rate = 0.005084: loss = 0.000349\n",
      "TP = 237, FP = 366, FN = 77, TN = 16984\n",
      "precision = 0.392384, recall = 0.752381, f_score = 0.348529\n",
      "Minibatch loss at step 6600 learning_rate = 0.005084: loss = 0.001504\n",
      "TP = 229, FP = 252, FN = 85, TN = 17098\n",
      "precision = 0.475104, recall = 0.726984, f_score = 0.404594\n",
      "Minibatch loss at step 6700 learning_rate = 0.005084: loss = 0.001299\n",
      "TP = 249, FP = 474, FN = 65, TN = 16876\n",
      "precision = 0.343923, recall = 0.790476, f_score = 0.315990\n",
      "Minibatch loss at step 6800 learning_rate = 0.005084: loss = 0.000266\n",
      "TP = 221, FP = 202, FN = 93, TN = 17148\n",
      "precision = 0.521226, recall = 0.701587, f_score = 0.428295\n",
      "Minibatch loss at step 6900 learning_rate = 0.005084: loss = 0.160690\n",
      "TP = 250, FP = 485, FN = 64, TN = 16865\n",
      "precision = 0.339674, recall = 0.793651, f_score = 0.312891\n",
      "Minibatch loss at step 7000 learning_rate = 0.004575: loss = 0.000192\n",
      "TP = 234, FP = 291, FN = 80, TN = 17059\n",
      "precision = 0.444867, recall = 0.742857, f_score = 0.386777\n",
      "Minibatch loss at step 7100 learning_rate = 0.004575: loss = 0.117816\n",
      "TP = 218, FP = 182, FN = 96, TN = 17168\n",
      "precision = 0.543641, recall = 0.692063, f_score = 0.439516\n",
      "Minibatch loss at step 7200 learning_rate = 0.004575: loss = 0.005874\n",
      "TP = 212, FP = 166, FN = 102, TN = 17184\n",
      "precision = 0.559367, recall = 0.673016, f_score = 0.441667\n",
      "Minibatch loss at step 7300 learning_rate = 0.004575: loss = 0.034282\n",
      "TP = 239, FP = 383, FN = 75, TN = 16967\n",
      "precision = 0.383628, recall = 0.758730, f_score = 0.342898\n",
      "Minibatch loss at step 7400 learning_rate = 0.004575: loss = 0.019987\n",
      "TP = 222, FP = 208, FN = 92, TN = 17142\n",
      "precision = 0.515081, recall = 0.704762, f_score = 0.425287\n",
      "Minibatch loss at step 7500 learning_rate = 0.004118: loss = 0.000385\n",
      "TP = 231, FP = 238, FN = 83, TN = 17112\n",
      "precision = 0.491489, recall = 0.733333, f_score = 0.418478\n",
      "Minibatch loss at step 7600 learning_rate = 0.004118: loss = 0.000196\n",
      "TP = 237, FP = 382, FN = 77, TN = 16968\n",
      "precision = 0.382258, recall = 0.752381, f_score = 0.340517\n",
      "Minibatch loss at step 7700 learning_rate = 0.004118: loss = 0.000324\n",
      "TP = 234, FP = 255, FN = 80, TN = 17095\n",
      "precision = 0.477551, recall = 0.742857, f_score = 0.411248\n",
      "Minibatch loss at step 7800 learning_rate = 0.004118: loss = 0.110435\n",
      "TP = 251, FP = 481, FN = 63, TN = 16869\n",
      "precision = 0.342428, recall = 0.796825, f_score = 0.315723\n",
      "Minibatch loss at step 7900 learning_rate = 0.004118: loss = 0.152405\n",
      "TP = 228, FP = 228, FN = 86, TN = 17122\n",
      "precision = 0.498906, recall = 0.723810, f_score = 0.420664\n",
      "Minibatch loss at step 8000 learning_rate = 0.003706: loss = 0.000372\n",
      "TP = 256, FP = 518, FN = 58, TN = 16832\n",
      "precision = 0.330323, recall = 0.812698, f_score = 0.307692\n",
      "Minibatch loss at step 8100 learning_rate = 0.003706: loss = 0.000476\n",
      "TP = 237, FP = 319, FN = 77, TN = 17031\n",
      "precision = 0.425494, recall = 0.752381, f_score = 0.374408\n",
      "Minibatch loss at step 8200 learning_rate = 0.003706: loss = 0.007243\n",
      "TP = 203, FP = 147, FN = 111, TN = 17203\n",
      "precision = 0.578348, recall = 0.644444, f_score = 0.440347\n",
      "Minibatch loss at step 8300 learning_rate = 0.003706: loss = 0.002599\n",
      "TP = 223, FP = 207, FN = 91, TN = 17143\n",
      "precision = 0.517401, recall = 0.707937, f_score = 0.428023\n",
      "Minibatch loss at step 8400 learning_rate = 0.003706: loss = 0.001150\n",
      "TP = 240, FP = 392, FN = 74, TN = 16958\n",
      "precision = 0.379147, recall = 0.761905, f_score = 0.339943\n",
      "Minibatch loss at step 8500 learning_rate = 0.003335: loss = 0.000385\n",
      "TP = 233, FP = 261, FN = 81, TN = 17089\n",
      "precision = 0.470707, recall = 0.739683, f_score = 0.405217\n",
      "Minibatch loss at step 8600 learning_rate = 0.003335: loss = 0.011541\n",
      "TP = 252, FP = 483, FN = 62, TN = 16867\n",
      "precision = 0.342391, recall = 0.800000, f_score = 0.316186\n",
      "Minibatch loss at step 8700 learning_rate = 0.003335: loss = 0.000497\n",
      "TP = 238, FP = 381, FN = 76, TN = 16969\n",
      "precision = 0.383871, recall = 0.755556, f_score = 0.342446\n",
      "Minibatch loss at step 8800 learning_rate = 0.003335: loss = 0.000427\n",
      "TP = 234, FP = 255, FN = 80, TN = 17095\n",
      "precision = 0.477551, recall = 0.742857, f_score = 0.411248\n",
      "Minibatch loss at step 8900 learning_rate = 0.003335: loss = 0.001626\n",
      "TP = 250, FP = 461, FN = 64, TN = 16889\n",
      "precision = 0.351124, recall = 0.793651, f_score = 0.322581\n",
      "Minibatch loss at step 9000 learning_rate = 0.003002: loss = 0.000355\n",
      "TP = 222, FP = 198, FN = 92, TN = 17152\n",
      "precision = 0.527316, recall = 0.704762, f_score = 0.433594\n",
      "Minibatch loss at step 9100 learning_rate = 0.003002: loss = 0.000416\n",
      "TP = 254, FP = 510, FN = 60, TN = 16840\n",
      "precision = 0.332026, recall = 0.806349, f_score = 0.308252\n",
      "Minibatch loss at step 9200 learning_rate = 0.003002: loss = 0.245326\n",
      "TP = 237, FP = 323, FN = 77, TN = 17027\n",
      "precision = 0.422460, recall = 0.752381, f_score = 0.372057\n",
      "Minibatch loss at step 9300 learning_rate = 0.003002: loss = 0.011311\n",
      "TP = 227, FP = 223, FN = 87, TN = 17127\n",
      "precision = 0.503326, recall = 0.720635, f_score = 0.422719\n",
      "Minibatch loss at step 9400 learning_rate = 0.003002: loss = 0.032297\n",
      "TP = 234, FP = 258, FN = 80, TN = 17092\n",
      "precision = 0.474645, recall = 0.742857, f_score = 0.409091\n",
      "Minibatch loss at step 9500 learning_rate = 0.002702: loss = 0.429163\n",
      "TP = 238, FP = 340, FN = 76, TN = 17010\n",
      "precision = 0.411054, recall = 0.755556, f_score = 0.363914\n",
      "Minibatch loss at step 9600 learning_rate = 0.002702: loss = 0.000750\n",
      "TP = 236, FP = 284, FN = 78, TN = 17066\n",
      "precision = 0.452975, recall = 0.749206, f_score = 0.394649\n",
      "Minibatch loss at step 9700 learning_rate = 0.002702: loss = 0.000364\n",
      "TP = 253, FP = 502, FN = 61, TN = 16848\n",
      "precision = 0.334656, recall = 0.803175, f_score = 0.310049\n",
      "Minibatch loss at step 9800 learning_rate = 0.002702: loss = 0.000443\n",
      "TP = 238, FP = 372, FN = 76, TN = 16978\n",
      "precision = 0.389525, recall = 0.755556, f_score = 0.346939\n",
      "Minibatch loss at step 9900 learning_rate = 0.002702: loss = 0.000219\n",
      "TP = 233, FP = 257, FN = 81, TN = 17093\n",
      "precision = 0.474542, recall = 0.739683, f_score = 0.408056\n",
      "Minibatch loss at step 10000 learning_rate = 0.002432: loss = 0.000514\n",
      "TP = 250, FP = 452, FN = 64, TN = 16898\n",
      "precision = 0.355619, recall = 0.793651, f_score = 0.326371\n",
      "Minibatch loss at step 10100 learning_rate = 0.002432: loss = 0.000305\n",
      "TP = 228, FP = 232, FN = 86, TN = 17118\n",
      "precision = 0.494577, recall = 0.723810, f_score = 0.417582\n",
      "Minibatch loss at step 10200 learning_rate = 0.002432: loss = 0.000282\n",
      "TP = 254, FP = 501, FN = 60, TN = 16849\n",
      "precision = 0.335979, recall = 0.806349, f_score = 0.311656\n",
      "Minibatch loss at step 10300 learning_rate = 0.002432: loss = 0.099217\n",
      "TP = 238, FP = 354, FN = 76, TN = 16996\n",
      "precision = 0.401349, recall = 0.755556, f_score = 0.356287\n",
      "Minibatch loss at step 10400 learning_rate = 0.002432: loss = 0.001049\n",
      "TP = 236, FP = 289, FN = 78, TN = 17061\n",
      "precision = 0.448669, recall = 0.749206, f_score = 0.391376\n",
      "Minibatch loss at step 10500 learning_rate = 0.002188: loss = 0.012454\n",
      "TP = 237, FP = 321, FN = 77, TN = 17029\n",
      "precision = 0.423971, recall = 0.752381, f_score = 0.373228\n",
      "Minibatch loss at step 10600 learning_rate = 0.002188: loss = 0.001309\n",
      "TP = 216, FP = 171, FN = 98, TN = 17179\n",
      "precision = 0.556701, recall = 0.685714, f_score = 0.445361\n",
      "Minibatch loss at step 10700 learning_rate = 0.002188: loss = 0.003873\n",
      "TP = 237, FP = 307, FN = 77, TN = 17043\n",
      "precision = 0.434862, recall = 0.752381, f_score = 0.381643\n",
      "Minibatch loss at step 10800 learning_rate = 0.002188: loss = 0.001099\n",
      "TP = 254, FP = 499, FN = 60, TN = 16851\n",
      "precision = 0.336870, recall = 0.806349, f_score = 0.312423\n",
      "Minibatch loss at step 10900 learning_rate = 0.002188: loss = 0.000337\n",
      "TP = 238, FP = 364, FN = 76, TN = 16986\n",
      "precision = 0.394693, recall = 0.755556, f_score = 0.351032\n",
      "Minibatch loss at step 11000 learning_rate = 0.001970: loss = 0.000266\n",
      "TP = 234, FP = 262, FN = 80, TN = 17088\n",
      "precision = 0.470825, recall = 0.742857, f_score = 0.406250\n",
      "Minibatch loss at step 11100 learning_rate = 0.001970: loss = 0.000158\n",
      "TP = 247, FP = 433, FN = 67, TN = 16917\n",
      "precision = 0.362702, recall = 0.784127, f_score = 0.330656\n",
      "Minibatch loss at step 11200 learning_rate = 0.001970: loss = 0.000309\n",
      "TP = 234, FP = 250, FN = 80, TN = 17100\n",
      "precision = 0.482474, recall = 0.742857, f_score = 0.414894\n",
      "Minibatch loss at step 11300 learning_rate = 0.001970: loss = 0.004153\n",
      "TP = 254, FP = 493, FN = 60, TN = 16857\n",
      "precision = 0.339572, recall = 0.806349, f_score = 0.314746\n",
      "Minibatch loss at step 11400 learning_rate = 0.001970: loss = 0.114883\n",
      "TP = 237, FP = 324, FN = 77, TN = 17026\n",
      "precision = 0.421708, recall = 0.752381, f_score = 0.371473\n",
      "Minibatch loss at step 11500 learning_rate = 0.001773: loss = 0.000280\n",
      "TP = 237, FP = 296, FN = 77, TN = 17054\n",
      "precision = 0.443820, recall = 0.752381, f_score = 0.388525\n",
      "Minibatch loss at step 11600 learning_rate = 0.001773: loss = 0.002993\n",
      "TP = 237, FP = 321, FN = 77, TN = 17029\n",
      "precision = 0.423971, recall = 0.752381, f_score = 0.373228\n",
      "Minibatch loss at step 11700 learning_rate = 0.001773: loss = 0.087124\n",
      "TP = 231, FP = 234, FN = 83, TN = 17116\n",
      "precision = 0.495708, recall = 0.733333, f_score = 0.421533\n",
      "Minibatch loss at step 11800 learning_rate = 0.001773: loss = 0.000832\n",
      "TP = 237, FP = 314, FN = 77, TN = 17036\n",
      "precision = 0.429348, recall = 0.752381, f_score = 0.377389\n",
      "Minibatch loss at step 11900 learning_rate = 0.001773: loss = 0.000720\n",
      "TP = 254, FP = 494, FN = 60, TN = 16856\n",
      "precision = 0.339119, recall = 0.806349, f_score = 0.314356\n",
      "Minibatch loss at step 12000 learning_rate = 0.001595: loss = 0.000247\n",
      "TP = 238, FP = 352, FN = 76, TN = 16998\n",
      "precision = 0.402707, recall = 0.755556, f_score = 0.357357\n",
      "Minibatch loss at step 12100 learning_rate = 0.001595: loss = 0.158274\n",
      "TP = 235, FP = 266, FN = 79, TN = 17084\n",
      "precision = 0.468127, recall = 0.746032, f_score = 0.405172\n",
      "Minibatch loss at step 12200 learning_rate = 0.001595: loss = 0.000337\n",
      "TP = 246, FP = 413, FN = 68, TN = 16937\n",
      "precision = 0.372727, recall = 0.780952, f_score = 0.338377\n",
      "Minibatch loss at step 12300 learning_rate = 0.001595: loss = 0.000332\n",
      "TP = 236, FP = 279, FN = 78, TN = 17071\n",
      "precision = 0.457364, recall = 0.749206, f_score = 0.397976\n",
      "Minibatch loss at step 12400 learning_rate = 0.001595: loss = 0.157115\n",
      "TP = 252, FP = 488, FN = 62, TN = 16862\n",
      "precision = 0.340081, recall = 0.800000, f_score = 0.314214\n",
      "Minibatch loss at step 12500 learning_rate = 0.001436: loss = 0.000523\n",
      "TP = 238, FP = 352, FN = 76, TN = 16998\n",
      "precision = 0.402707, recall = 0.755556, f_score = 0.357357\n",
      "Minibatch loss at step 12600 learning_rate = 0.001436: loss = 0.081420\n",
      "TP = 237, FP = 307, FN = 77, TN = 17043\n",
      "precision = 0.434862, recall = 0.752381, f_score = 0.381643\n",
      "Minibatch loss at step 12700 learning_rate = 0.001436: loss = 0.000326\n",
      "TP = 237, FP = 320, FN = 77, TN = 17030\n",
      "precision = 0.424731, recall = 0.752381, f_score = 0.373817\n",
      "Minibatch loss at step 12800 learning_rate = 0.001436: loss = 0.152991\n",
      "TP = 230, FP = 228, FN = 84, TN = 17122\n",
      "precision = 0.501089, recall = 0.730159, f_score = 0.424354\n",
      "Minibatch loss at step 12900 learning_rate = 0.001436: loss = 0.000231\n",
      "TP = 238, FP = 315, FN = 76, TN = 17035\n",
      "precision = 0.429603, recall = 0.755556, f_score = 0.378378\n",
      "Minibatch loss at step 13000 learning_rate = 0.001292: loss = 0.001596\n",
      "TP = 253, FP = 490, FN = 61, TN = 16860\n",
      "precision = 0.340054, recall = 0.803175, f_score = 0.314677\n",
      "Minibatch loss at step 13100 learning_rate = 0.001292: loss = 0.000423\n",
      "TP = 238, FP = 344, FN = 76, TN = 17006\n",
      "precision = 0.408233, recall = 0.755556, f_score = 0.361702\n",
      "Minibatch loss at step 13200 learning_rate = 0.001292: loss = 0.000470\n",
      "TP = 235, FP = 263, FN = 79, TN = 17087\n",
      "precision = 0.470942, recall = 0.746032, f_score = 0.407279\n",
      "Minibatch loss at step 13300 learning_rate = 0.001292: loss = 0.000382\n",
      "TP = 243, FP = 389, FN = 71, TN = 16961\n",
      "precision = 0.383886, recall = 0.771429, f_score = 0.345661\n",
      "Minibatch loss at step 13400 learning_rate = 0.001292: loss = 0.000284\n",
      "TP = 236, FP = 293, FN = 78, TN = 17057\n",
      "precision = 0.445283, recall = 0.749206, f_score = 0.388797\n",
      "Minibatch loss at step 13500 learning_rate = 0.001163: loss = 0.000449\n",
      "TP = 253, FP = 489, FN = 61, TN = 16861\n",
      "precision = 0.340511, recall = 0.803175, f_score = 0.315068\n",
      "Minibatch loss at step 13600 learning_rate = 0.001163: loss = 0.000536\n",
      "TP = 238, FP = 345, FN = 76, TN = 17005\n",
      "precision = 0.407534, recall = 0.755556, f_score = 0.361153\n",
      "Minibatch loss at step 13700 learning_rate = 0.001163: loss = 0.127964\n",
      "TP = 236, FP = 296, FN = 78, TN = 17054\n",
      "precision = 0.442777, recall = 0.749206, f_score = 0.386885\n",
      "Minibatch loss at step 13800 learning_rate = 0.001163: loss = 0.028945\n",
      "TP = 238, FP = 321, FN = 76, TN = 17029\n",
      "precision = 0.425000, recall = 0.755556, f_score = 0.374803\n",
      "Minibatch loss at step 13900 learning_rate = 0.001163: loss = 0.087290\n",
      "TP = 239, FP = 350, FN = 75, TN = 17000\n",
      "precision = 0.405085, recall = 0.758730, f_score = 0.359940\n",
      "Minibatch loss at step 14000 learning_rate = 0.001047: loss = 0.000224\n",
      "TP = 238, FP = 317, FN = 76, TN = 17033\n",
      "precision = 0.428058, recall = 0.755556, f_score = 0.377179\n",
      "Minibatch loss at step 14100 learning_rate = 0.001047: loss = 0.000218\n",
      "TP = 251, FP = 471, FN = 63, TN = 16879\n",
      "precision = 0.347165, recall = 0.796825, f_score = 0.319745\n",
      "Minibatch loss at step 14200 learning_rate = 0.001047: loss = 0.000534\n",
      "TP = 238, FP = 342, FN = 76, TN = 17008\n",
      "precision = 0.409639, recall = 0.755556, f_score = 0.362805\n",
      "Minibatch loss at step 14300 learning_rate = 0.001047: loss = 0.000223\n",
      "TP = 236, FP = 270, FN = 78, TN = 17080\n",
      "precision = 0.465483, recall = 0.749206, f_score = 0.404110\n",
      "Minibatch loss at step 14400 learning_rate = 0.001047: loss = 0.000248\n",
      "TP = 241, FP = 371, FN = 73, TN = 16979\n",
      "precision = 0.393148, recall = 0.765079, f_score = 0.351825\n",
      "Minibatch loss at step 14500 learning_rate = 0.000942: loss = 0.001378\n",
      "TP = 236, FP = 295, FN = 78, TN = 17055\n",
      "precision = 0.443609, recall = 0.749206, f_score = 0.387521\n",
      "Minibatch loss at step 14600 learning_rate = 0.000942: loss = 0.159172\n",
      "TP = 248, FP = 419, FN = 66, TN = 16931\n",
      "precision = 0.371257, recall = 0.787302, f_score = 0.338336\n",
      "Minibatch loss at step 14700 learning_rate = 0.000942: loss = 0.011933\n",
      "TP = 238, FP = 317, FN = 76, TN = 17033\n",
      "precision = 0.428058, recall = 0.755556, f_score = 0.377179\n",
      "Minibatch loss at step 14800 learning_rate = 0.000942: loss = 0.044642\n",
      "TP = 236, FP = 302, FN = 78, TN = 17048\n",
      "precision = 0.437848, recall = 0.749206, f_score = 0.383117\n",
      "Minibatch loss at step 14900 learning_rate = 0.000942: loss = 0.095355\n",
      "TP = 236, FP = 296, FN = 78, TN = 17054\n",
      "precision = 0.442777, recall = 0.749206, f_score = 0.386885\n",
      "Minibatch loss at step 15000 learning_rate = 0.000848: loss = 0.235872\n",
      "TP = 240, FP = 358, FN = 74, TN = 16992\n",
      "precision = 0.400668, recall = 0.761905, f_score = 0.357143\n",
      "Minibatch loss at step 15100 learning_rate = 0.000848: loss = 0.000293\n",
      "TP = 238, FP = 319, FN = 76, TN = 17031\n",
      "precision = 0.426523, recall = 0.755556, f_score = 0.375987\n",
      "Minibatch loss at step 15200 learning_rate = 0.000848: loss = 0.000384\n",
      "TP = 251, FP = 465, FN = 63, TN = 16885\n",
      "precision = 0.350070, recall = 0.796825, f_score = 0.322208\n",
      "Minibatch loss at step 15300 learning_rate = 0.000848: loss = 0.057766\n",
      "TP = 238, FP = 342, FN = 76, TN = 17008\n",
      "precision = 0.409639, recall = 0.755556, f_score = 0.362805\n",
      "Minibatch loss at step 15400 learning_rate = 0.000848: loss = 0.000478\n",
      "TP = 236, FP = 275, FN = 78, TN = 17075\n",
      "precision = 0.460938, recall = 0.749206, f_score = 0.400679\n",
      "Minibatch loss at step 15500 learning_rate = 0.000763: loss = 0.000975\n",
      "TP = 240, FP = 362, FN = 74, TN = 16988\n",
      "precision = 0.398010, recall = 0.761905, f_score = 0.355030\n",
      "Minibatch loss at step 15600 learning_rate = 0.000763: loss = 0.000935\n",
      "TP = 237, FP = 299, FN = 77, TN = 17051\n",
      "precision = 0.441341, recall = 0.752381, f_score = 0.386623\n",
      "Minibatch loss at step 15700 learning_rate = 0.000763: loss = 0.019420\n",
      "TP = 246, FP = 395, FN = 68, TN = 16955\n",
      "precision = 0.383178, recall = 0.780952, f_score = 0.346968\n",
      "Minibatch loss at step 15800 learning_rate = 0.000763: loss = 0.006717\n",
      "TP = 238, FP = 329, FN = 76, TN = 17021\n",
      "precision = 0.419014, recall = 0.755556, f_score = 0.370140\n",
      "Minibatch loss at step 15900 learning_rate = 0.000763: loss = 0.011051\n",
      "TP = 238, FP = 309, FN = 76, TN = 17041\n",
      "precision = 0.434307, recall = 0.755556, f_score = 0.382022\n",
      "Minibatch loss at step 16000 learning_rate = 0.000687: loss = 0.001372\n",
      "TP = 236, FP = 292, FN = 78, TN = 17058\n",
      "precision = 0.446125, recall = 0.749206, f_score = 0.389439\n",
      "Minibatch loss at step 16100 learning_rate = 0.000687: loss = 0.032130\n",
      "TP = 239, FP = 344, FN = 75, TN = 17006\n",
      "precision = 0.409247, recall = 0.758730, f_score = 0.363222\n",
      "Minibatch loss at step 16200 learning_rate = 0.000687: loss = 0.000698\n",
      "TP = 238, FP = 313, FN = 76, TN = 17037\n",
      "precision = 0.431159, recall = 0.755556, f_score = 0.379585\n",
      "Minibatch loss at step 16300 learning_rate = 0.000687: loss = 0.000395\n",
      "TP = 251, FP = 449, FN = 63, TN = 16901\n",
      "precision = 0.358060, recall = 0.796825, f_score = 0.328965\n",
      "Minibatch loss at step 16400 learning_rate = 0.000687: loss = 0.000435\n",
      "TP = 239, FP = 343, FN = 75, TN = 17007\n",
      "precision = 0.409949, recall = 0.758730, f_score = 0.363775\n",
      "Minibatch loss at step 16500 learning_rate = 0.000618: loss = 0.000296\n",
      "TP = 236, FP = 285, FN = 78, TN = 17065\n",
      "precision = 0.452107, recall = 0.749206, f_score = 0.393990\n",
      "Minibatch loss at step 16600 learning_rate = 0.000618: loss = 0.000221\n",
      "TP = 239, FP = 345, FN = 75, TN = 17005\n",
      "precision = 0.408547, recall = 0.758730, f_score = 0.362671\n",
      "Minibatch loss at step 16700 learning_rate = 0.000618: loss = 0.001710\n",
      "TP = 237, FP = 299, FN = 77, TN = 17051\n",
      "precision = 0.441341, recall = 0.752381, f_score = 0.386623\n",
      "Minibatch loss at step 16800 learning_rate = 0.000618: loss = 0.002155\n",
      "TP = 243, FP = 369, FN = 71, TN = 16981\n",
      "precision = 0.396411, recall = 0.771429, f_score = 0.355783\n",
      "Minibatch loss at step 16900 learning_rate = 0.000618: loss = 0.001020\n",
      "TP = 239, FP = 334, FN = 75, TN = 17016\n",
      "precision = 0.416376, recall = 0.758730, f_score = 0.368827\n",
      "Minibatch loss at step 17000 learning_rate = 0.000556: loss = 0.003246\n",
      "TP = 239, FP = 341, FN = 75, TN = 17009\n",
      "precision = 0.411360, recall = 0.758730, f_score = 0.364885\n",
      "Minibatch loss at step 17100 learning_rate = 0.000556: loss = 0.001056\n",
      "TP = 236, FP = 295, FN = 78, TN = 17055\n",
      "precision = 0.443609, recall = 0.749206, f_score = 0.387521\n",
      "Minibatch loss at step 17200 learning_rate = 0.000556: loss = 0.000275\n",
      "TP = 240, FP = 349, FN = 74, TN = 17001\n",
      "precision = 0.406780, recall = 0.761905, f_score = 0.361991\n",
      "Minibatch loss at step 17300 learning_rate = 0.000556: loss = 0.000508\n",
      "TP = 238, FP = 314, FN = 76, TN = 17036\n",
      "precision = 0.430380, recall = 0.755556, f_score = 0.378981\n",
      "Minibatch loss at step 17400 learning_rate = 0.000556: loss = 0.000224\n",
      "TP = 251, FP = 434, FN = 63, TN = 16916\n",
      "precision = 0.365889, recall = 0.796825, f_score = 0.335561\n",
      "Minibatch loss at step 17500 learning_rate = 0.000501: loss = 0.003341\n",
      "TP = 239, FP = 344, FN = 75, TN = 17006\n",
      "precision = 0.409247, recall = 0.758730, f_score = 0.363222\n",
      "Minibatch loss at step 17600 learning_rate = 0.000501: loss = 0.159996\n",
      "TP = 236, FP = 296, FN = 78, TN = 17054\n",
      "precision = 0.442777, recall = 0.749206, f_score = 0.386885\n",
      "Minibatch loss at step 17700 learning_rate = 0.000501: loss = 0.000978\n",
      "TP = 239, FP = 341, FN = 75, TN = 17009\n",
      "precision = 0.411360, recall = 0.758730, f_score = 0.364885\n",
      "Minibatch loss at step 17800 learning_rate = 0.000501: loss = 0.000208\n",
      "TP = 238, FP = 316, FN = 76, TN = 17034\n",
      "precision = 0.428829, recall = 0.755556, f_score = 0.377778\n",
      "Minibatch loss at step 17900 learning_rate = 0.000501: loss = 0.000522\n",
      "TP = 245, FP = 378, FN = 69, TN = 16972\n",
      "precision = 0.392628, recall = 0.777778, f_score = 0.354046\n",
      "Minibatch loss at step 18000 learning_rate = 0.000451: loss = 0.172424\n",
      "TP = 238, FP = 320, FN = 76, TN = 17030\n",
      "precision = 0.425760, recall = 0.755556, f_score = 0.375394\n",
      "Minibatch loss at step 18100 learning_rate = 0.000451: loss = 0.127177\n",
      "TP = 239, FP = 341, FN = 75, TN = 17009\n",
      "precision = 0.411360, recall = 0.758730, f_score = 0.364885\n",
      "Minibatch loss at step 18200 learning_rate = 0.000451: loss = 0.362112\n",
      "TP = 237, FP = 304, FN = 77, TN = 17046\n",
      "precision = 0.437269, recall = 0.752381, f_score = 0.383495\n",
      "Minibatch loss at step 18300 learning_rate = 0.000451: loss = 0.002314\n",
      "TP = 239, FP = 340, FN = 75, TN = 17010\n",
      "precision = 0.412069, recall = 0.758730, f_score = 0.365443\n",
      "Minibatch loss at step 18400 learning_rate = 0.000451: loss = 0.006879\n",
      "TP = 238, FP = 313, FN = 76, TN = 17037\n",
      "precision = 0.431159, recall = 0.755556, f_score = 0.379585\n",
      "Minibatch loss at step 18500 learning_rate = 0.000406: loss = 0.231865\n",
      "TP = 249, FP = 414, FN = 65, TN = 16936\n",
      "precision = 0.375000, recall = 0.790476, f_score = 0.342033\n",
      "Minibatch loss at step 18600 learning_rate = 0.000406: loss = 0.000257\n",
      "TP = 239, FP = 345, FN = 75, TN = 17005\n",
      "precision = 0.408547, recall = 0.758730, f_score = 0.362671\n",
      "Minibatch loss at step 18700 learning_rate = 0.000406: loss = 0.271700\n",
      "TP = 236, FP = 293, FN = 78, TN = 17057\n",
      "precision = 0.445283, recall = 0.749206, f_score = 0.388797\n",
      "Minibatch loss at step 18800 learning_rate = 0.000406: loss = 0.000432\n",
      "TP = 239, FP = 340, FN = 75, TN = 17010\n",
      "precision = 0.412069, recall = 0.758730, f_score = 0.365443\n",
      "Minibatch loss at step 18900 learning_rate = 0.000406: loss = 0.100731\n",
      "TP = 238, FP = 316, FN = 76, TN = 17034\n",
      "precision = 0.428829, recall = 0.755556, f_score = 0.377778\n",
      "Minibatch loss at step 19000 learning_rate = 0.000365: loss = 0.000282\n",
      "TP = 243, FP = 368, FN = 71, TN = 16982\n",
      "precision = 0.397059, recall = 0.771429, f_score = 0.356305\n",
      "Minibatch loss at step 19100 learning_rate = 0.000365: loss = 0.001417\n",
      "TP = 240, FP = 345, FN = 74, TN = 17005\n",
      "precision = 0.409556, recall = 0.761905, f_score = 0.364188\n",
      "Minibatch loss at step 19200 learning_rate = 0.000365: loss = 0.005834\n",
      "TP = 239, FP = 341, FN = 75, TN = 17009\n",
      "precision = 0.411360, recall = 0.758730, f_score = 0.364885\n",
      "Minibatch loss at step 19300 learning_rate = 0.000365: loss = 0.003441\n",
      "TP = 237, FP = 313, FN = 77, TN = 17037\n",
      "precision = 0.430127, recall = 0.752381, f_score = 0.377990\n",
      "Minibatch loss at step 19400 learning_rate = 0.000365: loss = 0.001048\n",
      "TP = 239, FP = 338, FN = 75, TN = 17012\n",
      "precision = 0.413495, recall = 0.758730, f_score = 0.366564\n",
      "Minibatch loss at step 19500 learning_rate = 0.000328: loss = 0.000305\n",
      "TP = 238, FP = 318, FN = 76, TN = 17032\n",
      "precision = 0.427289, recall = 0.755556, f_score = 0.376582\n",
      "Minibatch loss at step 19600 learning_rate = 0.000328: loss = 0.014800\n",
      "TP = 246, FP = 392, FN = 68, TN = 16958\n",
      "precision = 0.384977, recall = 0.780952, f_score = 0.348442\n",
      "Minibatch loss at step 19700 learning_rate = 0.000328: loss = 0.056663\n",
      "TP = 239, FP = 344, FN = 75, TN = 17006\n",
      "precision = 0.409247, recall = 0.758730, f_score = 0.363222\n",
      "Minibatch loss at step 19800 learning_rate = 0.000328: loss = 0.003763\n",
      "TP = 236, FP = 295, FN = 78, TN = 17055\n",
      "precision = 0.443609, recall = 0.749206, f_score = 0.387521\n",
      "Minibatch loss at step 19900 learning_rate = 0.000328: loss = 0.040020\n",
      "TP = 239, FP = 331, FN = 75, TN = 17019\n",
      "precision = 0.418564, recall = 0.758730, f_score = 0.370543\n",
      "3787.30178595\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "batch_size = 128\n",
    "steps = 20000\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    test_feed_dict = {inputs1:test_data_n[:,:,:70], inputs2:test_data_n[:,:,70:], labels:v_test_label}\n",
    "    for step in range(steps):\n",
    "        off = step * batch_size % (total_size - batch_size)\n",
    "        batch_data = train_data_n[off:off+batch_size, :, :]\n",
    "        batch_label = v_train_label[off:off+batch_size, :]\n",
    "        feed_dict = {inputs1:batch_data[:,:,:70], inputs2:batch_data[:,:,70:], labels:batch_label}\n",
    "        l, _, lr = session.run([loss, op, learning_rate], feed_dict=feed_dict)\n",
    "        if step % 100 == 0:\n",
    "            print('Minibatch loss at step %d learning_rate = %f: loss = %f' % (step, lr, l))\n",
    "            tp, fp, fn, tn, precision, recall, f_score= session.run([TP, FP, FN, TN, pre, rec, f_s], feed_dict=test_feed_dict)\n",
    "            print(\"TP = %d, FP = %d, FN = %d, TN = %d\" % (tp, fp, fn, tn))\n",
    "            print(\"precision = %f, recall = %f, f_score = %f\" % (precision, recall, f_score))\n",
    "et= time.time()\n",
    "print(et-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
