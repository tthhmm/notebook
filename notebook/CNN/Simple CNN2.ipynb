{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import display, Image\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell, rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load data first\n",
    "pickle_file = '/home/htan/proj/TensorFlow/data/visibility/' +  'ASOS+NWP.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    test_dataset = save['test_dataset']\n",
    "    train_old = save['t_v_dataset']\n",
    "    del save\n",
    "\n",
    "\n",
    "test_data_1 = test_dataset['data_ASOS']\n",
    "test_data_2 = test_dataset['data_NWP']\n",
    "test_label = test_dataset['label']\n",
    "train_data_1 = train_old['data_ASOS']\n",
    "train_data_2 = train_old['data_NWP']\n",
    "train_label = train_old['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70,) (70,)\n"
     ]
    }
   ],
   "source": [
    "#dataset normalize\n",
    "mean1 = train_data_1.mean(axis = 0)\n",
    "std1 = train_data_1.std(axis = 0)\n",
    "print(mean1.shape, std1.shape)\n",
    "train_data_1_n = (train_data_1 - mean1)/std1\n",
    "#validate_data_n = (validate_data - mean)/std\n",
    "test_data_1_n = (test_data_1 - mean1)/std1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12,) (12,)\n"
     ]
    }
   ],
   "source": [
    "#dataset normalize\n",
    "mean2 = train_data_2.mean(axis = 0)\n",
    "std2 = train_data_2.std(axis = 0)\n",
    "print(mean2.shape, std2.shape)\n",
    "train_data_2_n = (train_data_2 - mean2)/std2\n",
    "#validate_data_n = (validate_data - mean)/std\n",
    "test_data_2_n = (test_data_2 - mean2)/std2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = np.hstack((train_data_1_n[:, :63], train_data_1_n[:, -1:],train_data_1_n[:, 63:-1] ))\n",
    "test_data = np.hstack((test_data_1_n[:, :63], test_data_1_n[:, -1:],test_data_1_n[:, 63:-1] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.concatenate((train_data.reshape(train_data.shape[0], 7, 10), \n",
    "                             np.repeat(train_data_2_n, 7, axis = 0).reshape(train_data_2_n.shape[0], 7, 12)), axis = 2)\n",
    "test_data = np.concatenate((test_data.reshape(test_data.shape[0], 7, 10), \n",
    "                             np.repeat(test_data_2_n, 7, axis = 0).reshape(test_data_2_n.shape[0], 7, 12)), axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116657, 7, 22), (29165, 7, 22))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116657, 7, 22), (29165, 7, 22))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_2d = train_data.reshape(train_data.shape[0], 7, 22, 1)\n",
    "test_data_2d = test_data.reshape(test_data.shape[0], 7, 22, 1)\n",
    "train_label = train_label.reshape(-1, 1)\n",
    "test_label = test_label.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116657, 7, 22, 1), (116657, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_2d.shape, train_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regression problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "patch_size = 3\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "num_channels = 1\n",
    "num_labels = 1\n",
    "num_feature = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, 7, num_feature, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_test_dataset = tf.constant(test_data_2d, dtype=tf.float32)\n",
    "    tf_test_labels = tf.constant(test_label, dtype=tf.float32) \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([num_feature * 7 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    pred = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(tf.abs(pred - tf_train_labels))\n",
    "    \n",
    "    \n",
    "    # Learning rate decay\n",
    "    global_step = tf.Variable(0)\n",
    "    starter_learning_rate = 0.01\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 2000, 0.95, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    MAE = tf.reduce_mean(tf.abs(model(tf_test_dataset) - tf_test_labels))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 100001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "        batch_data = train_data_2d[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_label[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, lr = session.run([optimizer, loss, learning_rate], feed_dict=feed_dict)\n",
    "        if step % 500 == 0:\n",
    "            print('Minibatch loss at step %d learning rate = %f: loss = %f, MAE = %f' % (step, lr, l, MAE.eval()))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other type of method to use the NWP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.hstack((train_data_1_n[:, :63], train_data_1_n[:, -1:],train_data_1_n[:, 63:-1] ))\n",
    "test_data = np.hstack((test_data_1_n[:, :63], test_data_1_n[:, -1:],test_data_1_n[:, 63:-1] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116657, 70), (29165, 70))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_2d = train_data.reshape(train_data.shape[0], 7, 10, 1)\n",
    "test_data_2d = test_data.reshape(test_data.shape[0], 7, 10, 1)\n",
    "train_label = train_label.reshape(-1, 1)\n",
    "test_label = test_label.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_NWP = train_data_2_n\n",
    "test_data_NWP = test_data_2_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29165, 12), (29165, 7, 10, 1))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_NWP.shape, test_data_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "patch_size = 7\n",
    "depth = 32\n",
    "num_hidden = 64\n",
    "num_channels = 1\n",
    "num_labels = 1\n",
    "num_feature = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset1 = tf.placeholder(tf.float32, shape=(batch_size, 7, num_feature, num_channels))\n",
    "    tf_train_dataset2 = tf.placeholder(tf.float32, shape=(batch_size, 12))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_test_dataset1 = tf.constant(test_data_2d, dtype=tf.float32)\n",
    "    tf_test_dataset2 = tf.constant(test_data_NWP, dtype=tf.float32)\n",
    "    tf_test_labels = tf.constant(test_label, dtype=tf.float32) \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, 1, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, 1, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([num_feature * 7 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden + 12, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model(data1, data2):\n",
    "        conv = tf.nn.conv2d(data1, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        concat = tf.concat(1, [hidden, data2])\n",
    "        return tf.matmul(concat, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    pred = model(tf_train_dataset1, tf_train_dataset2)\n",
    "    loss = tf.reduce_mean(tf.abs(pred - tf_train_labels))\n",
    "    \n",
    "    # Learning rate decay\n",
    "    global_step = tf.Variable(0)\n",
    "    starter_learning_rate = 0.02\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 4000, 0.95, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    MAE = tf.reduce_mean(tf.abs(model(tf_test_dataset1, tf_test_dataset2) - tf_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 learning rate = 0.020000: loss = 9.519822, MAE = 14.396765\n",
      "Minibatch loss at step 500 learning rate = 0.020000: loss = 1.250537, MAE = 0.684987\n",
      "Minibatch loss at step 1000 learning rate = 0.020000: loss = 0.841218, MAE = 0.633878\n",
      "Minibatch loss at step 1500 learning rate = 0.020000: loss = 0.814739, MAE = 0.839496\n",
      "Minibatch loss at step 2000 learning rate = 0.020000: loss = 0.515709, MAE = 0.597903\n",
      "Minibatch loss at step 2500 learning rate = 0.020000: loss = 0.582989, MAE = 0.684882\n",
      "Minibatch loss at step 3000 learning rate = 0.020000: loss = 0.999996, MAE = 0.603188\n",
      "Minibatch loss at step 3500 learning rate = 0.020000: loss = 0.960646, MAE = 0.567667\n",
      "Minibatch loss at step 4000 learning rate = 0.019000: loss = 0.536711, MAE = 0.591064\n",
      "Minibatch loss at step 4500 learning rate = 0.019000: loss = 0.349664, MAE = 0.567524\n",
      "Minibatch loss at step 5000 learning rate = 0.019000: loss = 0.472264, MAE = 0.454232\n",
      "Minibatch loss at step 5500 learning rate = 0.019000: loss = 0.435358, MAE = 0.546494\n",
      "Minibatch loss at step 6000 learning rate = 0.019000: loss = 1.013995, MAE = 0.581784\n",
      "Minibatch loss at step 6500 learning rate = 0.019000: loss = 0.690746, MAE = 0.463395\n",
      "Minibatch loss at step 7000 learning rate = 0.019000: loss = 0.433044, MAE = 0.483472\n",
      "Minibatch loss at step 7500 learning rate = 0.019000: loss = 0.531487, MAE = 0.648778\n",
      "Minibatch loss at step 8000 learning rate = 0.018050: loss = 0.435787, MAE = 0.625546\n",
      "Minibatch loss at step 8500 learning rate = 0.018050: loss = 0.286590, MAE = 0.514422\n",
      "Minibatch loss at step 9000 learning rate = 0.018050: loss = 0.068955, MAE = 0.424738\n",
      "Minibatch loss at step 9500 learning rate = 0.018050: loss = 1.327549, MAE = 0.419386\n",
      "Minibatch loss at step 10000 learning rate = 0.018050: loss = 0.091139, MAE = 0.552723\n",
      "Minibatch loss at step 10500 learning rate = 0.018050: loss = 0.183741, MAE = 0.556365\n",
      "Minibatch loss at step 11000 learning rate = 0.018050: loss = 0.696283, MAE = 0.408743\n",
      "Minibatch loss at step 11500 learning rate = 0.018050: loss = 0.111175, MAE = 0.598258\n",
      "Minibatch loss at step 12000 learning rate = 0.017147: loss = 0.627159, MAE = 0.423751\n",
      "Minibatch loss at step 12500 learning rate = 0.017147: loss = 0.189980, MAE = 0.588232\n",
      "Minibatch loss at step 13000 learning rate = 0.017147: loss = 1.166819, MAE = 0.432626\n",
      "Minibatch loss at step 13500 learning rate = 0.017147: loss = 1.240590, MAE = 0.437334\n",
      "Minibatch loss at step 14000 learning rate = 0.017147: loss = 0.297389, MAE = 0.454300\n",
      "Minibatch loss at step 14500 learning rate = 0.017147: loss = 1.460612, MAE = 0.425717\n",
      "Minibatch loss at step 15000 learning rate = 0.017147: loss = 0.711896, MAE = 0.414442\n",
      "Minibatch loss at step 15500 learning rate = 0.017147: loss = 0.330455, MAE = 0.415400\n",
      "Minibatch loss at step 16000 learning rate = 0.016290: loss = 1.129811, MAE = 0.465784\n",
      "Minibatch loss at step 16500 learning rate = 0.016290: loss = 0.155417, MAE = 0.554145\n",
      "Minibatch loss at step 17000 learning rate = 0.016290: loss = 0.837723, MAE = 0.416154\n",
      "Minibatch loss at step 17500 learning rate = 0.016290: loss = 0.112557, MAE = 0.552159\n",
      "Minibatch loss at step 18000 learning rate = 0.016290: loss = 0.367540, MAE = 0.433682\n",
      "Minibatch loss at step 18500 learning rate = 0.016290: loss = 0.238048, MAE = 0.405421\n",
      "Minibatch loss at step 19000 learning rate = 0.016290: loss = 0.217171, MAE = 0.493535\n",
      "Minibatch loss at step 19500 learning rate = 0.016290: loss = 0.551478, MAE = 0.427410\n",
      "Minibatch loss at step 20000 learning rate = 0.015476: loss = 0.400689, MAE = 0.508691\n",
      "Minibatch loss at step 20500 learning rate = 0.015476: loss = 0.163997, MAE = 0.542063\n",
      "Minibatch loss at step 21000 learning rate = 0.015476: loss = 0.855456, MAE = 0.435257\n",
      "Minibatch loss at step 21500 learning rate = 0.015476: loss = 0.829640, MAE = 0.396232\n",
      "Minibatch loss at step 22000 learning rate = 0.015476: loss = 0.437609, MAE = 0.425404\n",
      "Minibatch loss at step 22500 learning rate = 0.015476: loss = 0.096545, MAE = 0.400263\n",
      "Minibatch loss at step 23000 learning rate = 0.015476: loss = 0.697705, MAE = 0.406626\n",
      "Minibatch loss at step 23500 learning rate = 0.015476: loss = 0.337017, MAE = 0.431789\n",
      "Minibatch loss at step 24000 learning rate = 0.014702: loss = 0.368855, MAE = 0.432985\n",
      "Minibatch loss at step 24500 learning rate = 0.014702: loss = 0.949845, MAE = 0.369431\n",
      "Minibatch loss at step 25000 learning rate = 0.014702: loss = 0.694690, MAE = 0.416289\n",
      "Minibatch loss at step 25500 learning rate = 0.014702: loss = 1.326798, MAE = 0.395602\n",
      "Minibatch loss at step 26000 learning rate = 0.014702: loss = 0.171508, MAE = 0.439993\n",
      "Minibatch loss at step 26500 learning rate = 0.014702: loss = 0.194953, MAE = 0.415700\n",
      "Minibatch loss at step 27000 learning rate = 0.014702: loss = 0.940520, MAE = 0.491529\n",
      "Minibatch loss at step 27500 learning rate = 0.014702: loss = 0.578201, MAE = 0.421829\n",
      "Minibatch loss at step 28000 learning rate = 0.013967: loss = 0.754702, MAE = 0.394519\n",
      "Minibatch loss at step 28500 learning rate = 0.013967: loss = 0.084156, MAE = 0.434352\n",
      "Minibatch loss at step 29000 learning rate = 0.013967: loss = 0.788268, MAE = 0.420086\n",
      "Minibatch loss at step 29500 learning rate = 0.013967: loss = 0.599367, MAE = 0.383761\n",
      "Minibatch loss at step 30000 learning rate = 0.013967: loss = 0.121044, MAE = 0.448165\n",
      "Minibatch loss at step 30500 learning rate = 0.013967: loss = 0.259562, MAE = 0.431525\n",
      "Minibatch loss at step 31000 learning rate = 0.013967: loss = 0.028304, MAE = 0.430679\n",
      "Minibatch loss at step 31500 learning rate = 0.013967: loss = 0.332165, MAE = 0.428810\n",
      "Minibatch loss at step 32000 learning rate = 0.013268: loss = 0.491040, MAE = 0.389971\n",
      "Minibatch loss at step 32500 learning rate = 0.013268: loss = 0.501864, MAE = 0.381695\n",
      "Minibatch loss at step 33000 learning rate = 0.013268: loss = 0.758861, MAE = 0.418005\n",
      "Minibatch loss at step 33500 learning rate = 0.013268: loss = 0.086035, MAE = 0.416590\n",
      "Minibatch loss at step 34000 learning rate = 0.013268: loss = 0.499121, MAE = 0.392649\n",
      "Minibatch loss at step 34500 learning rate = 0.013268: loss = 0.052899, MAE = 0.483172\n",
      "Minibatch loss at step 35000 learning rate = 0.013268: loss = 0.179488, MAE = 0.434771\n",
      "Minibatch loss at step 35500 learning rate = 0.013268: loss = 0.322638, MAE = 0.380606\n",
      "Minibatch loss at step 36000 learning rate = 0.012605: loss = 0.529347, MAE = 0.384228\n",
      "Minibatch loss at step 36500 learning rate = 0.012605: loss = 0.711692, MAE = 0.377239\n",
      "Minibatch loss at step 37000 learning rate = 0.012605: loss = 0.788100, MAE = 0.386280\n",
      "Minibatch loss at step 37500 learning rate = 0.012605: loss = 0.032965, MAE = 0.448196\n",
      "Minibatch loss at step 38000 learning rate = 0.012605: loss = 0.607730, MAE = 0.385518\n",
      "Minibatch loss at step 38500 learning rate = 0.012605: loss = 0.476562, MAE = 0.405484\n",
      "Minibatch loss at step 39000 learning rate = 0.012605: loss = 0.882599, MAE = 0.405297\n",
      "Minibatch loss at step 39500 learning rate = 0.012605: loss = 0.253322, MAE = 0.412023\n",
      "Minibatch loss at step 40000 learning rate = 0.011975: loss = 0.576265, MAE = 0.398984\n",
      "Minibatch loss at step 40500 learning rate = 0.011975: loss = 1.124025, MAE = 0.398546\n",
      "Minibatch loss at step 41000 learning rate = 0.011975: loss = 0.531317, MAE = 0.388793\n",
      "Minibatch loss at step 41500 learning rate = 0.011975: loss = 0.166035, MAE = 0.388804\n",
      "Minibatch loss at step 42000 learning rate = 0.011975: loss = 0.780381, MAE = 0.405303\n",
      "Minibatch loss at step 42500 learning rate = 0.011975: loss = 0.753171, MAE = 0.391113\n",
      "Minibatch loss at step 43000 learning rate = 0.011975: loss = 0.999453, MAE = 0.384770\n",
      "Minibatch loss at step 43500 learning rate = 0.011975: loss = 0.120383, MAE = 0.393563\n",
      "Minibatch loss at step 44000 learning rate = 0.011376: loss = 0.761797, MAE = 0.412825\n",
      "Minibatch loss at step 44500 learning rate = 0.011376: loss = 1.248216, MAE = 0.398375\n",
      "Minibatch loss at step 45000 learning rate = 0.011376: loss = 0.339887, MAE = 0.384301\n",
      "Minibatch loss at step 45500 learning rate = 0.011376: loss = 0.065950, MAE = 0.422522\n",
      "Minibatch loss at step 46000 learning rate = 0.011376: loss = 0.329266, MAE = 0.391493\n",
      "Minibatch loss at step 46500 learning rate = 0.011376: loss = 0.670380, MAE = 0.399492\n",
      "Minibatch loss at step 47000 learning rate = 0.011376: loss = 0.773797, MAE = 0.373844\n",
      "Minibatch loss at step 47500 learning rate = 0.011376: loss = 0.530650, MAE = 0.391330\n",
      "Minibatch loss at step 48000 learning rate = 0.010807: loss = 0.158836, MAE = 0.388001\n",
      "Minibatch loss at step 48500 learning rate = 0.010807: loss = 0.719495, MAE = 0.459573\n",
      "Minibatch loss at step 49000 learning rate = 0.010807: loss = 0.027221, MAE = 0.403333\n",
      "Minibatch loss at step 49500 learning rate = 0.010807: loss = 0.121218, MAE = 0.402708\n",
      "Minibatch loss at step 50000 learning rate = 0.010807: loss = 0.291498, MAE = 0.392065\n",
      "Minibatch loss at step 50500 learning rate = 0.010807: loss = 1.829166, MAE = 0.413697\n",
      "Minibatch loss at step 51000 learning rate = 0.010807: loss = 0.048290, MAE = 0.407501\n",
      "Minibatch loss at step 51500 learning rate = 0.010807: loss = 0.233016, MAE = 0.372101\n",
      "Minibatch loss at step 52000 learning rate = 0.010267: loss = 0.216319, MAE = 0.393165\n",
      "Minibatch loss at step 52500 learning rate = 0.010267: loss = 0.064621, MAE = 0.428499\n",
      "Minibatch loss at step 53000 learning rate = 0.010267: loss = 0.044902, MAE = 0.417872\n",
      "Minibatch loss at step 53500 learning rate = 0.010267: loss = 0.937198, MAE = 0.409019\n",
      "Minibatch loss at step 54000 learning rate = 0.010267: loss = 0.294768, MAE = 0.391394\n",
      "Minibatch loss at step 54500 learning rate = 0.010267: loss = 0.023202, MAE = 0.414630\n",
      "Minibatch loss at step 55000 learning rate = 0.010267: loss = 0.053726, MAE = 0.394351\n",
      "Minibatch loss at step 55500 learning rate = 0.010267: loss = 0.854617, MAE = 0.382475\n",
      "Minibatch loss at step 56000 learning rate = 0.009753: loss = 1.768665, MAE = 0.439828\n",
      "Minibatch loss at step 56500 learning rate = 0.009753: loss = 0.492135, MAE = 0.386915\n",
      "Minibatch loss at step 57000 learning rate = 0.009753: loss = 0.065416, MAE = 0.413109\n",
      "Minibatch loss at step 57500 learning rate = 0.009753: loss = 0.084048, MAE = 0.391646\n",
      "Minibatch loss at step 58000 learning rate = 0.009753: loss = 1.291142, MAE = 0.366747\n",
      "Minibatch loss at step 58500 learning rate = 0.009753: loss = 0.799376, MAE = 0.377785\n",
      "Minibatch loss at step 59000 learning rate = 0.009753: loss = 0.320944, MAE = 0.399827\n",
      "Minibatch loss at step 59500 learning rate = 0.009753: loss = 0.683160, MAE = 0.378964\n",
      "Minibatch loss at step 60000 learning rate = 0.009266: loss = 0.024337, MAE = 0.400204\n",
      "Minibatch loss at step 60500 learning rate = 0.009266: loss = 0.969182, MAE = 0.384036\n",
      "Minibatch loss at step 61000 learning rate = 0.009266: loss = 1.269534, MAE = 0.372396\n",
      "Minibatch loss at step 61500 learning rate = 0.009266: loss = 1.122524, MAE = 0.365861\n",
      "Minibatch loss at step 62000 learning rate = 0.009266: loss = 0.038095, MAE = 0.366676\n",
      "Minibatch loss at step 62500 learning rate = 0.009266: loss = 0.708347, MAE = 0.380018\n",
      "Minibatch loss at step 63000 learning rate = 0.009266: loss = 0.728858, MAE = 0.370489\n",
      "Minibatch loss at step 63500 learning rate = 0.009266: loss = 0.019290, MAE = 0.406509\n",
      "Minibatch loss at step 64000 learning rate = 0.008803: loss = 0.414165, MAE = 0.366380\n",
      "Minibatch loss at step 64500 learning rate = 0.008803: loss = 0.050956, MAE = 0.426893\n",
      "Minibatch loss at step 65000 learning rate = 0.008803: loss = 0.935582, MAE = 0.363991\n",
      "Minibatch loss at step 65500 learning rate = 0.008803: loss = 0.060311, MAE = 0.379383\n",
      "Minibatch loss at step 66000 learning rate = 0.008803: loss = 0.820194, MAE = 0.388692\n",
      "Minibatch loss at step 66500 learning rate = 0.008803: loss = 0.695283, MAE = 0.372187\n",
      "Minibatch loss at step 67000 learning rate = 0.008803: loss = 0.076847, MAE = 0.396235\n",
      "Minibatch loss at step 67500 learning rate = 0.008803: loss = 0.232162, MAE = 0.376821\n",
      "Minibatch loss at step 68000 learning rate = 0.008362: loss = 0.069793, MAE = 0.366785\n",
      "Minibatch loss at step 68500 learning rate = 0.008362: loss = 0.917877, MAE = 0.370141\n",
      "Minibatch loss at step 69000 learning rate = 0.008362: loss = 0.065832, MAE = 0.371199\n",
      "Minibatch loss at step 69500 learning rate = 0.008362: loss = 0.049855, MAE = 0.383548\n",
      "Minibatch loss at step 70000 learning rate = 0.008362: loss = 0.315545, MAE = 0.370598\n",
      "Minibatch loss at step 70500 learning rate = 0.008362: loss = 0.672586, MAE = 0.362036\n",
      "Minibatch loss at step 71000 learning rate = 0.008362: loss = 0.208046, MAE = 0.382019\n",
      "Minibatch loss at step 71500 learning rate = 0.008362: loss = 0.552094, MAE = 0.380609\n",
      "Minibatch loss at step 72000 learning rate = 0.007944: loss = 0.465104, MAE = 0.365359\n",
      "Minibatch loss at step 72500 learning rate = 0.007944: loss = 1.120016, MAE = 0.366130\n",
      "Minibatch loss at step 73000 learning rate = 0.007944: loss = 0.975618, MAE = 0.364910\n",
      "Minibatch loss at step 73500 learning rate = 0.007944: loss = 0.231176, MAE = 0.368702\n",
      "Minibatch loss at step 74000 learning rate = 0.007944: loss = 0.335120, MAE = 0.372732\n",
      "Minibatch loss at step 74500 learning rate = 0.007944: loss = 0.722922, MAE = 0.364616\n",
      "Minibatch loss at step 75000 learning rate = 0.007944: loss = 0.031702, MAE = 0.385253\n",
      "Minibatch loss at step 75500 learning rate = 0.007944: loss = 0.740558, MAE = 0.366173\n",
      "Minibatch loss at step 76000 learning rate = 0.007547: loss = 0.808319, MAE = 0.394321\n",
      "Minibatch loss at step 76500 learning rate = 0.007547: loss = 0.572071, MAE = 0.355576\n",
      "Minibatch loss at step 77000 learning rate = 0.007547: loss = 0.035103, MAE = 0.376488\n",
      "Minibatch loss at step 77500 learning rate = 0.007547: loss = 0.024030, MAE = 0.377531\n",
      "Minibatch loss at step 78000 learning rate = 0.007547: loss = 0.164943, MAE = 0.370604\n",
      "Minibatch loss at step 78500 learning rate = 0.007547: loss = 0.431348, MAE = 0.370829\n",
      "Minibatch loss at step 79000 learning rate = 0.007547: loss = 0.166827, MAE = 0.370910\n",
      "Minibatch loss at step 79500 learning rate = 0.007547: loss = 0.477081, MAE = 0.365969\n",
      "Minibatch loss at step 80000 learning rate = 0.007170: loss = 0.021476, MAE = 0.376227\n",
      "Minibatch loss at step 80500 learning rate = 0.007170: loss = 0.035270, MAE = 0.382799\n",
      "Minibatch loss at step 81000 learning rate = 0.007170: loss = 0.087841, MAE = 0.372037\n",
      "Minibatch loss at step 81500 learning rate = 0.007170: loss = 0.016849, MAE = 0.366248\n",
      "Minibatch loss at step 82000 learning rate = 0.007170: loss = 0.563843, MAE = 0.410746\n",
      "Minibatch loss at step 82500 learning rate = 0.007170: loss = 0.211434, MAE = 0.360728\n",
      "Minibatch loss at step 83000 learning rate = 0.007170: loss = 0.400171, MAE = 0.373273\n",
      "Minibatch loss at step 83500 learning rate = 0.007170: loss = 0.936384, MAE = 0.356192\n",
      "Minibatch loss at step 84000 learning rate = 0.006811: loss = 0.819715, MAE = 0.364304\n",
      "Minibatch loss at step 84500 learning rate = 0.006811: loss = 0.768212, MAE = 0.358716\n",
      "Minibatch loss at step 85000 learning rate = 0.006811: loss = 0.046482, MAE = 0.385037\n",
      "Minibatch loss at step 85500 learning rate = 0.006811: loss = 0.286569, MAE = 0.362229\n",
      "Minibatch loss at step 86000 learning rate = 0.006811: loss = 0.680562, MAE = 0.360916\n",
      "Minibatch loss at step 86500 learning rate = 0.006811: loss = 0.034813, MAE = 0.376969\n",
      "Minibatch loss at step 87000 learning rate = 0.006811: loss = 0.987199, MAE = 0.371450\n",
      "Minibatch loss at step 87500 learning rate = 0.006811: loss = 0.039340, MAE = 0.379008\n",
      "Minibatch loss at step 88000 learning rate = 0.006471: loss = 0.297674, MAE = 0.373548\n",
      "Minibatch loss at step 88500 learning rate = 0.006471: loss = 0.046756, MAE = 0.369211\n",
      "Minibatch loss at step 89000 learning rate = 0.006471: loss = 0.220558, MAE = 0.380679\n",
      "Minibatch loss at step 89500 learning rate = 0.006471: loss = 0.451230, MAE = 0.366335\n",
      "Minibatch loss at step 90000 learning rate = 0.006471: loss = 0.765203, MAE = 0.394753\n",
      "Minibatch loss at step 90500 learning rate = 0.006471: loss = 0.342416, MAE = 0.389895\n",
      "Minibatch loss at step 91000 learning rate = 0.006471: loss = 0.392418, MAE = 0.371927\n",
      "Minibatch loss at step 91500 learning rate = 0.006471: loss = 0.044775, MAE = 0.398016\n",
      "Minibatch loss at step 92000 learning rate = 0.006147: loss = 0.021152, MAE = 0.377512\n",
      "Minibatch loss at step 92500 learning rate = 0.006147: loss = 0.075005, MAE = 0.370395\n",
      "Minibatch loss at step 93000 learning rate = 0.006147: loss = 0.252797, MAE = 0.370332\n",
      "Minibatch loss at step 93500 learning rate = 0.006147: loss = 0.036760, MAE = 0.368652\n",
      "Minibatch loss at step 94000 learning rate = 0.006147: loss = 0.822677, MAE = 0.361586\n",
      "Minibatch loss at step 94500 learning rate = 0.006147: loss = 0.249342, MAE = 0.362867\n",
      "Minibatch loss at step 95000 learning rate = 0.006147: loss = 0.057391, MAE = 0.371782\n",
      "Minibatch loss at step 95500 learning rate = 0.006147: loss = 0.229854, MAE = 0.359708\n",
      "Minibatch loss at step 96000 learning rate = 0.005840: loss = 0.025627, MAE = 0.370533\n",
      "Minibatch loss at step 96500 learning rate = 0.005840: loss = 0.856673, MAE = 0.359519\n",
      "Minibatch loss at step 97000 learning rate = 0.005840: loss = 0.176411, MAE = 0.363193\n",
      "Minibatch loss at step 97500 learning rate = 0.005840: loss = 0.288443, MAE = 0.363837\n",
      "Minibatch loss at step 98000 learning rate = 0.005840: loss = 0.686584, MAE = 0.370122\n",
      "Minibatch loss at step 98500 learning rate = 0.005840: loss = 0.269390, MAE = 0.359507\n",
      "Minibatch loss at step 99000 learning rate = 0.005840: loss = 0.048208, MAE = 0.370277\n",
      "Minibatch loss at step 99500 learning rate = 0.005840: loss = 0.319422, MAE = 0.361690\n",
      "Minibatch loss at step 100000 learning rate = 0.005548: loss = 1.389789, MAE = 0.371264\n",
      "Minibatch loss at step 100500 learning rate = 0.005548: loss = 0.027809, MAE = 0.374757\n",
      "Minibatch loss at step 101000 learning rate = 0.005548: loss = 0.021011, MAE = 0.376631\n",
      "Minibatch loss at step 101500 learning rate = 0.005548: loss = 0.058581, MAE = 0.366221\n",
      "Minibatch loss at step 102000 learning rate = 0.005548: loss = 0.156167, MAE = 0.365307\n",
      "Minibatch loss at step 102500 learning rate = 0.005548: loss = 0.605910, MAE = 0.364200\n",
      "Minibatch loss at step 103000 learning rate = 0.005548: loss = 0.558288, MAE = 0.361244\n",
      "Minibatch loss at step 103500 learning rate = 0.005548: loss = 0.014416, MAE = 0.353435\n",
      "Minibatch loss at step 104000 learning rate = 0.005270: loss = 0.867400, MAE = 0.354294\n",
      "Minibatch loss at step 104500 learning rate = 0.005270: loss = 0.044352, MAE = 0.361608\n",
      "Minibatch loss at step 105000 learning rate = 0.005270: loss = 0.213841, MAE = 0.360815\n",
      "Minibatch loss at step 105500 learning rate = 0.005270: loss = 0.372389, MAE = 0.367117\n",
      "Minibatch loss at step 106000 learning rate = 0.005270: loss = 0.709119, MAE = 0.372474\n",
      "Minibatch loss at step 106500 learning rate = 0.005270: loss = 0.029491, MAE = 0.367674\n",
      "Minibatch loss at step 107000 learning rate = 0.005270: loss = 0.051554, MAE = 0.364971\n",
      "Minibatch loss at step 107500 learning rate = 0.005270: loss = 0.085759, MAE = 0.366724\n",
      "Minibatch loss at step 108000 learning rate = 0.005007: loss = 0.215917, MAE = 0.370465\n",
      "Minibatch loss at step 108500 learning rate = 0.005007: loss = 0.807555, MAE = 0.369314\n",
      "Minibatch loss at step 109000 learning rate = 0.005007: loss = 0.012595, MAE = 0.363466\n",
      "Minibatch loss at step 109500 learning rate = 0.005007: loss = 1.568673, MAE = 0.368133\n",
      "Minibatch loss at step 110000 learning rate = 0.005007: loss = 0.916020, MAE = 0.353543\n",
      "Minibatch loss at step 110500 learning rate = 0.005007: loss = 0.545973, MAE = 0.362281\n",
      "Minibatch loss at step 111000 learning rate = 0.005007: loss = 0.363017, MAE = 0.358108\n",
      "Minibatch loss at step 111500 learning rate = 0.005007: loss = 0.039508, MAE = 0.390835\n",
      "Minibatch loss at step 112000 learning rate = 0.004757: loss = 0.106843, MAE = 0.363628\n",
      "Minibatch loss at step 112500 learning rate = 0.004757: loss = 0.899460, MAE = 0.364583\n",
      "Minibatch loss at step 113000 learning rate = 0.004757: loss = 0.161931, MAE = 0.365349\n",
      "Minibatch loss at step 113500 learning rate = 0.004757: loss = 0.666999, MAE = 0.365438\n",
      "Minibatch loss at step 114000 learning rate = 0.004757: loss = 0.037159, MAE = 0.359946\n",
      "Minibatch loss at step 114500 learning rate = 0.004757: loss = 0.286859, MAE = 0.362326\n",
      "Minibatch loss at step 115000 learning rate = 0.004757: loss = 0.013544, MAE = 0.368394\n",
      "Minibatch loss at step 115500 learning rate = 0.004757: loss = 0.655850, MAE = 0.369830\n",
      "Minibatch loss at step 116000 learning rate = 0.004519: loss = 0.129335, MAE = 0.371834\n",
      "Minibatch loss at step 116500 learning rate = 0.004519: loss = 0.619603, MAE = 0.360782\n",
      "Minibatch loss at step 117000 learning rate = 0.004519: loss = 0.008066, MAE = 0.367975\n",
      "Minibatch loss at step 117500 learning rate = 0.004519: loss = 0.268913, MAE = 0.363769\n",
      "Minibatch loss at step 118000 learning rate = 0.004519: loss = 0.755592, MAE = 0.358479\n",
      "Minibatch loss at step 118500 learning rate = 0.004519: loss = 0.079694, MAE = 0.365576\n",
      "Minibatch loss at step 119000 learning rate = 0.004519: loss = 0.716730, MAE = 0.354849\n",
      "Minibatch loss at step 119500 learning rate = 0.004519: loss = 0.730438, MAE = 0.351775\n",
      "Minibatch loss at step 120000 learning rate = 0.004293: loss = 1.001271, MAE = 0.354785\n",
      "Minibatch loss at step 120500 learning rate = 0.004293: loss = 0.069551, MAE = 0.360838\n",
      "Minibatch loss at step 121000 learning rate = 0.004293: loss = 0.568194, MAE = 0.357117\n",
      "Minibatch loss at step 121500 learning rate = 0.004293: loss = 0.229121, MAE = 0.359461\n",
      "Minibatch loss at step 122000 learning rate = 0.004293: loss = 0.905121, MAE = 0.356827\n",
      "Minibatch loss at step 122500 learning rate = 0.004293: loss = 0.053372, MAE = 0.366816\n",
      "Minibatch loss at step 123000 learning rate = 0.004293: loss = 0.052451, MAE = 0.362919\n",
      "Minibatch loss at step 123500 learning rate = 0.004293: loss = 0.979259, MAE = 0.366815\n",
      "Minibatch loss at step 124000 learning rate = 0.004078: loss = 0.106223, MAE = 0.357029\n",
      "Minibatch loss at step 124500 learning rate = 0.004078: loss = 0.432813, MAE = 0.366834\n",
      "Minibatch loss at step 125000 learning rate = 0.004078: loss = 0.559280, MAE = 0.355433\n",
      "Minibatch loss at step 125500 learning rate = 0.004078: loss = 0.386681, MAE = 0.358649\n",
      "Minibatch loss at step 126000 learning rate = 0.004078: loss = 0.215818, MAE = 0.359498\n",
      "Minibatch loss at step 126500 learning rate = 0.004078: loss = 0.030466, MAE = 0.361940\n",
      "Minibatch loss at step 127000 learning rate = 0.004078: loss = 0.305061, MAE = 0.362075\n",
      "Minibatch loss at step 127500 learning rate = 0.004078: loss = 0.669995, MAE = 0.354381\n",
      "Minibatch loss at step 128000 learning rate = 0.003874: loss = 0.027505, MAE = 0.364766\n",
      "Minibatch loss at step 128500 learning rate = 0.003874: loss = 0.666451, MAE = 0.355690\n",
      "Minibatch loss at step 129000 learning rate = 0.003874: loss = 0.029646, MAE = 0.358476\n",
      "Minibatch loss at step 129500 learning rate = 0.003874: loss = 0.726710, MAE = 0.353265\n",
      "Minibatch loss at step 130000 learning rate = 0.003874: loss = 0.015885, MAE = 0.362135\n",
      "Minibatch loss at step 130500 learning rate = 0.003874: loss = 0.025310, MAE = 0.358976\n",
      "Minibatch loss at step 131000 learning rate = 0.003874: loss = 0.121446, MAE = 0.363445\n",
      "Minibatch loss at step 131500 learning rate = 0.003874: loss = 0.273546, MAE = 0.357895\n",
      "Minibatch loss at step 132000 learning rate = 0.003681: loss = 0.723409, MAE = 0.355163\n",
      "Minibatch loss at step 132500 learning rate = 0.003681: loss = 0.923220, MAE = 0.356662\n",
      "Minibatch loss at step 133000 learning rate = 0.003681: loss = 0.202275, MAE = 0.358679\n",
      "Minibatch loss at step 133500 learning rate = 0.003681: loss = 0.961633, MAE = 0.363502\n",
      "Minibatch loss at step 134000 learning rate = 0.003681: loss = 0.070295, MAE = 0.358894\n",
      "Minibatch loss at step 134500 learning rate = 0.003681: loss = 0.466124, MAE = 0.356183\n",
      "Minibatch loss at step 135000 learning rate = 0.003681: loss = 0.534694, MAE = 0.363262\n",
      "Minibatch loss at step 135500 learning rate = 0.003681: loss = 0.558972, MAE = 0.354665\n",
      "Minibatch loss at step 136000 learning rate = 0.003496: loss = 0.193510, MAE = 0.360024\n",
      "Minibatch loss at step 136500 learning rate = 0.003496: loss = 1.079414, MAE = 0.358049\n",
      "Minibatch loss at step 137000 learning rate = 0.003496: loss = 0.577492, MAE = 0.371535\n",
      "Minibatch loss at step 137500 learning rate = 0.003496: loss = 0.152360, MAE = 0.355387\n",
      "Minibatch loss at step 138000 learning rate = 0.003496: loss = 0.030694, MAE = 0.363709\n",
      "Minibatch loss at step 138500 learning rate = 0.003496: loss = 0.513680, MAE = 0.361721\n",
      "Minibatch loss at step 139000 learning rate = 0.003496: loss = 0.564777, MAE = 0.358055\n",
      "Minibatch loss at step 139500 learning rate = 0.003496: loss = 0.598500, MAE = 0.357148\n",
      "Minibatch loss at step 140000 learning rate = 0.003322: loss = 0.018921, MAE = 0.358184\n",
      "Minibatch loss at step 140500 learning rate = 0.003322: loss = 0.739184, MAE = 0.353286\n",
      "Minibatch loss at step 141000 learning rate = 0.003322: loss = 0.977661, MAE = 0.358162\n",
      "Minibatch loss at step 141500 learning rate = 0.003322: loss = 0.017828, MAE = 0.368764\n",
      "Minibatch loss at step 142000 learning rate = 0.003322: loss = 0.013063, MAE = 0.362709\n",
      "Minibatch loss at step 142500 learning rate = 0.003322: loss = 0.005034, MAE = 0.358975\n",
      "Minibatch loss at step 143000 learning rate = 0.003322: loss = 0.292124, MAE = 0.365582\n",
      "Minibatch loss at step 143500 learning rate = 0.003322: loss = 0.592265, MAE = 0.352840\n",
      "Minibatch loss at step 144000 learning rate = 0.003156: loss = 0.320945, MAE = 0.357626\n",
      "Minibatch loss at step 144500 learning rate = 0.003156: loss = 0.011915, MAE = 0.355417\n",
      "Minibatch loss at step 145000 learning rate = 0.003156: loss = 0.963063, MAE = 0.360954\n",
      "Minibatch loss at step 145500 learning rate = 0.003156: loss = 0.195583, MAE = 0.355844\n",
      "Minibatch loss at step 146000 learning rate = 0.003156: loss = 0.369022, MAE = 0.357627\n",
      "Minibatch loss at step 146500 learning rate = 0.003156: loss = 0.089942, MAE = 0.357176\n",
      "Minibatch loss at step 147000 learning rate = 0.003156: loss = 1.512436, MAE = 0.371042\n",
      "Minibatch loss at step 147500 learning rate = 0.003156: loss = 0.831544, MAE = 0.356276\n",
      "Minibatch loss at step 148000 learning rate = 0.002998: loss = 0.010129, MAE = 0.356187\n",
      "Minibatch loss at step 148500 learning rate = 0.002998: loss = 0.052409, MAE = 0.357960\n",
      "Minibatch loss at step 149000 learning rate = 0.002998: loss = 0.266363, MAE = 0.353986\n",
      "Minibatch loss at step 149500 learning rate = 0.002998: loss = 0.180607, MAE = 0.355157\n",
      "Minibatch loss at step 150000 learning rate = 0.002998: loss = 0.778453, MAE = 0.353565\n",
      "Minibatch loss at step 150500 learning rate = 0.002998: loss = 0.096832, MAE = 0.353604\n",
      "Minibatch loss at step 151000 learning rate = 0.002998: loss = 0.488209, MAE = 0.357411\n",
      "Minibatch loss at step 151500 learning rate = 0.002998: loss = 0.084299, MAE = 0.363521\n",
      "Minibatch loss at step 152000 learning rate = 0.002848: loss = 0.401318, MAE = 0.367179\n",
      "Minibatch loss at step 152500 learning rate = 0.002848: loss = 0.360505, MAE = 0.369293\n",
      "Minibatch loss at step 153000 learning rate = 0.002848: loss = 0.675439, MAE = 0.357251\n",
      "Minibatch loss at step 153500 learning rate = 0.002848: loss = 0.031031, MAE = 0.353438\n",
      "Minibatch loss at step 154000 learning rate = 0.002848: loss = 0.007815, MAE = 0.357170\n",
      "Minibatch loss at step 154500 learning rate = 0.002848: loss = 0.919791, MAE = 0.355607\n",
      "Minibatch loss at step 155000 learning rate = 0.002848: loss = 0.694859, MAE = 0.350556\n",
      "Minibatch loss at step 155500 learning rate = 0.002848: loss = 0.450965, MAE = 0.353982\n",
      "Minibatch loss at step 156000 learning rate = 0.002706: loss = 0.309553, MAE = 0.356096\n",
      "Minibatch loss at step 156500 learning rate = 0.002706: loss = 0.197063, MAE = 0.357033\n",
      "Minibatch loss at step 157000 learning rate = 0.002706: loss = 0.034178, MAE = 0.366316\n",
      "Minibatch loss at step 157500 learning rate = 0.002706: loss = 1.117380, MAE = 0.356084\n",
      "Minibatch loss at step 158000 learning rate = 0.002706: loss = 0.382296, MAE = 0.358795\n",
      "Minibatch loss at step 158500 learning rate = 0.002706: loss = 0.297242, MAE = 0.355820\n",
      "Minibatch loss at step 159000 learning rate = 0.002706: loss = 0.337239, MAE = 0.355364\n",
      "Minibatch loss at step 159500 learning rate = 0.002706: loss = 0.428213, MAE = 0.353189\n",
      "Minibatch loss at step 160000 learning rate = 0.002570: loss = 0.016281, MAE = 0.356842\n",
      "Minibatch loss at step 160500 learning rate = 0.002570: loss = 0.684776, MAE = 0.353528\n",
      "Minibatch loss at step 161000 learning rate = 0.002570: loss = 0.818139, MAE = 0.355989\n",
      "Minibatch loss at step 161500 learning rate = 0.002570: loss = 0.803117, MAE = 0.352967\n",
      "Minibatch loss at step 162000 learning rate = 0.002570: loss = 0.361695, MAE = 0.354993\n",
      "Minibatch loss at step 162500 learning rate = 0.002570: loss = 0.838367, MAE = 0.363149\n",
      "Minibatch loss at step 163000 learning rate = 0.002570: loss = 0.479377, MAE = 0.351661\n",
      "Minibatch loss at step 163500 learning rate = 0.002570: loss = 0.015700, MAE = 0.355812\n",
      "Minibatch loss at step 164000 learning rate = 0.002442: loss = 0.109390, MAE = 0.363004\n",
      "Minibatch loss at step 164500 learning rate = 0.002442: loss = 0.006591, MAE = 0.353952\n",
      "Minibatch loss at step 165000 learning rate = 0.002442: loss = 0.796092, MAE = 0.355404\n",
      "Minibatch loss at step 165500 learning rate = 0.002442: loss = 0.017790, MAE = 0.355124\n",
      "Minibatch loss at step 166000 learning rate = 0.002442: loss = 0.005540, MAE = 0.353597\n",
      "Minibatch loss at step 166500 learning rate = 0.002442: loss = 0.192245, MAE = 0.352827\n",
      "Minibatch loss at step 167000 learning rate = 0.002442: loss = 0.445825, MAE = 0.358295\n",
      "Minibatch loss at step 167500 learning rate = 0.002442: loss = 0.503424, MAE = 0.356343\n",
      "Minibatch loss at step 168000 learning rate = 0.002320: loss = 0.123552, MAE = 0.356119\n",
      "Minibatch loss at step 168500 learning rate = 0.002320: loss = 0.929220, MAE = 0.357280\n",
      "Minibatch loss at step 169000 learning rate = 0.002320: loss = 0.852926, MAE = 0.355625\n",
      "Minibatch loss at step 169500 learning rate = 0.002320: loss = 0.429831, MAE = 0.353951\n",
      "Minibatch loss at step 170000 learning rate = 0.002320: loss = 0.011866, MAE = 0.355594\n",
      "Minibatch loss at step 170500 learning rate = 0.002320: loss = 0.370774, MAE = 0.356870\n",
      "Minibatch loss at step 171000 learning rate = 0.002320: loss = 0.539309, MAE = 0.352006\n",
      "Minibatch loss at step 171500 learning rate = 0.002320: loss = 0.015891, MAE = 0.354394\n",
      "Minibatch loss at step 172000 learning rate = 0.002204: loss = 0.929056, MAE = 0.354199\n",
      "Minibatch loss at step 172500 learning rate = 0.002204: loss = 0.866974, MAE = 0.363334\n",
      "Minibatch loss at step 173000 learning rate = 0.002204: loss = 0.797424, MAE = 0.352067\n",
      "Minibatch loss at step 173500 learning rate = 0.002204: loss = 0.025783, MAE = 0.355524\n",
      "Minibatch loss at step 174000 learning rate = 0.002204: loss = 0.406259, MAE = 0.354334\n",
      "Minibatch loss at step 174500 learning rate = 0.002204: loss = 0.397083, MAE = 0.352450\n",
      "Minibatch loss at step 175000 learning rate = 0.002204: loss = 0.521792, MAE = 0.352347\n",
      "Minibatch loss at step 175500 learning rate = 0.002204: loss = 0.113927, MAE = 0.355378\n",
      "Minibatch loss at step 176000 learning rate = 0.002093: loss = 0.757880, MAE = 0.351039\n",
      "Minibatch loss at step 176500 learning rate = 0.002093: loss = 0.049272, MAE = 0.353695\n",
      "Minibatch loss at step 177000 learning rate = 0.002093: loss = 0.419121, MAE = 0.358605\n",
      "Minibatch loss at step 177500 learning rate = 0.002093: loss = 0.154888, MAE = 0.354532\n",
      "Minibatch loss at step 178000 learning rate = 0.002093: loss = 0.003184, MAE = 0.355850\n",
      "Minibatch loss at step 178500 learning rate = 0.002093: loss = 0.579102, MAE = 0.354815\n",
      "Minibatch loss at step 179000 learning rate = 0.002093: loss = 0.648074, MAE = 0.351484\n",
      "Minibatch loss at step 179500 learning rate = 0.002093: loss = 0.586314, MAE = 0.356843\n",
      "Minibatch loss at step 180000 learning rate = 0.001989: loss = 0.019418, MAE = 0.350603\n",
      "Minibatch loss at step 180500 learning rate = 0.001989: loss = 0.043634, MAE = 0.356984\n",
      "Minibatch loss at step 181000 learning rate = 0.001989: loss = 0.250151, MAE = 0.351388\n",
      "Minibatch loss at step 181500 learning rate = 0.001989: loss = 0.433609, MAE = 0.356285\n",
      "Minibatch loss at step 182000 learning rate = 0.001989: loss = 0.554000, MAE = 0.352788\n",
      "Minibatch loss at step 182500 learning rate = 0.001989: loss = 1.014547, MAE = 0.355177\n",
      "Minibatch loss at step 183000 learning rate = 0.001989: loss = 0.025305, MAE = 0.356987\n",
      "Minibatch loss at step 183500 learning rate = 0.001989: loss = 0.503097, MAE = 0.353672\n",
      "Minibatch loss at step 184000 learning rate = 0.001889: loss = 0.005313, MAE = 0.353628\n",
      "Minibatch loss at step 184500 learning rate = 0.001889: loss = 0.209357, MAE = 0.353787\n",
      "Minibatch loss at step 185000 learning rate = 0.001889: loss = 0.317232, MAE = 0.352965\n",
      "Minibatch loss at step 185500 learning rate = 0.001889: loss = 0.244270, MAE = 0.354505\n",
      "Minibatch loss at step 186000 learning rate = 0.001889: loss = 0.002301, MAE = 0.351547\n",
      "Minibatch loss at step 186500 learning rate = 0.001889: loss = 1.101132, MAE = 0.351544\n",
      "Minibatch loss at step 187000 learning rate = 0.001889: loss = 0.448435, MAE = 0.354127\n",
      "Minibatch loss at step 187500 learning rate = 0.001889: loss = 0.254611, MAE = 0.353024\n",
      "Minibatch loss at step 188000 learning rate = 0.001795: loss = 0.689975, MAE = 0.355481\n",
      "Minibatch loss at step 188500 learning rate = 0.001795: loss = 0.030970, MAE = 0.355788\n",
      "Minibatch loss at step 189000 learning rate = 0.001795: loss = 0.028681, MAE = 0.356126\n",
      "Minibatch loss at step 189500 learning rate = 0.001795: loss = 0.598885, MAE = 0.353471\n",
      "Minibatch loss at step 190000 learning rate = 0.001795: loss = 0.081264, MAE = 0.352399\n",
      "Minibatch loss at step 190500 learning rate = 0.001795: loss = 0.626927, MAE = 0.352441\n",
      "Minibatch loss at step 191000 learning rate = 0.001795: loss = 0.009268, MAE = 0.352056\n",
      "Minibatch loss at step 191500 learning rate = 0.001795: loss = 0.193237, MAE = 0.353046\n",
      "Minibatch loss at step 192000 learning rate = 0.001705: loss = 0.060865, MAE = 0.352264\n",
      "Minibatch loss at step 192500 learning rate = 0.001705: loss = 0.201193, MAE = 0.356080\n",
      "Minibatch loss at step 193000 learning rate = 0.001705: loss = 0.915601, MAE = 0.353032\n",
      "Minibatch loss at step 193500 learning rate = 0.001705: loss = 0.034389, MAE = 0.353615\n",
      "Minibatch loss at step 194000 learning rate = 0.001705: loss = 0.443988, MAE = 0.352480\n",
      "Minibatch loss at step 194500 learning rate = 0.001705: loss = 0.666215, MAE = 0.353229\n",
      "Minibatch loss at step 195000 learning rate = 0.001705: loss = 0.153421, MAE = 0.355603\n",
      "Minibatch loss at step 195500 learning rate = 0.001705: loss = 0.412834, MAE = 0.352719\n",
      "Minibatch loss at step 196000 learning rate = 0.001620: loss = 0.136822, MAE = 0.353023\n",
      "Minibatch loss at step 196500 learning rate = 0.001620: loss = 0.989177, MAE = 0.353668\n",
      "Minibatch loss at step 197000 learning rate = 0.001620: loss = 0.009032, MAE = 0.353605\n",
      "Minibatch loss at step 197500 learning rate = 0.001620: loss = 0.026917, MAE = 0.354215\n",
      "Minibatch loss at step 198000 learning rate = 0.001620: loss = 0.008821, MAE = 0.360099\n",
      "Minibatch loss at step 198500 learning rate = 0.001620: loss = 0.728509, MAE = 0.353380\n",
      "Minibatch loss at step 199000 learning rate = 0.001620: loss = 0.040795, MAE = 0.354634\n",
      "Minibatch loss at step 199500 learning rate = 0.001620: loss = 0.452702, MAE = 0.354040\n",
      "Minibatch loss at step 200000 learning rate = 0.001539: loss = 0.169060, MAE = 0.350502\n",
      "5889.10695601\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "num_steps = 200001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "        batch_data1 = train_data_2d[offset:(offset + batch_size), :, :, :]\n",
    "        batch_data2 = train_data_NWP[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_label[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset1 : batch_data1,  tf_train_dataset2 : batch_data2, tf_train_labels : batch_labels}\n",
    "        _, l, lr = session.run([optimizer, loss, learning_rate], feed_dict=feed_dict)\n",
    "        if step % 500 == 0:\n",
    "            print('Minibatch loss at step %d learning rate = %f: loss = %f, MAE = %f' % (step, lr, l, MAE.eval()))\n",
    "et = time.time()\n",
    "print(et - st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "pre = Binarizer(threshold = 1.01)\n",
    "b_train_label = pre.transform(train_label.reshape(1, -1))\n",
    "b_test_label = pre.transform(test_label.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_train_label = 1 - b_train_label[0]\n",
    "c_test_label = 1 - b_test_label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_n = np.hstack((train_data_1_n, train_data_2_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data_n_resample = train_data_n\n",
    "c_train_label_resample = c_train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining classes statistics... 2 classes detected: Counter({0.0: 114677, 1.0: 1980})\n",
      "Finding the 5 nearest neighbours...\n",
      "done!\n",
      "Creating synthetic samples...Generated 1460 new samples ...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "#need to balanced the dataset\n",
    "from unbalanced_dataset.over_sampling import SMOTE\n",
    "sm = SMOTE(ratio = 0.03, kind='regular')\n",
    "train_data_n_resample, c_train_label_resample = sm.fit_transform(train_data_n, c_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_n_resample_ASOS = train_data_n_resample[:, :70]\n",
    "train_data_n_resample_NWP = train_data_n_resample[:, 70:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_ASOS_1d = np.hstack((train_data_n_resample_ASOS[:, :63], train_data_n_resample_ASOS[:, -1:],train_data_n_resample_ASOS[:, 63:-1] ))\n",
    "train_data_ASOS_2d = train_data_ASOS_1d.reshape(train_data_ASOS_1d.shape[0], 7, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_train_label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change from Indice to Vector\n",
    "''''''\n",
    "def makeIndicatorVars(T):\n",
    "    # Make sure T is two-dimensiona. Should be nSamples x 1.\n",
    "    if T.ndim == 1:\n",
    "        T = T.reshape((-1,1))    \n",
    "    return (T == np.unique(T)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_train_label = makeIndicatorVars(c_train_label.reshape(-1, 1))\n",
    "v_test_label = makeIndicatorVars(c_test_label.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_train_label_resample = makeIndicatorVars(c_train_label_resample.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((116657, 2), (29165, 2))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_train_label.shape, v_test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shuffle the data set\n",
    "arr = np.arange(c_train_label_resample.shape[0])\n",
    "np.random.shuffle(arr)\n",
    "train_data_ASOS_2d_s =  train_data_ASOS_2d[arr]\n",
    "train_data_n_resample_NWP_s = train_data_n_resample_NWP[arr]\n",
    "v_train_label_resample_s = v_train_label_resample[arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "patch_size = 7\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "num_channels = 1\n",
    "num_labels = 2\n",
    "num_feature = 10\n",
    "ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset1 = tf.placeholder(tf.float32, shape=(batch_size, 7, num_feature, num_channels))\n",
    "    tf_train_dataset2 = tf.placeholder(tf.float32, shape=(batch_size, 12))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_test_dataset1 = tf.constant(test_data_2d, dtype=tf.float32)\n",
    "    tf_test_dataset2 = tf.constant(test_data_NWP, dtype=tf.float32)\n",
    "    tf_test_labels = tf.constant(v_test_label, dtype=tf.float32) \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, 1, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, 1, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([num_feature * 7 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden + 12, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model(data1, data2):\n",
    "        conv = tf.nn.conv2d(data1, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        concat = tf.concat(1, [hidden, data2])\n",
    "        return tf.matmul(concat, layer4_weights) + layer4_biases\n",
    "\n",
    "    def acc(predict, label):\n",
    "        #correct_prediction = tf.equal(predicted_label, tf_train_label)\n",
    "        correct_prediction = tf.equal(tf.argmax(predict, 1), tf.argmax(label, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        predict_event = tf.reduce_sum(tf.argmax(predict, 1))\n",
    "        label_event = tf.reduce_sum(tf.argmax(label, 1))\n",
    "        true_positive = tf.reduce_sum(tf.cast(tf.equal((tf.argmax(predict, 1) + tf.argmax(label, 1)), 2), tf.int64))\n",
    "        true_negative = tf.reduce_sum(tf.cast(tf.equal((tf.argmax(predict, 1) + tf.argmax(label, 1)), 0), tf.int64))\n",
    "        false_positive = predict_event - true_positive \n",
    "        false_negative = label_event - true_positive\n",
    "        return accuracy, false_positive, false_negative, true_positive, true_negative\n",
    "    def ROC(FP, FN, TP, TN):\n",
    "        TP_percent = TP / (TP + FN) \n",
    "        FP_percent = FP / (FP + TN) \n",
    "        return TP_percent, FP_percent\n",
    "    \n",
    "    def PRC(FP, FN, TP, TN):\n",
    "        precision = TP / (TP + FP + 1)\n",
    "        recall = TP / (TP + FN + 1)\n",
    "        csi = TP / (TP + FP + FN)\n",
    "        return precision, recall, csi    \n",
    "    \n",
    "    # Training computation.\n",
    "     # Training computation.\n",
    "    pred = model(tf_train_dataset1, tf_train_dataset2)\n",
    "    class_weight = tf.constant([ratio, 1.0 - ratio])\n",
    "    weighted_pred = tf.mul(pred, class_weight) # shape [batch_size, 2]\n",
    "    #loss = tf.reduce_mean(tf.abs(pred - tf_train_labels))\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(weighted_pred, tf_train_labels)) # Softmax loss\n",
    "    \n",
    "    # Learning rate decay\n",
    "    global_step = tf.Variable(0)\n",
    "    starter_learning_rate = 0.03\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 2000, 0.95, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step = global_step)\n",
    "    \n",
    "    \n",
    "    # Evaluate model\n",
    "    test_pred = model(tf_test_dataset1, tf_test_dataset2)\n",
    "    correct_pred = tf.equal(tf.argmax(test_pred,1), tf.argmax(tf_test_labels,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    test_acc, FP, FN, TP, TN = acc(test_pred, tf_test_labels)\n",
    "    pre, rec, csi = PRC(FP, FN, TP, TN)\n",
    "    \n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(csi, global_step = global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 learning_rate = 0.030000: loss = 0.206536\n",
      "TP = 0, FP = 0, FN = 421, TN = 28744\n",
      "precision = 0.000000, recall = 0.000000, csi = 0.000000\n",
      "Minibatch loss at step 500 learning_rate = 0.030000: loss = 0.029069\n",
      "TP = 293, FP = 315, FN = 128, TN = 28429\n",
      "precision = 0.481117, recall = 0.694313, csi = 0.398098\n",
      "Minibatch loss at step 1000 learning_rate = 0.030000: loss = 0.127750\n",
      "TP = 308, FP = 404, FN = 113, TN = 28340\n",
      "precision = 0.431978, recall = 0.729858, csi = 0.373333\n",
      "Minibatch loss at step 1500 learning_rate = 0.030000: loss = 0.065384\n",
      "TP = 284, FP = 236, FN = 137, TN = 28508\n",
      "precision = 0.545106, recall = 0.672986, csi = 0.432268\n",
      "Minibatch loss at step 2000 learning_rate = 0.028500: loss = 0.054908\n",
      "TP = 310, FP = 354, FN = 111, TN = 28390\n",
      "precision = 0.466165, recall = 0.734597, csi = 0.400000\n",
      "Minibatch loss at step 2500 learning_rate = 0.028500: loss = 0.034386\n",
      "TP = 312, FP = 339, FN = 109, TN = 28405\n",
      "precision = 0.478528, recall = 0.739336, csi = 0.410526\n",
      "Minibatch loss at step 3000 learning_rate = 0.028500: loss = 0.018926\n",
      "TP = 292, FP = 229, FN = 129, TN = 28515\n",
      "precision = 0.559387, recall = 0.691943, csi = 0.449231\n",
      "Minibatch loss at step 3500 learning_rate = 0.028500: loss = 0.018594\n",
      "TP = 267, FP = 190, FN = 154, TN = 28554\n",
      "precision = 0.582969, recall = 0.632701, csi = 0.436989\n",
      "Minibatch loss at step 4000 learning_rate = 0.027075: loss = 0.032000\n",
      "TP = 267, FP = 166, FN = 154, TN = 28578\n",
      "precision = 0.615207, recall = 0.632701, csi = 0.454855\n",
      "Minibatch loss at step 4500 learning_rate = 0.027075: loss = 0.028051\n",
      "TP = 272, FP = 183, FN = 149, TN = 28561\n",
      "precision = 0.596491, recall = 0.644550, csi = 0.450331\n",
      "Minibatch loss at step 5000 learning_rate = 0.027075: loss = 0.053973\n",
      "TP = 314, FP = 318, FN = 107, TN = 28426\n",
      "precision = 0.496051, recall = 0.744076, csi = 0.424899\n",
      "Minibatch loss at step 5500 learning_rate = 0.027075: loss = 0.040270\n",
      "TP = 315, FP = 317, FN = 106, TN = 28427\n",
      "precision = 0.497630, recall = 0.746445, csi = 0.426829\n",
      "Minibatch loss at step 6000 learning_rate = 0.025721: loss = 0.072030\n",
      "TP = 237, FP = 122, FN = 184, TN = 28622\n",
      "precision = 0.658333, recall = 0.561611, csi = 0.436464\n",
      "Minibatch loss at step 6500 learning_rate = 0.025721: loss = 0.092809\n",
      "TP = 325, FP = 382, FN = 96, TN = 28362\n",
      "precision = 0.459040, recall = 0.770142, csi = 0.404732\n",
      "Minibatch loss at step 7000 learning_rate = 0.025721: loss = 0.084622\n",
      "TP = 329, FP = 427, FN = 92, TN = 28317\n",
      "precision = 0.434610, recall = 0.779621, csi = 0.387972\n",
      "Minibatch loss at step 7500 learning_rate = 0.025721: loss = 0.074748\n",
      "TP = 288, FP = 221, FN = 133, TN = 28523\n",
      "precision = 0.564706, recall = 0.682464, csi = 0.448598\n",
      "Minibatch loss at step 8000 learning_rate = 0.024435: loss = 0.033349\n",
      "TP = 328, FP = 386, FN = 93, TN = 28358\n",
      "precision = 0.458741, recall = 0.777251, csi = 0.406444\n",
      "Minibatch loss at step 8500 learning_rate = 0.024435: loss = 0.059220\n",
      "TP = 226, FP = 98, FN = 195, TN = 28646\n",
      "precision = 0.695385, recall = 0.535545, csi = 0.435453\n",
      "Minibatch loss at step 9000 learning_rate = 0.024435: loss = 0.048489\n",
      "TP = 297, FP = 250, FN = 124, TN = 28494\n",
      "precision = 0.541971, recall = 0.703791, csi = 0.442623\n",
      "Minibatch loss at step 9500 learning_rate = 0.024435: loss = 0.026765\n",
      "TP = 274, FP = 181, FN = 147, TN = 28563\n",
      "precision = 0.600877, recall = 0.649289, csi = 0.455150\n",
      "Minibatch loss at step 10000 learning_rate = 0.023213: loss = 0.020956\n",
      "TP = 275, FP = 180, FN = 146, TN = 28564\n",
      "precision = 0.603070, recall = 0.651659, csi = 0.457571\n",
      "Minibatch loss at step 10500 learning_rate = 0.023213: loss = 0.037070\n",
      "TP = 307, FP = 270, FN = 114, TN = 28474\n",
      "precision = 0.531142, recall = 0.727488, csi = 0.444284\n",
      "Minibatch loss at step 11000 learning_rate = 0.023213: loss = 0.006317\n",
      "TP = 231, FP = 100, FN = 190, TN = 28644\n",
      "precision = 0.695783, recall = 0.547393, csi = 0.443378\n",
      "Minibatch loss at step 11500 learning_rate = 0.023213: loss = 0.034589\n",
      "TP = 228, FP = 100, FN = 193, TN = 28644\n",
      "precision = 0.693009, recall = 0.540284, csi = 0.437620\n",
      "Minibatch loss at step 12000 learning_rate = 0.022053: loss = 0.014133\n",
      "TP = 231, FP = 96, FN = 190, TN = 28648\n",
      "precision = 0.704268, recall = 0.547393, csi = 0.446809\n",
      "Minibatch loss at step 12500 learning_rate = 0.022053: loss = 0.028003\n",
      "TP = 301, FP = 244, FN = 120, TN = 28500\n",
      "precision = 0.551282, recall = 0.713270, csi = 0.452632\n",
      "Minibatch loss at step 13000 learning_rate = 0.022053: loss = 0.048366\n",
      "TP = 278, FP = 183, FN = 143, TN = 28561\n",
      "precision = 0.601732, recall = 0.658768, csi = 0.460265\n",
      "Minibatch loss at step 13500 learning_rate = 0.022053: loss = 0.023763\n",
      "TP = 303, FP = 261, FN = 118, TN = 28483\n",
      "precision = 0.536283, recall = 0.718009, csi = 0.444282\n",
      "Minibatch loss at step 14000 learning_rate = 0.020950: loss = 0.052390\n",
      "TP = 302, FP = 236, FN = 119, TN = 28508\n",
      "precision = 0.560297, recall = 0.715640, csi = 0.459665\n",
      "Minibatch loss at step 14500 learning_rate = 0.020950: loss = 0.037545\n",
      "TP = 262, FP = 133, FN = 159, TN = 28611\n",
      "precision = 0.661616, recall = 0.620853, csi = 0.472924\n",
      "Minibatch loss at step 15000 learning_rate = 0.020950: loss = 0.037188\n",
      "TP = 302, FP = 255, FN = 119, TN = 28489\n",
      "precision = 0.541219, recall = 0.715640, csi = 0.446746\n",
      "Minibatch loss at step 15500 learning_rate = 0.020950: loss = 0.026836\n",
      "TP = 313, FP = 339, FN = 108, TN = 28405\n",
      "precision = 0.479326, recall = 0.741706, csi = 0.411842\n",
      "Minibatch loss at step 16000 learning_rate = 0.019903: loss = 0.019152\n",
      "TP = 213, FP = 76, FN = 208, TN = 28668\n",
      "precision = 0.734483, recall = 0.504739, csi = 0.428571\n",
      "Minibatch loss at step 16500 learning_rate = 0.019903: loss = 0.055852\n",
      "TP = 205, FP = 79, FN = 216, TN = 28665\n",
      "precision = 0.719298, recall = 0.485782, csi = 0.410000\n",
      "Minibatch loss at step 17000 learning_rate = 0.019903: loss = 0.012916\n",
      "TP = 219, FP = 87, FN = 202, TN = 28657\n",
      "precision = 0.713355, recall = 0.518957, csi = 0.431102\n",
      "Minibatch loss at step 17500 learning_rate = 0.019903: loss = 0.014060\n",
      "TP = 259, FP = 131, FN = 162, TN = 28613\n",
      "precision = 0.662404, recall = 0.613744, csi = 0.469203\n",
      "Minibatch loss at step 18000 learning_rate = 0.018907: loss = 0.060242\n",
      "TP = 279, FP = 166, FN = 142, TN = 28578\n",
      "precision = 0.625561, recall = 0.661137, csi = 0.475298\n",
      "Minibatch loss at step 18500 learning_rate = 0.018907: loss = 0.033423\n",
      "TP = 231, FP = 95, FN = 190, TN = 28649\n",
      "precision = 0.706422, recall = 0.547393, csi = 0.447674\n",
      "Minibatch loss at step 19000 learning_rate = 0.018907: loss = 0.042685\n",
      "TP = 305, FP = 307, FN = 116, TN = 28437\n",
      "precision = 0.497553, recall = 0.722749, csi = 0.418956\n",
      "Minibatch loss at step 19500 learning_rate = 0.018907: loss = 0.009705\n",
      "TP = 287, FP = 202, FN = 134, TN = 28542\n",
      "precision = 0.585714, recall = 0.680095, csi = 0.460674\n",
      "Minibatch loss at step 20000 learning_rate = 0.017962: loss = 0.123469\n",
      "TP = 284, FP = 181, FN = 137, TN = 28563\n",
      "precision = 0.609442, recall = 0.672986, csi = 0.471761\n",
      "528.529836178\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "num_steps = 40001\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "        batch_data1 = train_data_ASOS_2d_s[offset:(offset + batch_size), :, :, :]\n",
    "        batch_data2 = train_data_n_resample_NWP_s[offset:(offset + batch_size), :]\n",
    "        batch_labels = v_train_label_resample_s[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset1 : batch_data1,  tf_train_dataset2 : batch_data2, tf_train_labels : batch_labels}\n",
    "        _, l, lr = session.run([optimizer, loss, learning_rate], feed_dict=feed_dict)\n",
    "        if step % 500 == 0:\n",
    "            print('Minibatch loss at step %d learning_rate = %f: loss = %f' % (step, lr, l))\n",
    "            tp, fp, fn, tn, precision, recall, f_score= session.run([TP, FP, FN, TN, pre, rec,csi])\n",
    "            print(\"TP = %d, FP = %d, FN = %d, TN = %d\" % (tp, fp, fn, tn))\n",
    "            print(\"precision = %f, recall = %f, csi = %f\" % (precision, recall, f_score))\n",
    "et = time.time()\n",
    "print(et-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
